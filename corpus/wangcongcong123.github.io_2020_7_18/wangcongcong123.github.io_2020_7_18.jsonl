{"id": "https://wangcongcong123.github.io/_p0", "contents": "Ph.D Candidate at CS, University College Dublin"}
{"id": "https://wangcongcong123.github.io/_p1", "contents": "TL;DR. This link provides the code repository that contains two readily downloadable fine-tuned GPT-2 weights, a quick start guide of how to customize Autocoder, and a list of future pointers to this project. Although this blog looks like a technical introduction to Autocoder, I also by the way talk about a lot of relevant stuff, such as nice work, status quo, and future directions in NLP."}
{"id": "https://wangcongcong123.github.io/_p2", "contents": "Human have walked into an uncharted territory since the outbreak of the COVID-19 coronavirus. Much effort has been made to stop the crisis. For example, in machine learning community, some people have been seeking computational techniques for extracting insights from COVID-19 literature, such as the COVID-19 Open Research Dataset Challenge (CORD-19), which was also mentioned in the news."}
{"id": "https://wangcongcong123.github.io/_p3", "contents": "This post guides you to write a python script that is able to monitor the open-access repository of electronic preprints (arXiv) for automatic post on Twitter, named Feeder-bot. Its workflow is illustrated as follows."}
{"id": "https://wangcongcong123.github.io/_p4", "contents": "Text classification as an important task in natural lanugage understanding (NLP) has been widely studied over the last several decades. The task describes input as a document and output as the category of which the document belongs to. In literature, both supervised and unsupervised methods have been applied for text classification."}
{"id": "https://wangcongcong123.github.io/_p5", "contents": "In this post, I share the Chinese Room Thought Experiment that I played with kids in my Communication and Outreach course."}
{"id": "https://wangcongcong123.github.io/_p6", "contents": "As I said, currently, I am a big fun of AI2. This began when I got knowing its work into an open-source NLP research library - AllenNLP. The more I hack the library, the more attractive it is for me. A short paragraph of praise below was written when I was hacking the tool."}
{"id": "https://wangcongcong123.github.io/mailto:wangcongcongcc@gmail.com_p0", "contents": "Ph.D Candidate at CS, University College Dublin"}
{"id": "https://wangcongcong123.github.io//more_p0", "contents": "Ph.D Candidate at CS, University College Dublin"}
{"id": "https://wangcongcong123.github.io//more_p1", "contents": "I am a PhD candidate in the School of Computer Science, UCD, under the supervision of Dr. David Lillis. My PhD research focuses on NLP for emergency response. In my spare time, I like coding stuff that I feel hard to get from the internet. Currently, I am a fan of my-field related big open source companies like AI2 and HuggingFace. There\u2019s really much to learn from them! In addition, my interests include cooking, reading and non-fictional or fictional writing. More about me go to my r\u00e9sum\u00e9."}
{"id": "https://wangcongcong123.github.io//more_p2", "contents": "***\t24/03/2020: Transfer report."}
{"id": "https://wangcongcong123.github.io//more_p3", "contents": "For anyone interested, below are recommended tools for mantaining a blog site. These are just so far personally thinking good choices. Let me know if you have better alternatives."}
{"id": "https://wangcongcong123.github.io//more_p4", "contents": "I use jekyll. A quick start guide for building a Jekyll blog can be found here jekyll-now. Jekyll also provides easy ways for tracking the data flow of the site via Google Analytics and highlighting programming codes (with one or several lines of codes). If I feel I need to use more front-end GUI components in my blogs, I can easily intergrate the bootstrap package with the site as well."}
{"id": "https://wangcongcong123.github.io//more_p5", "contents": "I draw via draw.io and export graphs to svg files so that I can render the graphs as tensor images in my blogs. I also consider Google Slides some times."}
{"id": "https://wangcongcong123.github.io//more_p6", "contents": "I use both Mathpix and MathJax. The former helps me convert equations in screenshots to editable latex format. The latter helps display the equations correctly in all browsers."}
{"id": "https://wangcongcong123.github.io//more_p7", "contents": "I usually first crop the table/graph portions from the papers with some pdf tools such as pdfresizer. With the cropped pdf portions, I then convert them to svgs files after adding references so that I can use them as tensor images in my blogs. There are many online pdf2svg convertors such as ZAMZAR. I know this sounds a bit troublesome, so I am looking for better ways for citing tensor images from pdf files."}
{"id": "https://wangcongcong123.github.io//tools_p0", "contents": "Ph.D Candidate at CS, University College Dublin"}
{"id": "https://wangcongcong123.github.io//tools_p1", "contents": "Feeder-Bot: a tool for automating updates of RSS feeds to your platform (Twitter for example). Blog Code"}
{"id": "https://wangcongcong123.github.io//tools_p2", "contents": "Autocoder: A basic and simple tool for code auto completion. Blog Code"}
{"id": "https://wangcongcong123.github.io//publications_p0", "contents": "Ph.D Candidate at CS, University College Dublin"}
{"id": "https://wangcongcong123.github.io//publications_p1", "contents": "Google Scholar"}
{"id": "https://wangcongcong123.github.io//publications_p2", "contents": "Congcong Wang, David Lillis. Classification for Crisis-Related Tweets Leveraging\nWord Embeddings and Data Augmentation. In Proceedings of the Twenty-Eighth Text REtrieval Conference (TREC 2019), Gaithersburg, MD, 2020. PDF\nBibTex\nPoster\nCode"}
{"id": "https://wangcongcong123.github.io//projects_p0", "contents": "Ph.D Candidate at CS, University College Dublin"}
{"id": "https://wangcongcong123.github.io//projects_p1", "contents": "Covid19-search: A retrieval system for searching covid-19 relevant papers built upon CORD19 dataset. Mar, 2020. Code Live Demo Blog"}
{"id": "https://wangcongcong123.github.io//projects_p2", "contents": "MLP-TC: Machine Learning Package for Text Classification. Dec, 2019. Code"}
{"id": "https://wangcongcong123.github.io//projects_p3", "contents": "RSEval: An online evaluation platform for recommender systems. Oct, 2017 - May, 2018\nPDF\nSlides\nCode"}
{"id": "https://wangcongcong123.github.io/#figure5_p0", "contents": "Ph.D Candidate at CS, University College Dublin"}
{"id": "https://wangcongcong123.github.io/#figure5_p1", "contents": "TL;DR. This link provides the code repository that contains two readily downloadable fine-tuned GPT-2 weights, a quick start guide of how to customize Autocoder, and a list of future pointers to this project. Although this blog looks like a technical introduction to Autocoder, I also by the way talk about a lot of relevant stuff, such as nice work, status quo, and future directions in NLP."}
{"id": "https://wangcongcong123.github.io/#figure5_p2", "contents": "Human have walked into an uncharted territory since the outbreak of the COVID-19 coronavirus. Much effort has been made to stop the crisis. For example, in machine learning community, some people have been seeking computational techniques for extracting insights from COVID-19 literature, such as the COVID-19 Open Research Dataset Challenge (CORD-19), which was also mentioned in the news."}
{"id": "https://wangcongcong123.github.io/#figure5_p3", "contents": "This post guides you to write a python script that is able to monitor the open-access repository of electronic preprints (arXiv) for automatic post on Twitter, named Feeder-bot. Its workflow is illustrated as follows."}
{"id": "https://wangcongcong123.github.io/#figure5_p4", "contents": "Text classification as an important task in natural lanugage understanding (NLP) has been widely studied over the last several decades. The task describes input as a document and output as the category of which the document belongs to. In literature, both supervised and unsupervised methods have been applied for text classification."}
{"id": "https://wangcongcong123.github.io/#figure5_p5", "contents": "In this post, I share the Chinese Room Thought Experiment that I played with kids in my Communication and Outreach course."}
{"id": "https://wangcongcong123.github.io/#figure5_p6", "contents": "As I said, currently, I am a big fun of AI2. This began when I got knowing its work into an open-source NLP research library - AllenNLP. The more I hack the library, the more attractive it is for me. A short paragraph of praise below was written when I was hacking the tool."}
{"id": "https://wangcongcong123.github.io/#figure3_p0", "contents": "Ph.D Candidate at CS, University College Dublin"}
{"id": "https://wangcongcong123.github.io/#figure3_p1", "contents": "TL;DR. This link provides the code repository that contains two readily downloadable fine-tuned GPT-2 weights, a quick start guide of how to customize Autocoder, and a list of future pointers to this project. Although this blog looks like a technical introduction to Autocoder, I also by the way talk about a lot of relevant stuff, such as nice work, status quo, and future directions in NLP."}
{"id": "https://wangcongcong123.github.io/#figure3_p2", "contents": "Human have walked into an uncharted territory since the outbreak of the COVID-19 coronavirus. Much effort has been made to stop the crisis. For example, in machine learning community, some people have been seeking computational techniques for extracting insights from COVID-19 literature, such as the COVID-19 Open Research Dataset Challenge (CORD-19), which was also mentioned in the news."}
{"id": "https://wangcongcong123.github.io/#figure3_p3", "contents": "This post guides you to write a python script that is able to monitor the open-access repository of electronic preprints (arXiv) for automatic post on Twitter, named Feeder-bot. Its workflow is illustrated as follows."}
{"id": "https://wangcongcong123.github.io/#figure3_p4", "contents": "Text classification as an important task in natural lanugage understanding (NLP) has been widely studied over the last several decades. The task describes input as a document and output as the category of which the document belongs to. In literature, both supervised and unsupervised methods have been applied for text classification."}
{"id": "https://wangcongcong123.github.io/#figure3_p5", "contents": "In this post, I share the Chinese Room Thought Experiment that I played with kids in my Communication and Outreach course."}
{"id": "https://wangcongcong123.github.io/#figure3_p6", "contents": "As I said, currently, I am a big fun of AI2. This began when I got knowing its work into an open-source NLP research library - AllenNLP. The more I hack the library, the more attractive it is for me. A short paragraph of praise below was written when I was hacking the tool."}
{"id": "https://wangcongcong123.github.io/#figure4_p0", "contents": "Ph.D Candidate at CS, University College Dublin"}
{"id": "https://wangcongcong123.github.io/#figure4_p1", "contents": "TL;DR. This link provides the code repository that contains two readily downloadable fine-tuned GPT-2 weights, a quick start guide of how to customize Autocoder, and a list of future pointers to this project. Although this blog looks like a technical introduction to Autocoder, I also by the way talk about a lot of relevant stuff, such as nice work, status quo, and future directions in NLP."}
{"id": "https://wangcongcong123.github.io/#figure4_p2", "contents": "Human have walked into an uncharted territory since the outbreak of the COVID-19 coronavirus. Much effort has been made to stop the crisis. For example, in machine learning community, some people have been seeking computational techniques for extracting insights from COVID-19 literature, such as the COVID-19 Open Research Dataset Challenge (CORD-19), which was also mentioned in the news."}
{"id": "https://wangcongcong123.github.io/#figure4_p3", "contents": "This post guides you to write a python script that is able to monitor the open-access repository of electronic preprints (arXiv) for automatic post on Twitter, named Feeder-bot. Its workflow is illustrated as follows."}
{"id": "https://wangcongcong123.github.io/#figure4_p4", "contents": "Text classification as an important task in natural lanugage understanding (NLP) has been widely studied over the last several decades. The task describes input as a document and output as the category of which the document belongs to. In literature, both supervised and unsupervised methods have been applied for text classification."}
{"id": "https://wangcongcong123.github.io/#figure4_p5", "contents": "In this post, I share the Chinese Room Thought Experiment that I played with kids in my Communication and Outreach course."}
{"id": "https://wangcongcong123.github.io/#figure4_p6", "contents": "As I said, currently, I am a big fun of AI2. This began when I got knowing its work into an open-source NLP research library - AllenNLP. The more I hack the library, the more attractive it is for me. A short paragraph of praise below was written when I was hacking the tool."}
{"id": "https://wangcongcong123.github.io/#figure1_p0", "contents": "Ph.D Candidate at CS, University College Dublin"}
{"id": "https://wangcongcong123.github.io/#figure1_p1", "contents": "TL;DR. This link provides the code repository that contains two readily downloadable fine-tuned GPT-2 weights, a quick start guide of how to customize Autocoder, and a list of future pointers to this project. Although this blog looks like a technical introduction to Autocoder, I also by the way talk about a lot of relevant stuff, such as nice work, status quo, and future directions in NLP."}
{"id": "https://wangcongcong123.github.io/#figure1_p2", "contents": "Human have walked into an uncharted territory since the outbreak of the COVID-19 coronavirus. Much effort has been made to stop the crisis. For example, in machine learning community, some people have been seeking computational techniques for extracting insights from COVID-19 literature, such as the COVID-19 Open Research Dataset Challenge (CORD-19), which was also mentioned in the news."}
{"id": "https://wangcongcong123.github.io/#figure1_p3", "contents": "This post guides you to write a python script that is able to monitor the open-access repository of electronic preprints (arXiv) for automatic post on Twitter, named Feeder-bot. Its workflow is illustrated as follows."}
{"id": "https://wangcongcong123.github.io/#figure1_p4", "contents": "Text classification as an important task in natural lanugage understanding (NLP) has been widely studied over the last several decades. The task describes input as a document and output as the category of which the document belongs to. In literature, both supervised and unsupervised methods have been applied for text classification."}
{"id": "https://wangcongcong123.github.io/#figure1_p5", "contents": "In this post, I share the Chinese Room Thought Experiment that I played with kids in my Communication and Outreach course."}
{"id": "https://wangcongcong123.github.io/#figure1_p6", "contents": "As I said, currently, I am a big fun of AI2. This began when I got knowing its work into an open-source NLP research library - AllenNLP. The more I hack the library, the more attractive it is for me. A short paragraph of praise below was written when I was hacking the tool."}
{"id": "https://wangcongcong123.github.io/#section_5_p0", "contents": "Ph.D Candidate at CS, University College Dublin"}
{"id": "https://wangcongcong123.github.io/#section_5_p1", "contents": "TL;DR. This link provides the code repository that contains two readily downloadable fine-tuned GPT-2 weights, a quick start guide of how to customize Autocoder, and a list of future pointers to this project. Although this blog looks like a technical introduction to Autocoder, I also by the way talk about a lot of relevant stuff, such as nice work, status quo, and future directions in NLP."}
{"id": "https://wangcongcong123.github.io/#section_5_p2", "contents": "Human have walked into an uncharted territory since the outbreak of the COVID-19 coronavirus. Much effort has been made to stop the crisis. For example, in machine learning community, some people have been seeking computational techniques for extracting insights from COVID-19 literature, such as the COVID-19 Open Research Dataset Challenge (CORD-19), which was also mentioned in the news."}
{"id": "https://wangcongcong123.github.io/#section_5_p3", "contents": "This post guides you to write a python script that is able to monitor the open-access repository of electronic preprints (arXiv) for automatic post on Twitter, named Feeder-bot. Its workflow is illustrated as follows."}
{"id": "https://wangcongcong123.github.io/#section_5_p4", "contents": "Text classification as an important task in natural lanugage understanding (NLP) has been widely studied over the last several decades. The task describes input as a document and output as the category of which the document belongs to. In literature, both supervised and unsupervised methods have been applied for text classification."}
{"id": "https://wangcongcong123.github.io/#section_5_p5", "contents": "In this post, I share the Chinese Room Thought Experiment that I played with kids in my Communication and Outreach course."}
{"id": "https://wangcongcong123.github.io/#section_5_p6", "contents": "As I said, currently, I am a big fun of AI2. This began when I got knowing its work into an open-source NLP research library - AllenNLP. The more I hack the library, the more attractive it is for me. A short paragraph of praise below was written when I was hacking the tool."}
{"id": "https://wangcongcong123.github.io/#section_4_p0", "contents": "Ph.D Candidate at CS, University College Dublin"}
{"id": "https://wangcongcong123.github.io/#section_4_p1", "contents": "TL;DR. This link provides the code repository that contains two readily downloadable fine-tuned GPT-2 weights, a quick start guide of how to customize Autocoder, and a list of future pointers to this project. Although this blog looks like a technical introduction to Autocoder, I also by the way talk about a lot of relevant stuff, such as nice work, status quo, and future directions in NLP."}
{"id": "https://wangcongcong123.github.io/#section_4_p2", "contents": "Human have walked into an uncharted territory since the outbreak of the COVID-19 coronavirus. Much effort has been made to stop the crisis. For example, in machine learning community, some people have been seeking computational techniques for extracting insights from COVID-19 literature, such as the COVID-19 Open Research Dataset Challenge (CORD-19), which was also mentioned in the news."}
{"id": "https://wangcongcong123.github.io/#section_4_p3", "contents": "This post guides you to write a python script that is able to monitor the open-access repository of electronic preprints (arXiv) for automatic post on Twitter, named Feeder-bot. Its workflow is illustrated as follows."}
{"id": "https://wangcongcong123.github.io/#section_4_p4", "contents": "Text classification as an important task in natural lanugage understanding (NLP) has been widely studied over the last several decades. The task describes input as a document and output as the category of which the document belongs to. In literature, both supervised and unsupervised methods have been applied for text classification."}
{"id": "https://wangcongcong123.github.io/#section_4_p5", "contents": "In this post, I share the Chinese Room Thought Experiment that I played with kids in my Communication and Outreach course."}
{"id": "https://wangcongcong123.github.io/#section_4_p6", "contents": "As I said, currently, I am a big fun of AI2. This began when I got knowing its work into an open-source NLP research library - AllenNLP. The more I hack the library, the more attractive it is for me. A short paragraph of praise below was written when I was hacking the tool."}
{"id": "https://wangcongcong123.github.io/#section_3_p0", "contents": "Ph.D Candidate at CS, University College Dublin"}
{"id": "https://wangcongcong123.github.io/#section_3_p1", "contents": "TL;DR. This link provides the code repository that contains two readily downloadable fine-tuned GPT-2 weights, a quick start guide of how to customize Autocoder, and a list of future pointers to this project. Although this blog looks like a technical introduction to Autocoder, I also by the way talk about a lot of relevant stuff, such as nice work, status quo, and future directions in NLP."}
{"id": "https://wangcongcong123.github.io/#section_3_p2", "contents": "Human have walked into an uncharted territory since the outbreak of the COVID-19 coronavirus. Much effort has been made to stop the crisis. For example, in machine learning community, some people have been seeking computational techniques for extracting insights from COVID-19 literature, such as the COVID-19 Open Research Dataset Challenge (CORD-19), which was also mentioned in the news."}
{"id": "https://wangcongcong123.github.io/#section_3_p3", "contents": "This post guides you to write a python script that is able to monitor the open-access repository of electronic preprints (arXiv) for automatic post on Twitter, named Feeder-bot. Its workflow is illustrated as follows."}
{"id": "https://wangcongcong123.github.io/#section_3_p4", "contents": "Text classification as an important task in natural lanugage understanding (NLP) has been widely studied over the last several decades. The task describes input as a document and output as the category of which the document belongs to. In literature, both supervised and unsupervised methods have been applied for text classification."}
{"id": "https://wangcongcong123.github.io/#section_3_p5", "contents": "In this post, I share the Chinese Room Thought Experiment that I played with kids in my Communication and Outreach course."}
{"id": "https://wangcongcong123.github.io/#section_3_p6", "contents": "As I said, currently, I am a big fun of AI2. This began when I got knowing its work into an open-source NLP research library - AllenNLP. The more I hack the library, the more attractive it is for me. A short paragraph of praise below was written when I was hacking the tool."}
{"id": "https://wangcongcong123.github.io/#section_2_p0", "contents": "Ph.D Candidate at CS, University College Dublin"}
{"id": "https://wangcongcong123.github.io/#section_2_p1", "contents": "TL;DR. This link provides the code repository that contains two readily downloadable fine-tuned GPT-2 weights, a quick start guide of how to customize Autocoder, and a list of future pointers to this project. Although this blog looks like a technical introduction to Autocoder, I also by the way talk about a lot of relevant stuff, such as nice work, status quo, and future directions in NLP."}
{"id": "https://wangcongcong123.github.io/#section_2_p2", "contents": "Human have walked into an uncharted territory since the outbreak of the COVID-19 coronavirus. Much effort has been made to stop the crisis. For example, in machine learning community, some people have been seeking computational techniques for extracting insights from COVID-19 literature, such as the COVID-19 Open Research Dataset Challenge (CORD-19), which was also mentioned in the news."}
{"id": "https://wangcongcong123.github.io/#section_2_p3", "contents": "This post guides you to write a python script that is able to monitor the open-access repository of electronic preprints (arXiv) for automatic post on Twitter, named Feeder-bot. Its workflow is illustrated as follows."}
{"id": "https://wangcongcong123.github.io/#section_2_p4", "contents": "Text classification as an important task in natural lanugage understanding (NLP) has been widely studied over the last several decades. The task describes input as a document and output as the category of which the document belongs to. In literature, both supervised and unsupervised methods have been applied for text classification."}
{"id": "https://wangcongcong123.github.io/#section_2_p5", "contents": "In this post, I share the Chinese Room Thought Experiment that I played with kids in my Communication and Outreach course."}
{"id": "https://wangcongcong123.github.io/#section_2_p6", "contents": "As I said, currently, I am a big fun of AI2. This began when I got knowing its work into an open-source NLP research library - AllenNLP. The more I hack the library, the more attractive it is for me. A short paragraph of praise below was written when I was hacking the tool."}
{"id": "https://wangcongcong123.github.io/#section_1_p0", "contents": "Ph.D Candidate at CS, University College Dublin"}
{"id": "https://wangcongcong123.github.io/#section_1_p1", "contents": "TL;DR. This link provides the code repository that contains two readily downloadable fine-tuned GPT-2 weights, a quick start guide of how to customize Autocoder, and a list of future pointers to this project. Although this blog looks like a technical introduction to Autocoder, I also by the way talk about a lot of relevant stuff, such as nice work, status quo, and future directions in NLP."}
{"id": "https://wangcongcong123.github.io/#section_1_p2", "contents": "Human have walked into an uncharted territory since the outbreak of the COVID-19 coronavirus. Much effort has been made to stop the crisis. For example, in machine learning community, some people have been seeking computational techniques for extracting insights from COVID-19 literature, such as the COVID-19 Open Research Dataset Challenge (CORD-19), which was also mentioned in the news."}
{"id": "https://wangcongcong123.github.io/#section_1_p3", "contents": "This post guides you to write a python script that is able to monitor the open-access repository of electronic preprints (arXiv) for automatic post on Twitter, named Feeder-bot. Its workflow is illustrated as follows."}
{"id": "https://wangcongcong123.github.io/#section_1_p4", "contents": "Text classification as an important task in natural lanugage understanding (NLP) has been widely studied over the last several decades. The task describes input as a document and output as the category of which the document belongs to. In literature, both supervised and unsupervised methods have been applied for text classification."}
{"id": "https://wangcongcong123.github.io/#section_1_p5", "contents": "In this post, I share the Chinese Room Thought Experiment that I played with kids in my Communication and Outreach course."}
{"id": "https://wangcongcong123.github.io/#section_1_p6", "contents": "As I said, currently, I am a big fun of AI2. This began when I got knowing its work into an open-source NLP research library - AllenNLP. The more I hack the library, the more attractive it is for me. A short paragraph of praise below was written when I was hacking the tool."}
{"id": "https://wangcongcong123.github.io//AutoCoder/_p0", "contents": "Ph.D Candidate at CS, University College Dublin"}
{"id": "https://wangcongcong123.github.io//AutoCoder/_p1", "contents": "TL;DR. This link provides the code repository that contains two readily downloadable fine-tuned GPT-2 weights, a quick start guide of how to customize Autocoder, and a list of future pointers to this project. Although this blog looks like a technical introduction to Autocoder, I also by the way talk about a lot of relevant stuff, such as nice work, status quo, and future directions in NLP."}
{"id": "https://wangcongcong123.github.io//AutoCoder/_p2", "contents": "This blog introduces \ud83e\udd16Autocoder - A basic and simple tool for code generation, built upon the pre-trained gpt-2 variants provided by HuggingFace\u2019s \ud83e\udd17transformers library. Below presents the workflow of Autocoder at training and inference time."}
{"id": "https://wangcongcong123.github.io//AutoCoder/_p3", "contents": "This blog first gives an introduction to the project\u2019s background, and then reveals the details of how the dataset is prepared and the fine-tuning process is conducted in programming with Python. I will also give some of my personal reflections on the generated codes by Autocoder. Finally, a list of future pointers to Autocoder will be presented. The outline of this blog is organized as follows."}
{"id": "https://wangcongcong123.github.io//AutoCoder/_p5", "contents": "Autocoder is designed for the code completion task (CCT) where a sequence of codes written by the programmer \ud83d\udc68\u200d\ud83d\udcbb are detected as the context to prompt the automatic generation of the uncompleted codes by a program \ud83e\udd16. Currently, many IDEs are able to auto-complete code but in a limited way. That says they only perform well in completing short sequence of codes in situations where the generated codes do not so heavily depend on the context of long sequence, such as, in PyCharm, completing a method\u2019s name, variable\u2019s name, etc. When there is a bottleneck, people try hard to escape from it although it is usually hard. Facing this bottleneck, now it makes sense to think about if the CCT can be advanced by taking a further step, namely, taking longer context into account for generating useable code in more complicated situations."}
{"id": "https://wangcongcong123.github.io//AutoCoder/_p6", "contents": "Based on the current progress of deep learning, the answer is yes but in a semi-automatic way. It is likely to train a deep model big enough to memorize or summarize rules or patterns of statistically commonly-used codes by the real programmers. For example, in Figure 1, the factorial code snippet is generated correctly basically because the model learns from the training codes that most programmers have written it in this way. However, for situations where the code required for generation is unusual such as changing n to another name, it is easy to fool the model to generate some unexpected codes\uff08will talk more about this in Section 4)."}
{"id": "https://wangcongcong123.github.io//AutoCoder/_p7", "contents": "This reflects an important weakness of the trained model -  the lack of reasoning, which has raised much concern in the deep learning community over the last few years. For knowing about the concern, it is recommended to read the book of why or the Chinese Room Experiment. To gain a general sense of how far nowadays the deep models are from reasoning, the following graph from the GPT-3 paper (Brown et al., 2020) tells something. I annotated the gap indicating the difference between human\u2019s performance and machine\u2019s (a very big model with 178B params) performance in the Winogrande task (Sakaguchi et al., 2019) in few-shot setting (making predictions with only few examples fed to train the model)."}
{"id": "https://wangcongcong123.github.io//AutoCoder/_p8", "contents": "Hence, given the lack of reasoning, the codes need to be tweaked by humans if they are generated in an unexpected way (semi-automatic). Although it is likely that we will not see milestones in the near future for empowering the model with strong reasoning ability, fortunately we have seen some milestones these years, capable of memorizing and summarizing stuff very well. GPT (Radford et al., 2018) is undoubtedly one of the milestones."}
{"id": "https://wangcongcong123.github.io//AutoCoder/_p9", "contents": "GPT is short for Generative Pre-Training, initially proposed by OpenAI back to 2018 (quite a while ago \ud83d\ude02) for language modeling. In stricter terms, it is called casual language modeling (CLM in order for differentiating it from other language models such as Masked Language Modeling), namely, next word prediction given previous consecutive words in a sequence, which is illustrated below."}
{"id": "https://wangcongcong123.github.io//AutoCoder/_p10", "contents": "Briefly interpreting GPT from its name, generative indicates it is something that is very good at generating stuff. One important element making GPT\u2019s generative ability powerful is that it applies an attention mechanism called masked self-attention mechanism (MSA), The MSA was initially used in the decoder of the transformer architecture from the paper titling \u201cAttention is all you need\u201d (Vaswani et al., 2017). The transformer paper by Google is deemed as a very important work in NLP since it proposed a new network architecture paradigm that completely relies on attention instead of previously commonly-used recurrent architectures such as LSTM for learning sequential data like texts very well. It is important or even transformative (that\u2019s probably why it is called transformer) so that some claims like \u201cLSTM is dead. Long Live Transformers!\u201d went viral in the community. Below I draw a graph hopefully helpful for understanding the self-attention mechanism in the transformer architecture."}
{"id": "https://wangcongcong123.github.io//AutoCoder/_p11", "contents": "For knowing more about the transformer architecture and its self-attention mechanism (what is the intuition behind and why it works so well), here are a number of really helpful tutorials from the Made With ML community. Specifically, for knowing more on GPT, the illustrated GPT-2 blog is a good starting point."}
{"id": "https://wangcongcong123.github.io//AutoCoder/_p12", "contents": "The other part of GPT is pre-training. Pre-training is a very trendy term in transfer learning. What usually appears simultaneously with pre-training is fine-tuning. Basically, transfer learning says training a model beforehand on general dataset to learn some general features from the dataset (pre-training) and then transfers the learnt features to downstream specific tasks (fine-tuning). To gain a broad enough features, the dataset required for pre-training tends to be large scale and thus the pre-training is usually conducted in an unsupervised way. In computer vision, the general dataset for pre-training usually refers to the large-score image dataset - ImageNet and the learnt features are some graphical knowledge such as contours, colors etc. The downstream tasks can be identifying specific objects in an image or adding texts to describe the image, etc. By comparison, in NLP, the large-scale dataset usually refers to the unlabeled internet texts such as Wikipedia, Book Corpus or Common Crawl, etc. For example, BERT (Devlin et al., 2018) is pre-trained on Wikipedia (2,500 million words) and Book Corpus (800 million words). The learnt features in this way of pre-training include some general linguistic knowledge such as syntax or semantics (Rogers et al., 2020) that can be used for language understanding in downstream tasks. For fine-tuning in NLP, the downstream tasks are just all kinds of language tasks, such as, sentence/document classification, sequence labelling, reading comprehension, etc. Figure 4 illustrates the perspective of transfer learning in NLP."}
{"id": "https://wangcongcong123.github.io//AutoCoder/_p13", "contents": "To understand transfer learning in mathematics, the pre-training changes the model\u2019s weights/parameters from random initialization so as to fit well to the defined pre-training tasks (such as language modeling task in NLP) using the large-scale dataset. Fine-tuning then adjusts the weights so as to fits well to a specific task (such as sentiment classification in NLP)."}
{"id": "https://wangcongcong123.github.io//AutoCoder/_p14", "contents": "GPT is a good example of transfer learning, it is pre-trained on the internet text through language modeling and can be fine-tuned for downstream tasks. What derives from GPT is GPT-2 that simply is a larger model ($10x$ parameters) trained on more data ($10x$ and more diverse) than GPT. As we know CCT requires a model that can generate codes in a sequential way, GPT-2 is a good option for this task due to its generative attribute and good generative quality. Now we get the GPT-2 hammer in our toolkit. Referring back to Figure 4, since the pre-training is expensive in both memory and time, we fine-tune the model instead of pre-training from scratch. Fortunately, there are many pre-trained weights with different sizes available in the \ud83e\udd17transformers library (see Figure 1). However, \nall is ready except for one thing - the downstream dataset, which is introduced in the next section."}
{"id": "https://wangcongcong123.github.io//AutoCoder/_p16", "contents": "We know GPT-2 was initially proposed for language understanding through the pre-training on a CLM task. However, we should not limit its application only in language. To put it another way, the reason that it is good at generating coherent paragraphs of text is because it \u201cknows\u201d well to output the next occurrence given the context as the input in a sequence. In CLM, text just fits the pattern where the next occurrence is the next word and the context is the previous words in a sentence (see Figure 3). With this view, it is easy to deduce GPT-2 can be used for tasks involving sequence alike datasets. Image is possible (such as Image-GPT) if we think of an image as nothing but a sequence of numeric pixels. Music is possible if we think of a piece of music as a sequence of notes, such as MuseNet."}
{"id": "https://wangcongcong123.github.io//AutoCoder/_p17", "contents": "It can be easier to understand if we think about GPT-2 or any other deep models from the mathematics perspective. The truth is that they just know numbers and do not know stuff like positions, faces, colors, or words directly. They can know different words indirectly because different words are represented by vectors (nothing but different numbers) before being fed to the models. They know positions because the positions are represented by different numbers before being fed to the models."}
{"id": "https://wangcongcong123.github.io//AutoCoder/_p18", "contents": "Codes can be taken as sequential alike data for sure and thus the CCT can be performed by GPT-2. Below details the process of code dataset preparation for Autocoder."}
{"id": "https://wangcongcong123.github.io//AutoCoder/_p19", "contents": "First, we need to get the codes online. Here we avoid using some existing benchmarking datasets because we want to achieve this CCT from the very beginning stage - dataset mining. For this purpose, there are so many open source libraries that provide large volumes of codes available. As a starting point, let\u2019s just focuses on The Algorithms library now. We want Autocoder to help auto-complete codes at a general level. The codes of The Algorithms suits the need! Also, I think the codes from The Algorithms is well written (high-quality codes!). We want Autocoder to be able to generate both Python and Java codes so we simply clone the corresponding repositories from The Algorithms."}
{"id": "https://wangcongcong123.github.io//AutoCoder/_p20", "contents": "Next, we preprocess the dataset in the way as illustrated below."}
{"id": "https://wangcongcong123.github.io//AutoCoder/_p21", "contents": "We see from the graph that a source code file is split into several examples by applying a slicing window. The slicing window is just used to split the code content in a file into several segments. The segments are referred as the code snippets that we want the GPT-2 model to learn something from and can help auto complete code snippets at testing time. Also, we don\u2019t use the whole file as a code snippet because GPT-2 is sensitive to the input example/sequence length (The self attention mechanism has a quadratic complexity of both memory and time with respect to the sequence length, so a research branch has been working on optimizing this bottleneck, such as, Linformer). Because we used both Python and Java codes as the training set, two special tokens known as control codes, namely, <python> and <java> are added to the GPT2 tokenizer. With the control codes prepended at the start of each example as seen from Figure 5, the model will know which programming language an input example corresponds to. This will also help model make decisions between languages for generating codes accordingly at testing time. The following presents the core codes of how this is implemented in Python."}
{"id": "https://wangcongcong123.github.io//AutoCoder/_p22", "contents": "For the whole script of dataset preparation, go to this script."}
{"id": "https://wangcongcong123.github.io//AutoCoder/_p24", "contents": "Before fine-tuning, we split the dataset into a train set and development set at a ratio of $9:1$. This leads to ending up with around $69k$ training examples and $7k$ development examples."}
{"id": "https://wangcongcong123.github.io//AutoCoder/_p25", "contents": "The \ud83e\udd17transformers library contains the implementation of the GPT2 model with a language modeling head on top (linear layer with weights tied to the input embeddings), named GPT2LMHeadModel in the library. We use it for fine-tuning, where the GPT2 model is initialized by the pre-trained GPT2 weights before fine-tuning. The fine-tuning process trains the GPT2LMHeadModel in a batch size of $4$ per GPU. We set the maximum sequence length to be $256$ due to computational resources restrictions."}
{"id": "https://wangcongcong123.github.io//AutoCoder/_p26", "contents": "Although there are different sized pre-trained variants such as distilgpt2, gpt2-large, gpt2-medium, etc., we select distilgpt2 and gpt2-medium for fine-tuning. Both are fine-tuned on two GPUs (12GB RTX 2080Ti and 8GB RTX 2070 Super) with around $24$ hours for fine-tuning gpt2-medium (approx. $86k$ total steps ) and $8$ hours for distilgpt2 (approx. $69k$ total steps). We use Adam as the optimiser and set the learning rate to be $5e-5$ with warmup ratio $20%$ of the total training steps. During the fine-tuning, the best model saved is determined by perplexity evaluated on the development set with evaluation step of $200$. For tracking the training process, we use the awesome wandb tool for recording the experimental details. Here logs the training details of fine-tuning distilgpt2 and gpt2-medium for Autocoder. Below plots the state of development loss during fine-tuning from wandb."}
{"id": "https://wangcongcong123.github.io//AutoCoder/_p27", "contents": "hello world"}
{"id": "https://wangcongcong123.github.io//AutoCoder/_p28", "contents": "This repository provides the code-level details of Autocoder."}
{"id": "https://wangcongcong123.github.io//AutoCoder/_p30", "contents": "With the fine-tuned models, now let\u2019s try some examples for code completion by Autocoder and qualitatively analyze its performance. The examples presented in this section is based on the fine-tuned gpt2-medium version in Python."}
{"id": "https://wangcongcong123.github.io//AutoCoder/_p31", "contents": "The first example is: when I enter def factorial as the prompt code for Autocoder, which I intend to implement a function of calculating the factorial of a number. What Autocoder returns to me is"}
{"id": "https://wangcongcong123.github.io//AutoCoder/_p32", "contents": "Regarding the performance in time, this generation takes around 5 seconds (on a computer equipped with a strong CPU and a RTX 2080 GPU). This may need to be optimized if in a real-life use case. Aside from the temporal performance, this looks a very good-quality generation. However, for now, I take a grain of salt regarding Autocoder\u2019s performance. I want to test it further with such an example: def factorial(m). What Autocoder returns to me is"}
{"id": "https://wangcongcong123.github.io//AutoCoder/_p33", "contents": "This turns out that only the first line of comment looks good. I have to say this does not go as well as I expected. Autocoder should give me the code snippet the same as the def factorial example except for only changing n to m if it really understands the code. This invokes something I\u2019d like to share."}
{"id": "https://wangcongcong123.github.io//AutoCoder/_p34", "contents": "As far as the evidence, it looks that Autocoder generates a nice code snippet for the def factorial example. This is most likely cherry-picked. When I review quickly back to the training source code, there actually exists such a factorial function. If Autocoder fits too much into the training code, it is overfitting or to say it memorizes code rigidly and does not know how to generalize to new situations. This is called a narrow generation ability of a trained deep model as in the literature (Chollet, 2019). In another words, a deep model lack of a broad generalization ability implies it lacks the reasoning ability. For now, Autocoder built upon GPT - a very powerful generative model nowadays, does not gain a reasoning performance near to humans. The def factorial(m) is a piece of evidence to justify this conclusion and the following I give one more example To further support the conclusion."}
{"id": "https://wangcongcong123.github.io//AutoCoder/_p35", "contents": "When I enter def sum as the prompt code, which I intend to implement a function of summing two numbers. However, Autocoder returns to me,"}
{"id": "https://wangcongcong123.github.io//AutoCoder/_p36", "contents": "Although the code makes sense, it is not what I expected. I later figure out the reason that it behaves like this is because there exists such a sum_of_series function instead of the sum(a,b) function in the training set."}
{"id": "https://wangcongcong123.github.io//AutoCoder/_p37", "contents": "What does this tell? I would say that this is a big and important question in the way making AI to human-level intelligence. Although it is not that easy to tackle on the reasoning issue, the good news is that much concern has been raised recently in the community. Also, some seemingly promising work has been done into this direction. For example, Chollet critically discusses the measure of intelligence, which is a nice piece of work to read. And, some insights from the HANS paper (McCoy et al., 2019) that analyzed a language inference task (MNLI, one of GLUE tasks) reveal that the state-of-the-art language understanding models including BERT performs well in MNLI because it summarizes some surface syntactic heuristics well from the training examples. Hence, its performance can go down quickly when making changes to the test set and breaking the heuristics. Similarly, Gardner, et al., 2020 proposed that datasets authors should adding more challenging test examples to the test set (called contrast sets originally) in order to more accurately evaluate a model\u2019s true linguistic capabilities. Although leaderboards usually look overtaken by modern deep models, demonstrating a sense of AI hype, they are more like task-specific tuning. What the future needs is to do more work than this."}
{"id": "https://wangcongcong123.github.io//AutoCoder/_p38", "contents": "Taking a step back from criticizing Autocoder, let\u2019s quickly recall how humans learn coding, which also applies to learning any other stuff. In analogy, the pre-training of GPT-2 is like our language learning in school in the past. The fine-tuning of GPT-2 for Autocoder is like we step into university and select a programming module for learning how to code. Autocoder at inference time is like we sit in the exam room for testing how well we learn from the past and the programming module. I\u2019d like to say humans will achieve a better score than Autocoder at 100% percent (maybe not sometimes\ud83d\ude02) if they go through all materials of the programming module (like Autocoder is trained through epochs of iterations of the training set). Then what is the problem?"}
{"id": "https://wangcongcong123.github.io//AutoCoder/_p39", "contents": "First, let\u2019s leave alone the topic of if it is possible to achieve AGI. Let\u2019s just hypothesize for now it is possible to approximate a machine to a human. With this premise, we learn something from the narrative. The key difference in the learning of humans and Autocoder is that we learn not just language stuff from schools. We also learn visual objects, physical interactions, emotional relationships, etc. This says that we have some common knowledge or common sense on the world. For example, we comprehend Pythagorean theorem most likely because our teachers once drew three lines on the blackboard as the visual aids to help us understand it when we were young. And we understand the lines because we can associate it to the physical world\u2019s concepts such as distances. Through all the way of our learning journey, we form a big network that contains a knowledge graph, basically, mirroring our understanding on the world. With knowledge accumulated, we become versed at organizing and associating the nodes in the knowledge graph. When we come to perform a new task even though it is a brand new task we have never tried before such as driving a car under a dark tunnel, we know how to take actions to minimize the possibly-induced damage based on our prior knowledge. That\u2019s the strength of human-level generalization and reasoning through prior experience. To empower machines with this strength, the intuitions behind human learning is instructive for future work made into machine learning."}
{"id": "https://wangcongcong123.github.io//AutoCoder/_p41", "contents": "As presented, now Autocoder is very basic, only serving as the starting point of more work into it. The following summarizes a list of future work for Autocoder."}
{"id": "https://wangcongcong123.github.io//AutoCoder/_p42", "contents": "Disclaimer: The introduction to Autocoder is by no means showing it generates codes with a good sense of reasoning. More of this blog\u2019s purpose is to introduce the state-of-the-art generative model GPT-2 and present an example of how it is used for code completion."}
{"id": "https://wangcongcong123.github.io//AutoCoder/_p43", "contents": "Last, thanks to JiaZheng who kindly provided the computation resources for making this blog possible."}
{"id": "https://wangcongcong123.github.io//AutoCoder/_p44", "contents": "If you have any thoughts you\u2019d like to share with me about the blog, send me an email via wangcongcongcc@gmail.com or you are welcome to talk with me on my Twitter."}
{"id": "https://wangcongcong123.github.io//CovidSearch/_p0", "contents": "Ph.D Candidate at CS, University College Dublin"}
{"id": "https://wangcongcong123.github.io//CovidSearch/_p1", "contents": "Human have walked into an uncharted territory since the outbreak of the COVID-19 coronavirus. Much effort has been made to stop the crisis. For example, in machine learning community, some people have been seeking computational techniques for extracting insights from COVID-19 literature, such as the COVID-19 Open Research Dataset Challenge (CORD-19), which was also mentioned in the news."}
{"id": "https://wangcongcong123.github.io//CovidSearch/_p2", "contents": "Motivated by the community, I built a system for this purpose, I call it Covid19Search - a system for querying COVID-19 literature or finding insights from the literature. This post mainly explains the technical details behind the system. Below presents the overall process of the system in user customized query task and insights extraction task that are based on the 10 pre-defined questions by CORD-19."}
{"id": "https://wangcongcong123.github.io//CovidSearch/_p3", "contents": "For those just seeking a quick exploration for the system. Below are important places to go."}
{"id": "https://wangcongcong123.github.io//CovidSearch/_p4", "contents": "If you find something interesting in Figure 1 and want to know more, I would  say the rest of this blog feeds you well."}
{"id": "https://wangcongcong123.github.io//CovidSearch/_p5", "contents": "Basically, in this task, the core idea is to match a user\u2019s query with all documents (known as the corpus, academic articles in this case). The query is used to describe the user\u2019s information need such as the example in Figure 1: What drugs are helpful for COVID-19?"}
{"id": "https://wangcongcong123.github.io//CovidSearch/_p6", "contents": "The matching process is to caculate the similarities between the user\u2019s query and every single document in the corpus so that the most relevant documents can be returned to the user. That is at a high level how an information retrieval (IR) system (the daily-used Google for example) works."}
{"id": "https://wangcongcong123.github.io//CovidSearch/_p7", "contents": "Now, we need describe it in a mathematical way since human written like texts are unknown to machines. Machines are computers so they sheerly depend on numbers/digits, like viruses rely on living hosts otherwise they would die soon."}
{"id": "https://wangcongcong123.github.io//CovidSearch/_p8", "contents": "Hence, a conversion of texts to numbers is needed. The process of converting texts to numbers can be well explained by introducing vectorization (other names include weighting, embedding, etc.), as presented in the picture below."}
{"id": "https://wangcongcong123.github.io//CovidSearch/_p9", "contents": "Obviously, vectorization converts a varied-length sequence of texts to a fixed-length list of numbers (i.e., vectors) so relevance can be quantified actually by machines."}
{"id": "https://wangcongcong123.github.io//CovidSearch/_p10", "contents": "The converted vectors are important because they have to represent the texts very well. To say representing two texts that are assumed very relevant with vector 1 and 2, the objective here is to ensure each position of vector 1 should be close to that of vector 2 because the same position usually represents a shared latent feature of texts."}
{"id": "https://wangcongcong123.github.io//CovidSearch/_p11", "contents": "Given the importance, extensive research work has been made for this purpose. Some salient ones are given below."}
{"id": "https://wangcongcong123.github.io//CovidSearch/_p12", "contents": "Figure 4 demonstrates the process of vectorization using the aforementioned two methods: Count and TF-IDF. The two methods are straightforward. Basically, before vectorization, a step known as preprocessing, is taken to build a vocabulary that contains all unique words in the corpus (see the left box of Figure 4)."}
{"id": "https://wangcongcong123.github.io//CovidSearch/_p13", "contents": "The left box also shows a list of stop words used in preprocessing. The stop words are considered not to be informative for differentiating one document from another and thus they are removed from the list of vocabulary. Here some other steps in  preprocessing such as stemming, lemmatization, min-frequency filtering, etc., are omitted for demonstration convenience."}
{"id": "https://wangcongcong123.github.io//CovidSearch/_p14", "contents": "The right box presents the process of count and TF-IDF vectorization on doc1. The count vectorizer is fed with the raw texts of each doc in corpus and generate a count vector with size the same as that of the vocabulary to represent the doc."}
{"id": "https://wangcongcong123.github.io//CovidSearch/_p15", "contents": "The score or weight of each position in the count vector indicates the number of occurrences of the word that has the same position in the vocabulary list. For example, 2 at the third position refers to that \u201ccovid\u201d(at the third position in the vocabulary list) occurs in doc1."}
{"id": "https://wangcongcong123.github.io//CovidSearch/_p16", "contents": "The TF-IDF method works a bit differently from the Count method. It calculates a weight by multiplying term frequency (TF: actually the count mentioned in Count method) with inverse document frequency (IDF). See the right box of Figure 4 to know what they refer to."}
{"id": "https://wangcongcong123.github.io//CovidSearch/_p17", "contents": "Different from Count, IDF is introduced in this method to give more importance to the words that are not common across all documents. To put it in another way, in the example of Figure 4, the word \u201ccovid\u201d occurs in every document in the corpus (it is ordinary and not outstanding across the corpus) and hence it is penalized to 0 (its IDF is 0 so its TF-IDF is 0). This is unlike  a very high score 2 given to \u201ccovid\u201d in the Count method. In analogy, a pizza topped with sausage as well as jalape\u00f1o makes it stand out from all other pizzas topped with only sausage."}
{"id": "https://wangcongcong123.github.io//CovidSearch/_p18", "contents": "In TF-IDF, IDF is usually computed beforehand given the corpus is fixed and can be directly called to compute TF-IDF vector for a user query, which makes it more efficient of the retrieval process."}
{"id": "https://wangcongcong123.github.io//CovidSearch/_p19", "contents": "For BM25, it is based on a probabilistic retrieval model with the introduction of extra parameters (k and b) as compared to TF-IDF. To know how the weight is exactly calculated in BM25, have a look at this page."}
{"id": "https://wangcongcong123.github.io//CovidSearch/_p20", "contents": "To run any one of Count, TF-IDF, or BM25 for retrieving COVID-19 related papers with respect to a search query, have a look at the script."}
{"id": "https://wangcongcong123.github.io//CovidSearch/_p21", "contents": "FastText is a word embedding method to learn linguistic information of texts through neural networks (NNs) based language model."}
{"id": "https://wangcongcong123.github.io//CovidSearch/_p22", "contents": "Word embedding is a way to represent a word with fixed-length vectors of continuous real numbers. Due to this feature, it has been widely studied for representing text inputs for downstream NNs based language tasks such as classification, reading comprehension, translation and so on. This section only introduces its use in IR retrieval."}
{"id": "https://wangcongcong123.github.io//CovidSearch/_p23", "contents": "It  maps  a  word  in  a  vocabulary  to  a  latent  vector  space  where  words  with similar contexts are in proximity. Through word embedding, a word is converted to a vector that summarizes the word\u2019s both syntactic and semantic information. That says the embedding process is similar to the vectorization process as mentioned."}
{"id": "https://wangcongcong123.github.io//CovidSearch/_p24", "contents": "Although a lot of methods have been proposed to learn embeddings in the literature, as a taxonomy in Figure 5, FastText embeds words by treating  each  word  as  being  composed  of  character  n-grams  instead of a word whole.  This allows it to not only learn rare words but also out-of-vocabulary words."}
{"id": "https://wangcongcong123.github.io//CovidSearch/_p25", "contents": "In addition, unlike context-based embeddings that requires re-construction of network architecture for generating contextualized word representations, FastText\u2019s pre-trained embeddings can be read directly and thus efficiently represent words without going through layers of NNs. As a starting point, Covid19Search now supports FastText for vectorizing words."}
{"id": "https://wangcongcong123.github.io//CovidSearch/_p26", "contents": "To get a pre-trained FastText on the COVID-19 articles (CORD-19 dataset), have a look at the script to figure out easily. To represent a sequence of words, Covid19Search averages across FastText embeddings for individual words."}
{"id": "https://wangcongcong123.github.io//CovidSearch/_p27", "contents": "The vectorization process ensures a user query and every document in the corpus can be represented with vectors. With reference back to Figure 3, now the vectors are ready for relevance computation. Regarding relevance measure, imagining the vectors projected into a vector space, the purpose here is to calculate the distance between vector pairs. The closer distance indicates the closer relationship or to say the more relevant to each other. Below are a list of relevance measures that come to my mind immediately."}
{"id": "https://wangcongcong123.github.io//CovidSearch/_p28", "contents": "As seen, there are so many different ways to achieve this purpose, different methods have different use cases. For example, Levenshtein distance is known as edit distance that calculates the number of steps to edit a word to another word by insertion, deletion or substitution (such as 2 for coronavirus to coranavrius). Due to this feature, it is widely used in auto correcting misspellings. Covid19Search implemented cosine similarity for relevance measure, as calculated below."}
{"id": "https://wangcongcong123.github.io//CovidSearch/_p29", "contents": "The calculation iterates over all documents in the corpus with the user query, ending up with a list of documents deemed relevant to the query. Documents are ranked by the relevance scores in descending order to return to users so that users can see the most relevant documents in the first place. This is how the user query task works in Covid19Search and generally how an IR system works in real world."}
{"id": "https://wangcongcong123.github.io//CovidSearch/_p30", "contents": "There are two additional highlights of Covid19Search in the user query task"}
{"id": "https://wangcongcong123.github.io//CovidSearch/_p31", "contents": "Insights extraction task matches 10 pre-defined questions with the corpus for capturing relevant insights from the literature that are useful for answering the question. For example, one pre-defined question is: What do we know about COVID-19 risk factors? With this question, a retrieval system should target at finding important sentences from the literature that indicate useful information on COVID-19 risk factors. Motivated by this, Covid19Search leverages a recently-proposed sentence-level embedding method for this kind of semantic search. The method is known as Sentence-BERT. Too long to read the original paper, here gives a simplified and brief describtion of how the technique is achieved in Covid19Search."}
{"id": "https://wangcongcong123.github.io//CovidSearch/_p32", "contents": "At a high level, Sentence-BERT is nothing but the vectorizer that converts a sentence/question to a fixed-length vector, working at the vectorization step as presented in Figure 3. Figure 7 presents the relevance matching between a pre-defined question and each sentence of each article of all in the corpus.  This says, an article is split into sentences and then feed the sentences one by one to Sentence-BERT that help encode the sentences to vectors. The process is the same applied to the pre-defined question so that the final relevance can be computed."}
{"id": "https://wangcongcong123.github.io//CovidSearch/_p33", "contents": "What makes Sentence-BERT outstanding is that it treats every sentence as a whole to generate a vector representation instead of individual words within the sentence as described in the user query task (bag-of-words). Hence, the representation is able to reveal the sentence\u2019s meaning as a whole to some extent. Indicated by its name, it is evident that Sentence-BERT leverages the state-of-the-art transformer based BERT model (now it supports other transformer based models such as RoBERTa, DistilBERT, ALBERT) for such general sentence embedding purpose (it actually uses siamese BERT-Networks pre-trained on inference datasets)."}
{"id": "https://wangcongcong123.github.io//CovidSearch/_p34", "contents": "Delving into the depth of BERT or Sentence-BERT is beyond the scope of this blog. However, for whoever is interested in knowing about BERT, the two papers (Transformer and BERT) are recommended to read first. If not satisfied with the understanding of BERT after reading the two papers, the two tutorials the annotated transformer and The Illustrated BERT are recommended."}
{"id": "https://wangcongcong123.github.io//CovidSearch/_p35", "contents": "Despite the good feature of Sentence-BERT, it is not used in the user query task because it takes too much time to compute the relevance between an arbitrary user defined query (this needs to be re-computed for different queries every time) and all sentences (much more than the number of documents) in the corpus. Since the insights extraction task has a fixed query space (i.e, the 10 kaggle questions), the process of relevance matching can be computed beforehand and saved to a pre-matched file. In real-time retrieval, the file can be directly loaded for efficient retrieval, see the script and demo. If you want to mine insights from scratch, adapt and run this script."}
{"id": "https://wangcongcong123.github.io//CovidSearch/_p36", "contents": "Last, thanks to JiaZheng who kindly provided the computation resources for semantically pre-matching sentences using sentences-transformers."}
{"id": "https://wangcongcong123.github.io//CovidSearch/_p37", "contents": "If you have any thoughts you\u2019d like to share with me about the blog, send me an email via wangcongcongcc@gmail.com or you are welcome to talk with me on my Twitter."}
{"id": "https://wangcongcong123.github.io/#workflow_p0", "contents": "Ph.D Candidate at CS, University College Dublin"}
{"id": "https://wangcongcong123.github.io/#workflow_p1", "contents": "TL;DR. This link provides the code repository that contains two readily downloadable fine-tuned GPT-2 weights, a quick start guide of how to customize Autocoder, and a list of future pointers to this project. Although this blog looks like a technical introduction to Autocoder, I also by the way talk about a lot of relevant stuff, such as nice work, status quo, and future directions in NLP."}
{"id": "https://wangcongcong123.github.io/#workflow_p2", "contents": "Human have walked into an uncharted territory since the outbreak of the COVID-19 coronavirus. Much effort has been made to stop the crisis. For example, in machine learning community, some people have been seeking computational techniques for extracting insights from COVID-19 literature, such as the COVID-19 Open Research Dataset Challenge (CORD-19), which was also mentioned in the news."}
{"id": "https://wangcongcong123.github.io/#workflow_p3", "contents": "This post guides you to write a python script that is able to monitor the open-access repository of electronic preprints (arXiv) for automatic post on Twitter, named Feeder-bot. Its workflow is illustrated as follows."}
{"id": "https://wangcongcong123.github.io/#workflow_p4", "contents": "Text classification as an important task in natural lanugage understanding (NLP) has been widely studied over the last several decades. The task describes input as a document and output as the category of which the document belongs to. In literature, both supervised and unsupervised methods have been applied for text classification."}
{"id": "https://wangcongcong123.github.io/#workflow_p5", "contents": "In this post, I share the Chinese Room Thought Experiment that I played with kids in my Communication and Outreach course."}
{"id": "https://wangcongcong123.github.io/#workflow_p6", "contents": "As I said, currently, I am a big fun of AI2. This began when I got knowing its work into an open-source NLP research library - AllenNLP. The more I hack the library, the more attractive it is for me. A short paragraph of praise below was written when I was hacking the tool."}
{"id": "https://wangcongcong123.github.io//FeederBot/_p0", "contents": "Ph.D Candidate at CS, University College Dublin"}
{"id": "https://wangcongcong123.github.io//FeederBot/_p1", "contents": "This post guides you to write a python script that is able to monitor the open-access repository of electronic preprints (arXiv) for automatic post on Twitter, named Feeder-bot. Its workflow is illustrated as follows."}
{"id": "https://wangcongcong123.github.io//FeederBot/_p2", "contents": "Basically, Feeder-bot listens to a RSS feed provider (arXiv in this example) through its API (arXiv API in this example) and forward and post any identified updates to the receiver (Twitter in this example) via its API (Twitter API)."}
{"id": "https://wangcongcong123.github.io//FeederBot/_p3", "contents": "For the code details, refer to the repository. For how I built this tool, follow the rest of this blog."}
{"id": "https://wangcongcong123.github.io//FeederBot/_p4", "contents": "Below are the major motivations for me writing this tool."}
{"id": "https://wangcongcong123.github.io//FeederBot/_p5", "contents": "For whoever has the similar needs as me, I hope this tool is useful for you. Although the tool\u2019s name is Feeder-bot, the actual focus of this blog is on the arXiv2Twitter bot that works as a starting example (As in the figure illustrating this example) for expanding Feeder-bot to support more RSS feeds or receivers in the future. This open source tool welcomes everyone to join in and contribute to the future work. Below I provide some ideas that future work can go upon."}
{"id": "https://wangcongcong123.github.io//FeederBot/_p6", "contents": "Prior to coding, I knew I have to gain a general knowledge on two stuff. One is the Twitter API that enables a bot to post on Twitter and the other one is arXiv API that offers interfaces to access to its content. The former is a bit troublesome to get. You have to go to the Twitter Developer website and make an application for keys before using its APIs. (Hint: if you have trouble filling in the application form, google \u201ctwitter developer account application example\u201d that may help)."}
{"id": "https://wangcongcong123.github.io//FeederBot/_p7", "contents": "Assume you have completed this step, now you get four pieces of information. They are API key, API secret key, Access token and Access token secret. Keep them well and you will use them later. The arXiv API is easily to get access without the application process. Just check its API documentations, and easy to figure out how to use."}
{"id": "https://wangcongcong123.github.io//FeederBot/_p8", "contents": "# This is an example of arXiv URL (HTTP-GET) for requesting the latest paper that containes the keyword \"NLP\""}
{"id": "https://wangcongcong123.github.io//FeederBot/_p9", "contents": "After gaining a general knowledge of the APIs, I searched if there are any open source implementations upon the APIs and tweepy is a good choice for Twitter. To use it, install it with the command: pip install tweepy."}
{"id": "https://wangcongcong123.github.io//FeederBot/_p10", "contents": "For arXiv API, the only task to take is to parse the Atom fromatted content returned via the API from arXiv (I have never heard of Atom prior to reading arXiv API). The package named atoma is a good choice for this task (figure this out by simply googling \u201cpython atom parser\u201d). Having had a look at the quick start of this package, it is enough to be used for this tool. To install it, enter the command: pip install atoma."}
{"id": "https://wangcongcong123.github.io//FeederBot/_p11", "contents": "At this stage, it is easy to write a script that monitors the latest paper on ARXIV_QUERY through its API and posts the paper\u2019s link (LATEST_PAPER_URL) and title (LATEST_PAPER_TITLE) if it is found. To decide if new relevant paper is uploaded on arXiv, the bot requests arXiv every REQUEST_INTERVAL seconds. To make the bot a bit more interesting and flexible, some extra parameters are added as well. Below give examples of explaining these parameters."}
{"id": "https://wangcongcong123.github.io//FeederBot/_p12", "contents": "1.Set ARXIV_QUERY to be \u201cbert+OR+nlp\u201d indicating the user\u2019s interest fields include both BERT and NLP."}
{"id": "https://wangcongcong123.github.io//FeederBot/_p13", "contents": "2.Same as the above except that the update from arXiv is checked every 1800 seconds."}
{"id": "https://wangcongcong123.github.io//FeederBot/_p14", "contents": "3.Same as the above except that only the papers on ariXv with the update date no more than 5 days ago are considered to be forwarded and posted to Twitter."}
{"id": "https://wangcongcong123.github.io//FeederBot/_p15", "contents": "4.The papers containing the keywords both coronovirous and convid are tracked and post content is prepended with \u201c#virus,#covid2019,#coronovirous\u201d."}
{"id": "https://wangcongcong123.github.io//FeederBot/_p16", "contents": "5.The papers containing the keywords both crisis and classification are tracked and post content is prepended with \u201c#crisisresponse,#NLP,#classification\u201d."}
{"id": "https://wangcongcong123.github.io//FeederBot/_p17", "contents": "Where python arxiv_twitter.py -h"}
{"id": "https://wangcongcong123.github.io//FeederBot/_p18", "contents": "In conclusion, Feeder-bot helps you listen to updates from arXiv and automatically forward them to Twitter if updates found. You just need to specify the some arugments such as arxiv_query, hashtags_prepend to run the bot !\nPlease go to the code repository, where code is provided."}
{"id": "https://wangcongcong123.github.io//FeederBot/_p19", "contents": "I had no any idea of how to implement it in the beginning. Later on, I researched and completed this small program quickly. What I learnt from this experience is that one can build up a good stuff efficiently as long as he/she has solid basics of programming/knowledge and that stuff is what he/she really needs. The solid basics help him/her fix bugs quickly and thus reduce the pain in the development process. That is what makes him/her passionate about and thus a final release is possible. \n"}
{"id": "https://wangcongcong123.github.io//FeederBot/_p20", "contents": "If you have any thoughts you\u2019d like to share with me about the blog, send me an email via wangcongcongcc@gmail.com or you are welcome to talk with me on my Twitter."}
{"id": "https://wangcongcong123.github.io/_p0", "contents": "Ph.D Candidate at CS, University College Dublin"}
{"id": "https://wangcongcong123.github.io/_p1", "contents": "TL;DR. This link provides the code repository that contains two readily downloadable fine-tuned GPT-2 weights, a quick start guide of how to customize Autocoder, and a list of future pointers to this project. Although this blog looks like a technical introduction to Autocoder, I also by the way talk about a lot of relevant stuff, such as nice work, status quo, and future directions in NLP."}
{"id": "https://wangcongcong123.github.io/_p2", "contents": "Human have walked into an uncharted territory since the outbreak of the COVID-19 coronavirus. Much effort has been made to stop the crisis. For example, in machine learning community, some people have been seeking computational techniques for extracting insights from COVID-19 literature, such as the COVID-19 Open Research Dataset Challenge (CORD-19), which was also mentioned in the news."}
{"id": "https://wangcongcong123.github.io/_p3", "contents": "This post guides you to write a python script that is able to monitor the open-access repository of electronic preprints (arXiv) for automatic post on Twitter, named Feeder-bot. Its workflow is illustrated as follows."}
{"id": "https://wangcongcong123.github.io/_p4", "contents": "Text classification as an important task in natural lanugage understanding (NLP) has been widely studied over the last several decades. The task describes input as a document and output as the category of which the document belongs to. In literature, both supervised and unsupervised methods have been applied for text classification."}
{"id": "https://wangcongcong123.github.io/_p5", "contents": "In this post, I share the Chinese Room Thought Experiment that I played with kids in my Communication and Outreach course."}
{"id": "https://wangcongcong123.github.io/_p6", "contents": "As I said, currently, I am a big fun of AI2. This began when I got knowing its work into an open-source NLP research library - AllenNLP. The more I hack the library, the more attractive it is for me. A short paragraph of praise below was written when I was hacking the tool."}
{"id": "https://wangcongcong123.github.io//Classification/_p0", "contents": "Ph.D Candidate at CS, University College Dublin"}
{"id": "https://wangcongcong123.github.io//Classification/_p1", "contents": "Text classification as an important task in natural lanugage understanding (NLP) has been widely studied over the last several decades. The task describes input as a document and output as the category of which the document belongs to. In literature, both supervised and unsupervised methods have been applied for text classification."}
{"id": "https://wangcongcong123.github.io//Classification/_p2", "contents": "This blog only sets its scope on supervised learning methods. More specifically, it only focues on deep learning supervised methods that have gained great success in recent years. For knowing about latest advances of unsupervised methods in text classification, here is a recommended recently-published paper (Haj-Yahia, et al., 2019)."}
{"id": "https://wangcongcong123.github.io//Classification/_p3", "contents": "This blog amis to use three different deep leanring methods for text classification on three datasets. The code repository associated with this blog can be found here. Three datasets used in the blog are 5AbstractsGroup, SST-2 and reddit_jokes. These datasets are conditioned under license so you need to download them through their original sources."}
{"id": "https://wangcongcong123.github.io//Classification/_p4", "contents": "The structure of the blog is organized as follows."}
{"id": "https://wangcongcong123.github.io//Classification/_p5", "contents": "Table 1 gives details of the three datasets that are used in the blog. As seen from the table, the three datasets are different in types, including topic, sentiment and joke. They are also different in size, ranging from 6k to 155k. The diverse features make it more robust for experimental comparions between different methods later on."}
{"id": "https://wangcongcong123.github.io//Classification/_p6", "contents": "5AbstractsGroup\tis a topic-based datasets. It consists of 6246 examples. Indeed, this is not a big dataset for training a deep learning model. The examples are extracted from Web of Science and are a collection of academic papers with five different topics - business, artificial intelligence, sociology, transport and law. In this blog, I only use abstracts of these papers as training texts. The dataset is split into train/dev/test: 4996/624/626."}
{"id": "https://wangcongcong123.github.io//Classification/_p7", "contents": "SST-2 refers to Standford Sentiment Treeback 2 and it is a famous dataset in sentiment analysis. It consists of short text sequences that are manually anotated in terms of their sentiment polarity, namely, positive (1) or negative (0). This dataset with 68220 examples is relatively larger than 5AbstractsGroup. The standard dataset from the official website contains a training set (67349 examples) and a development set (871 examples). I keep the split scenario and evaluate on the development set so as to compare state-of-the-art performance in parallel (check GLUE benchmark leaderboard)."}
{"id": "https://wangcongcong123.github.io//Classification/_p8", "contents": "reddit_jokes is a collection of English plaintext jokes scraped from Reddit. Originally, it has 195k examples and each joke is assigned a post score indicating the joke\u2019s popularity or to say the extent of humor. I removed jokes with long texts and only keep those with words less than 50. Also, the title and body fields in the orignal datsets are combined to a single content field with <t/b> as separator. Because orginally the jokes are represented by a post score that is not suitable for text classification. I transformed the numeric scores into four categorical classes indicating four different levels of humor. The four classes are low, medium, high and crazy that are generated by setting up a score range. A joke that falls in a higher score range is assigned to a higher level of humor. After the preprocessing, 155027 examples are kept and subsequently split to train/dev/test: 124021/15502/15504."}
{"id": "https://wangcongcong123.github.io//Classification/_p9", "contents": "This section presents a popular recurrent nerual network (RNN) method - Bidirectional Long short-term Memory (BiLSTM) for classification on the three datasets. LSTM (Hochreiter, et al., 1997) is memory-based unit in RNN, which helps to summarize the state of previous  tokens in a sequence given a token at time step . For example, Figure 1 (a) shows the process of LSTM in RNN (picture credit: Zhang, et al., 2018). At time step , a token  can be represented by a vector  through an embedding layer. The LSTM unit takes  with previous  hidden states as input to output a new hidden state that summarizes important information of the sequence so far (from time step 0 to ). It summarizes important information by remembering or forgetting information through the recurrent process. It ends up with a vector that summarizes the whole sequence at the last time step. The last vector is normally used as representation input of subsequent feedforward layers for generating probabilities distribution over all classes. Unlike LSTM only summarizing information in one direction from left to right, BiLSTM sees context of a token at time step  in both left-to-right and right-to-left directions (as you can see from Figure 1 (b)). To better understand how LSTM/BiLSTM works, one point worth bearing in mind is that, in the recurrent process, all stuff are represented by vectors so that complicated mathematic functions can be leveraged on."}
{"id": "https://wangcongcong123.github.io//Classification/_p10", "contents": "If you are not so familar with LSTM and avid reading learner, here is a recommended learning route for LSTM."}
{"id": "https://wangcongcong123.github.io//Classification/_p11", "contents": "Know basics of neural network including computational graphs, perceptrons, training criterion, and activication functions, gradient descent optimazition methods and back propagation, etc."}
{"id": "https://wangcongcong123.github.io//Classification/_p12", "contents": "Definitely have a look at colah\u2019s blog on understanding LSTM networks where you find excitingly how mathematics leverage \u201cmemory\u201d on neural network"}
{"id": "https://wangcongcong123.github.io//Classification/_p13", "contents": "Next, it\u2019s recommended to go deeper into implementation level. Although there are many open souce libraries that you can rely on to implement LSTM in real-world applications. For me, I am interested in designing deep learning models for NLP tasks, so AllenNLP is highly recommended if you have the same interest. With AllenNLP, you will easily understand how matrix is transformed in a neural network model (I recommend you to do this through breakpoint debugging in IDE like PyCharm) . Not only this benefit, this library is beautifully designed for NLP researchers and enables you to customize a model flexiblely like lego stacking."}
{"id": "https://wangcongcong123.github.io//Classification/_p14", "contents": "Paper reading for research interests: Zhang, et al., 2018; Young, Tom, et al., 2018; Zhou, Peng, et al., 2016; Yang, Zichao, et al., 2016; Zhou, Chunting, et al., 2015 etc."}
{"id": "https://wangcongcong123.github.io//Classification/_p15", "contents": "Back to the blog\u2019s focus, the following graph presents the architecture of a simple BiLSTM model that are trained on the three datasets. It embeds the text first (through glove.6B.100d) and then encode it with a BiLSTM Encoder, and then pass the result through a feedforward network, the output is used as the scores for each label. The model is implemented with reference to AllenNLP\u2019s built-in basic classifier."}
{"id": "https://wangcongcong123.github.io//Classification/_p16", "contents": "In terms of experimental setup, examples are processed in batch of 64 and trained in 40 epochs with patience=5 for early stopping based on accuracy evaluation on development/validation set. Adam with learning rate 0.001 is used as optimizer and cross entropy is used as objective function. The experiment is run on a single nvidia RTX 2060 6GB GPU. With this GPU configuration and simplicity of the BiLSTM model, any one of the three experiments for each dataset is run no more than 3 minutes. The evaluation is reported on the predictions for test set except that SST is evaluated on development set."}
{"id": "https://wangcongcong123.github.io//Classification/_p17", "contents": "McCann, Bryan, et al. 2017 first implemented BCN (Biattentive Classification Network ) integrated with CoVe (Context Vectors) on many classification datasets. More recently, Peters, Matthew E., et al. 2018 in his ELMo (Em-beddings from Language Models) paper integrated with BCN achieves state-of-the-art (SOTA) performance in many language understanding tasks including classification task. As you can see from the leaderboard, the BCN+ELMo model achieved SOTA performance on SST-5 fine-grained sentiment classification task."}
{"id": "https://wangcongcong123.github.io//Classification/_p18", "contents": "Going deep into the model is beyond the scope of the blog. To know its internals, have a look at my previous blog that guides you to understand BCN+ELMo. For implementation, the model is designed based on a condensed version of the original one in AllenNLP. The original model is bigger where d is 300 (see the following graph), whereas I adapt it to be 100. In addition, the glove embedding I used is 6B.100d instead of 840B.300d as in the original. I change the model to be a smaller one in order to well suit my local hardware\u2019s capacity. If hardware like GPU or TPU are guaranteed, it is suggested to try the original that may give better performance. Basically, I adapted the BCN+ELMo structure to be as shown in the following graph. As presented in ELMo paper, there are four pre-trained ELMo models different in size. I used the original one with 93.6 millions. Check here to know different-size pre-trained ELMo models."}
{"id": "https://wangcongcong123.github.io//Classification/_p19", "contents": "Regarding the experimental setup, most of hyper parameters like learning rate, optimizer and number of epochs are kept as the same as in BiLSTM experiment. The only difference is that the batch size is changed from 64 to 16. Still, this is because the CUDA out-of-memory issue can be avoided if giving a relatively small batch size. Due to more complicated architecture of BCN+ELMo, the training time is a lot longer than BiLSTM. It is estimated that training BCN+ELMo on any of the three datasets is more than 30 minutes."}
{"id": "https://wangcongcong123.github.io//Classification/_p20", "contents": "BERT (Bidirectional Encoder Representations from Transformers) was proposed by Devlin, Jacob, et al., 2019 that has gained great attention in NLP community due to BERT\u2019s powerful performance in many sentence-level language tasks. BERT is similar to ELMo for building contextual representations. However, BERT is a Transformer-based architecture that only applies self-attention mechanism to pre-train a language model (Attention is all you need). Unlike ELMo that is trained on a LSTM-based Bi-LM (Bidirectional Language Model), BERT is trained with a different language modeling objective called masked language modeling. There are two different-size BERT pre-trained models in publication. The following table (credit: Munikar, et al., 2019) shows BERT_base VS. BERT_large. In experiment, I used the base version."}
{"id": "https://wangcongcong123.github.io//Classification/_p21", "contents": "If you are not so familar with BERT and avid reading learner, here is a recommended learning route for BERT."}
{"id": "https://wangcongcong123.github.io//Classification/_p22", "contents": "Get a good understanding of Google\u2019s Transformer (Vaswani, Ashish, et al., 2017) via the The Illustrated Transformer blog by Jay Alammar. This blog did a great job in explaining Transformer with nice visualization. Definitely worth checking if you want to know what is really going on behind Transformer."}
{"id": "https://wangcongcong123.github.io//Classification/_p23", "contents": "Next, I recommend Chris McCormick\u2019s tutorials on BERT embedding and Fine-Tuning. The tutorials bring you to understand BERT at implementation level. I like the tutorials because the author not only presents the code but also well explain why we need that code in details step by step."}
{"id": "https://wangcongcong123.github.io//Classification/_p24", "contents": "Go check \ud83e\udd17 Transformers (Wolf, Thomas, et al., 2019) developed by HuggingFace. It is nowadays a popular open source library that provides state-of-the-art general-purpose architectures (BERT, GPT-2, etc.) for Natural Language Understanding (NLU) and Natural Language Generation (NLG). You may notice a cute emoji \ud83e\udd17 before the name that distinguishes it from the commonly-known google Transformer. So don\u2019t be confused with the name. Actually, the library changed its name three times this year, from pytorch-pretrained-bert, pytorch-transformers, to current \ud83e\udd17 Transformers. From the name evolution, we can see it was originally developed for BERT implementation in PyTorch. Now it is expanded to support other state-of-the-art architectures (not just BERT) implemented in both Pytorch and TensorFlow 2.0. Hacking this library helps know how to adapt state-of-the-art models to your problem domain."}
{"id": "https://wangcongcong123.github.io//Classification/_p25", "contents": "At this stage, you may have questions like why BERT works so well or what future work can be done in the area. I suggest you to have further paper reading. Since BERT came up, many follow-up research has been done. You can find a summary of relevant papers here."}
{"id": "https://wangcongcong123.github.io//Classification/_p26", "contents": "In the experiment of using BERT to do classification, I adapted the BertForSequenceClassification model in \ud83e\udd17 Transformers to fine tune on my prepared three datasets. The train batch size is set to be 8 (you may change it to a proper number compatible with your hardware) and number of training epochs is 3. Other hyper parameters keep the same as default in  \ud83e\udd17 Transformers. Due to the fine-tunning process requiring only a small number of epochs in BERT, it doesn\u2019t take long time to finish training on the three datasets. In next section, I will report experimental results showing how different models influence performance given different dataset domains."}
{"id": "https://wangcongcong123.github.io//Classification/_p27", "contents": "After training BiLSTM, BCN+ELMo, BERT on 5AbstractsGroup, SST2, and reddit_jokes, evaluation is conducted on test set of 5AbstractsGroup and reddit_jokes, and development set of SST2. Evaluation metrics chosen for performance report include precision, recall, F1 and Accuracy. Table 2 gives exact these metrics\u2019 scores of the three models on 5AbstractsGroup, SST2, and redditjokes."}
{"id": "https://wangcongcong123.github.io//Classification/_p28", "contents": "Taking a look at performance of the three models on redditjokes, we found the results are poor and not reliable. I doute this is an example of wrongly defining a machien learning problem. I explain the issue with such a possible reason. The ground truth of redditjokes is orignally the post scores on Reddit. These scores only represent the public\u2019s opinions on a joke. This kind of subjective opinions break groundtruth so such a task is hard for machines to find patterns between classes. A lesson from this task: enabling machine the ability in identifying the level of humor given a plaintext human-written joke is what Fran\u00e7ois tweeted: \u201cFor all the progress made, it seems like almost all important questions in AI remain unanswered. Many have not even been properly asked yet\u201d. Anyway, I think how to enable machine with humor remains an interesting and serious research question in the future. Due to the mistake here, I will ignore discussing performance on redditjokes in the following part."}
{"id": "https://wangcongcong123.github.io//Classification/_p29", "contents": "Now, if we have a look at the evaluation scores of BiLSTM, BCN+ELMo, BERT  on 5AbstractsGroup, and SST2, we found BCN+ELMo improves the performance to a large extent in comparison to BiLSTM. This is likely due to the application of bi-attention mechanism in BCN+ELMo. Although BERT achieves the best performance among the three methods on both datasets, it only improves the performance slightly on 5AbstractsGroup compared to BCN+ELMo. However, BERT hits good scores in all metrics on SST2, with 0.9268 and 0.9266 in F1 and accuracy respectively. These scores are also competitive with state-of-the-art accuracy in GLUE benchmarking lederboard. The experiment with BERT only applies the small-version model and it is expected to further improve accuracy if applying the large-version model or further tunning hyper-parameters. It is also an interesting future work to compare performance by appling other state-of-the-art models such as XLNet, RoBERTa (that are easily accessible via the \ud83e\udd17 Transformers library). Overall, as the experimental results revealed, BERT has an advantage over the other two methods in the classifcation tasks. More specific evaluation report on each class of each dataset using each method can be accessed here."}
{"id": "https://wangcongcong123.github.io//Classification/_p30", "contents": "This blog presents three commonly-practised deep methods for text classification, namely, BiLSTM, BCN+ELMo, and BERT. Three datasets are given to test the performance of the three methods. Although methods like BERT nowadays have achieved a very good performance not only in text classification but also other language understanding tasks, many research problems still remain to be tackled in the future. For example, a recent hot topic around AI is GreenAI (Strubell, Emma, et al., 2019; Roy Schwartz, et al., 2019; Sanh, Victor, et al., 2019). GreenAI emphazies future research should not only blindly pursue accuracy but also care about carbon footprint produced by training big deep models. As the tweet puts: \u201cThe future is not about bigger models, it\u2019s about bigger ideas\u201d."}
{"id": "https://wangcongcong123.github.io//Classification/_p31", "contents": "For all the progress made, it seems like almost all important questions in AI remain unanswered. Many have not even been properly asked yet."}
{"id": "https://wangcongcong123.github.io//Classification/_p32", "contents": "Souce code: github repository"}
{"id": "https://wangcongcong123.github.io//Classification/_p33", "contents": "If you have any doubts or any my mistakes you found in the blog, send me an email via wangcongcongcc@gmail.com or you are welcome to talk with me about any NLP relevant questions through my Twitter account."}
{"id": "https://wangcongcong123.github.io//Chatbot/_p0", "contents": "Ph.D Candidate at CS, University College Dublin"}
{"id": "https://wangcongcong123.github.io//Chatbot/_p1", "contents": "In this post, I share the Chinese Room Thought Experiment that I played with kids in my Communication and Outreach course."}
{"id": "https://wangcongcong123.github.io//Chatbot/_p2", "contents": "In this activity, the children will learn about how chat bots work and how machine understands human language. This activity basically starts by demonstrating children how the popular chat bot Siri answers human questions. Then the so-called Chinese Room Thought experiment is conducted to ask the children (assume they do not know Chinese) to answer questions written in Chinese with only the rulebook as reference. The purpose of the demonstration and experiment comes to the theory explanation. In the end section, explanations on how machine like Siri understands human language and the limitations of machine understanding human language are summarised for the children."}
{"id": "https://wangcongcong123.github.io//Chatbot/_p3", "contents": "Demonstration"}
{"id": "https://wangcongcong123.github.io//Chatbot/_p4", "contents": "Demonstrate Siri to the Children and explain who Siri is. There is a pre-recorded video in the slides. In the video is Siri asked three questions (Figure 1): 1) What is Siri? 2) How old are you? 3) How do say hello in Chinese? Siri seems answer these questions very well. Some questions are thrown out for the children after the demonstration. For example, ask if they know how Siri understands human language. Next, an important question to ask the children is if they know speaking Chinese. Most children may say no honestly. However, it is worth taking a grain of salt at this moment to believe they really don\u2019t know Chinese. Here is how the following experiment comes to justify."}
{"id": "https://wangcongcong123.github.io//Chatbot/_p6", "contents": "Experiments"}
{"id": "https://wangcongcong123.github.io//Chatbot/_p7", "contents": "This is so called The Chinese Room Experiment. In this experiment, divide the children into groups with 2 in each. Each group is offered a rulebook which contains clues of Chinese questions to corresponding answers. There are two parts of the experiment. In the first part, ask the children to turn to page 1 of the rulebook and present them a question card written in Chinese. What the children need to do is to find the corresponding correct answer within 1 minutes with only the rulebook\u2019s page 1 as reference. It is easy to find the answer because the rulebook\u2019s first page contains question-answer pairs in the form of IF [a-Chinese-question], THEN [the answer]. They just need to match the answer with the one in the question card to sort out the correct one (Figure 2)."}
{"id": "https://wangcongcong123.github.io//Chatbot/_p9", "contents": "Next, the experiment goes to the second part which is slightly different from the first one. In this part, children need to turn to page 2 of the rulebook which contains clues of so-called Chinese cloze test - filling in the masked blank in a sentence only given the surrounding context words. Now a cloze test question card is presented to the Children. They sort out the question (pick the most likely one out of three choices to fill in the blank) with only the rulebook\u2019s page 2 as reference (Figure 3). After the two parts, it is the right time for me to say to them: I think you know Chinese because you answer my question cards so correctly. They are probably confused for what I am saying so. Now here is how the explanations come to its role."}
{"id": "https://wangcongcong123.github.io//Chatbot/_p11", "contents": "Explanation"}
{"id": "https://wangcongcong123.github.io//Chatbot/_p12", "contents": "The children actually do not know Chinese. The reason I think they know is because they answer my questions well. The secret is that they have the rulebook as reference, namely, they follow the written rules to answer my questions very well. This is exactly the same way how machines like Siri understand human language. That\u2019s to say machine understands human language in the way by summarising such a rulebook. How do machines summarise the rulebook? Simply speaking, they learn from large amounts of human-written texts. They learn the grammar rules and patterns in human language statistifcally from the texts. However, the sad news is that the perfect rule book has never existed. Human lanague is so complicated and evolved over time. Our brain are so intelligent so that we always come up with new words, new phrases, or innovative ways of expressing something. This makes machine really difficult to catch up with the human\u2019s intelligence. One interesting example is such a sentence: colorless green ideas sleep furiously. This sentence makes perfect sense in terms of grammar, but it is nonsense regarding the semnatics. To understand this level of meaning, machine expects further intelligence."}
{"id": "https://wangcongcong123.github.io//Chatbot/_p13", "contents": "To put a vision on the future of machine in understanding human language, it is not that near to the postive outcome, in my opinion. Hence, much work ahead is needed."}
{"id": "https://wangcongcong123.github.io//Chatbot/_p14", "contents": "Feedback is welcome!"}
{"id": "https://wangcongcong123.github.io//Chatbot/_p15", "contents": "If you have any doubts or any my mistakes you found in the blog, send me an email via wangcongcongcc@gmail.com or you are welcome to talk with me about any NLP relevant questions through my Twitter account."}
{"id": "https://wangcongcong123.github.io//AllenNLP/_p0", "contents": "Ph.D Candidate at CS, University College Dublin"}
{"id": "https://wangcongcong123.github.io//AllenNLP/_p1", "contents": "As I said, currently, I am a big fun of AI2. This began when I got knowing its work into an open-source NLP research library - AllenNLP. The more I hack the library, the more attractive it is for me. A short paragraph of praise below was written when I was hacking the tool."}
{"id": "https://wangcongcong123.github.io//AllenNLP/_p2", "contents": "I appreciate how the library strongly reflects the importance of reproducibility and conciseness regarding code design. It is not only a good example of my future code writing but also a motivation for me becoming an excellent researcher with the open-source concept in mind. The tool definitely helps me solve a lot of trivial time-consuming dirty work and enables me to focus on the most important part in research - logic and creative thinking."}
{"id": "https://wangcongcong123.github.io//AllenNLP/_p3", "contents": "If you aspire to be a NLP researcher, this wonderful library is highly recommended as starting point in terms of programing NLP. Anyway, it is a good tool for running reproducible experiments which I think is pretty much important for not only NLP reseachers but researchers in other areas. More about its contribution to reproducibility, the slides by Joel Grus are good references. To start AllenNLP, there are a number of good tutorials available on its git repository page."}
{"id": "https://wangcongcong123.github.io//AllenNLP/_p4", "contents": "Although I have played some time with AllenNLP on Mac OS and Linux, unfortunately, the library is still unavailable for Windows. Especially, it is an important need for me to conduct experiments with AllenNLPdirectly on my GPU-supported Windows. This blog provides a way of making a detour to enable AllenNLP run on Windows. Instructions are below."}
{"id": "https://wangcongcong123.github.io//AllenNLP/_p5", "contents": "Please go to the code repository, where readme file contains all details."}
{"id": "https://wangcongcong123.github.io//AllenNLP/_p6", "contents": "If you have any doubts or any my mistakes you found in the blog, send me an email via wangcongcongcc@gmail.com or you are welcome to talk with me about any NLP relevant questions through my Twitter account."}
{"id": "https://wangcongcong123.github.io//_p0", "contents": "Ph.D Candidate at CS, University College Dublin"}
{"id": "https://wangcongcong123.github.io//_p1", "contents": "TL;DR. This link provides the code repository that contains two readily downloadable fine-tuned GPT-2 weights, a quick start guide of how to customize Autocoder, and a list of future pointers to this project. Although this blog looks like a technical introduction to Autocoder, I also by the way talk about a lot of relevant stuff, such as nice work, status quo, and future directions in NLP."}
{"id": "https://wangcongcong123.github.io//_p2", "contents": "Human have walked into an uncharted territory since the outbreak of the COVID-19 coronavirus. Much effort has been made to stop the crisis. For example, in machine learning community, some people have been seeking computational techniques for extracting insights from COVID-19 literature, such as the COVID-19 Open Research Dataset Challenge (CORD-19), which was also mentioned in the news."}
{"id": "https://wangcongcong123.github.io//_p3", "contents": "This post guides you to write a python script that is able to monitor the open-access repository of electronic preprints (arXiv) for automatic post on Twitter, named Feeder-bot. Its workflow is illustrated as follows."}
{"id": "https://wangcongcong123.github.io//_p4", "contents": "Text classification as an important task in natural lanugage understanding (NLP) has been widely studied over the last several decades. The task describes input as a document and output as the category of which the document belongs to. In literature, both supervised and unsupervised methods have been applied for text classification."}
{"id": "https://wangcongcong123.github.io//_p5", "contents": "In this post, I share the Chinese Room Thought Experiment that I played with kids in my Communication and Outreach course."}
{"id": "https://wangcongcong123.github.io//_p6", "contents": "As I said, currently, I am a big fun of AI2. This began when I got knowing its work into an open-source NLP research library - AllenNLP. The more I hack the library, the more attractive it is for me. A short paragraph of praise below was written when I was hacking the tool."}
