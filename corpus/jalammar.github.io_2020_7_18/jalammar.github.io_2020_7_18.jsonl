{"id": "http://jalammar.github.io/_p0", "contents": "Visualizing machine learning one concept at a time.@JayAlammar on Twitter."}
{"id": "http://jalammar.github.io/_p1", "contents": "Check out the first video in my new series introducing the general public to AI and machine learning."}
{"id": "http://jalammar.github.io/_p2", "contents": "My aim for this series is to help people integrate ML into their world-view away from all the hype and overpromises that plauge the topic."}
{"id": "http://jalammar.github.io/_p3", "contents": "I had an incredible time organizing and speaking at the AI/machine learning track at QCon London 2020 where I invited and shared the stage with incredible speakers Vincent Warmerdam, Susanne Groothuis, Peter Elger, and Hien Luu."}
{"id": "http://jalammar.github.io/_p4", "contents": "QCon is a global software conference for software engineers, architects, and team leaders, with over 1,600 attendees in London. All speakers have a software background."}
{"id": "http://jalammar.github.io/_p5", "contents": "Translations: Russian"}
{"id": "http://jalammar.github.io/_p6", "contents": "Progress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents \u201cthe biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search\u201d."}
{"id": "http://jalammar.github.io/_p7", "contents": "This post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved."}
{"id": "http://jalammar.github.io/_p8", "contents": "Alongside this post, I\u2019ve prepared a notebook. You can see it here the notebook or run it on colab."}
{"id": "http://jalammar.github.io/_p9", "contents": "I had a great time speaking at the MIT Analytics Lab about some of my favorite ideas in natural language processing and their practical applications."}
{"id": "http://jalammar.github.io/_p10", "contents": "Discussions:\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\n"}
{"id": "http://jalammar.github.io/_p11", "contents": "Translations: Russian"}
{"id": "http://jalammar.github.io/_p12", "contents": "This year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn\u2019t a particularly novel architecture \u2013 it\u2019s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we\u2019ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we\u2019ll look at applications for the decoder-only transformer beyond language modeling."}
{"id": "http://jalammar.github.io/_p13", "contents": "My goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they\u2019ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve."}
{"id": "http://jalammar.github.io/_p14", "contents": "Discussions:\nHacker News (366 points, 21 comments), Reddit r/MachineLearning (256 points, 18 comments)\n\n\nTranslations: Chinese 1, Chinese 2, Japanese"}
{"id": "http://jalammar.github.io/_p15", "contents": "The NumPy package is the workhorse of data analysis, machine learning, and scientific computing in the python ecosystem. It vastly simplifies manipulating and crunching vectors and matrices. Some of python\u2019s leading package rely on NumPy as a fundamental piece of their infrastructure (examples include scikit-learn, SciPy, pandas, and tensorflow). Beyond the ability to slice and dice numeric data, mastering numpy will give you an edge when dealing and debugging with advanced usecases in these libraries."}
{"id": "http://jalammar.github.io/_p16", "contents": "In this post, we\u2019ll look at some of the main ways to use NumPy and how it can represent different types of data (tables, images, text\u2026etc) before we can serve them to machine learning models."}
{"id": "http://jalammar.github.io/_p17", "contents": "I gave a talk at Qcon London this year. Watch it here:"}
{"id": "http://jalammar.github.io/_p18", "contents": "Intuition & Use-Cases of Embeddings in NLP & beyond [YouTube]"}
{"id": "http://jalammar.github.io/_p19", "contents": "https://www.infoq.com/presentations/nlp-word-embedding/ [infoQ]"}
{"id": "http://jalammar.github.io/_p20", "contents": "In this video, I introduced word embeddings and the word2vec algorithm.  I then proceeded to discuss how the word2vec algorithm is used to create recommendation engines in companies like Airbnb and Alibaba. I close by glancing at real-world consequences of popular recommendation systems like those of YouTube and Facebook."}
{"id": "http://jalammar.github.io/_p21", "contents": "My Illustrated Word2vec post used and built on the materials I created for this talk (but didn\u2019t include anything on the recommender application of word2vec). This was my first talk at a technical conference and I spent quite a bit of time preparing for it. In the six weeks prior to the conference I spent about 100 hours working on the presentation and ended up with 200 slides. It was an interesting balancing act of trying to make it introductory but not shallow, suitable for senior engineers and architects yet not necessarily ones who have machine learning experience."}
{"id": "http://jalammar.github.io/_p22", "contents": " Discussions:\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\n\n\n\nTranslations: Chinese (Simplified), Korean, Portuguese, Russian\n"}
{"id": "http://jalammar.github.io/_p23", "contents": "I find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you\u2019ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you\u2019ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2)."}
{"id": "http://jalammar.github.io/_p24", "contents": "Word2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines."}
{"id": "http://jalammar.github.io/_p25", "contents": "In this post, we\u2019ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let\u2019s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?"}
{"id": "http://jalammar.github.io/_p26", "contents": "Discussions:\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\n\n\nTranslations: Chinese (Simplified), Japanese, Korean, Persian, Russian"}
{"id": "http://jalammar.github.io/_p27", "contents": "The year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It\u2019s been referred to as NLP\u2019s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks)."}
{"id": "http://jalammar.github.io/_p28", "contents": "Discussions:\nHacker News (195 points, 51 comments), Reddit r/Python (140 points, 18 comments)\n"}
{"id": "http://jalammar.github.io/_p29", "contents": "If you\u2019re planning to learn data analysis, machine learning, or data science tools in python, you\u2019re most likely going to be using the wonderful pandas library. Pandas is an open source library for data manipulation and analysis in python."}
{"id": "http://jalammar.github.io/_p30", "contents": "One of the easiest ways to think about that, is that you can load tables (and excel files) and then slice and dice them in multiple ways:"}
{"id": "http://jalammar.github.io/_p32", "contents": "Discussions:\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\n\n\nTranslations: Chinese (Simplified), Japanese, Korean, Russian\n\nWatch: MIT\u2019s Deep Learning State of the Art lecture referencing this post"}
{"id": "http://jalammar.github.io/_p33", "contents": "In the previous post, we looked at Attention \u2013 a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer \u2013 a model that uses attention to boost the speed with which these models can be trained. The Transformers outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud\u2019s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let\u2019s try to break the model apart and look at how it functions."}
{"id": "http://jalammar.github.io/_p34", "contents": "The Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard\u2019s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter."}
{"id": "http://jalammar.github.io/_p35", "contents": "Let\u2019s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another."}
{"id": "http://jalammar.github.io/_p36", "contents": "Translations: Chinese (Simplified), Japanese, Korean, Russian\n\nWatch: MIT\u2019s Deep Learning State of the Art lecture referencing this post"}
{"id": "http://jalammar.github.io/_p37", "contents": "May 25th update: New graphics (RNN animation, word embedding graph), color coding, elaborated on the final attention example."}
{"id": "http://jalammar.github.io/_p38", "contents": "Note: The animations below are videos. Touch or hover on them (if you\u2019re using a mouse) to get play controls so you can pause if needed."}
{"id": "http://jalammar.github.io/_p39", "contents": "Sequence-to-sequence models are deep learning models that have achieved a lot of success in tasks like machine translation, text summarization, and image captioning. Google Translate started using such a model in production in late 2016. These models are explained in the two pioneering papers (Sutskever et al., 2014, Cho et al., 2014)."}
{"id": "http://jalammar.github.io/_p40", "contents": "I found, however, that understanding the model well enough to implement it requires unraveling a series of concepts that build on top of each other. I thought that a bunch of these ideas would be more accessible if expressed visually. That\u2019s what I aim to do in this post. You\u2019ll need some previous understanding of deep learning to get through this post. I hope it can be a useful companion to reading the papers mentioned above (and the attention papers linked later in the post)."}
{"id": "http://jalammar.github.io/_p41", "contents": "A sequence-to-sequence model is a model that takes a sequence of items (words, letters, features of an images\u2026etc) and outputs another sequence of items. A trained model would work like this:"}
{"id": "http://jalammar.github.io/_p42", "contents": "I love using python\u2019s Pandas package for data analysis. The 10 Minutes to pandas is a great place to start learning how to use it for data analysis."}
{"id": "http://jalammar.github.io/_p43", "contents": "Things get a lot more interesting once you\u2019re comfortable with the fundamentals and start with Reshaping and Pivot Tables. That guide shows some of the more interesting functions of reshaping data. Below are some visualizations to go along with the Pandas reshaping guide."}
{"id": "http://jalammar.github.io/_p44", "contents": "In the previous post, we looked at the basic concepts of neural networks. Let us now take another example as an excuse to guide us to explore some of the basic mathematical ideas involved in prediction with neural networks."}
{"id": "http://jalammar.github.io/_p45", "contents": "Discussions:\nHacker News (63 points, 8 comments), Reddit r/programming (312 points, 37 comments)\n\nTranslations: Spanish\n"}
{"id": "http://jalammar.github.io/_p46", "contents": "Update: Part 2 is now live: A Visual And Interactive Look at Basic Neural Network Math"}
{"id": "http://jalammar.github.io/_p47", "contents": "I\u2019m not a machine learning expert. I\u2019m a software engineer by training and I\u2019ve had little interaction with AI. I had always wanted to delve deeper into machine learning, but never really found my \u201cin\u201d. That\u2019s why when Google open sourced TensorFlow in November 2015, I got super excited and knew it was time to jump in and start the learning journey. Not to sound dramatic, but to me, it actually felt kind of like Prometheus handing down fire to mankind from the Mount Olympus of machine learning. In the back of my head was the idea that the entire field of Big Data and technologies like Hadoop were vastly accelerated when Google researchers released their Map Reduce paper. This time it\u2019s not a paper \u2013 it\u2019s the actual software they use internally after years and years of evolution."}
{"id": "http://jalammar.github.io/_p48", "contents": "So I started learning what I can about the basics of the topic, and saw the need for gentler resources for people with no experience in the field. This is my attempt at that."}
{"id": "http://jalammar.github.io/_p49", "contents": "Discussion:\nReddit r/Android (80 points, 16 comments)\n"}
{"id": "http://jalammar.github.io/_p51", "contents": "In November 2015, Google announced and open sourced TensorFlow, its latest and greatest machine learning library. This is a big deal for three reasons:"}
{"id": "http://jalammar.github.io/_p52", "contents": "This last reason is the operating reason for this post since we\u2019ll be focusing on Android. If you examine the tensorflow repo on GitHub, you\u2019ll find a little  tensorflow/examples/android directory. I\u2019ll try to shed some light on the Android TensorFlow example and some of the things going on under the hood."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p0", "contents": "Visualizing machine learning one concept at a time.@JayAlammar on Twitter."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p1", "contents": "Discussions:\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\n\n\nTranslations: Chinese (Simplified), Japanese, Korean, Russian\n\nWatch: MIT\u2019s Deep Learning State of the Art lecture referencing this post"}
{"id": "http://jalammar.github.io//illustrated-transformer/_p2", "contents": "In the previous post, we looked at Attention \u2013 a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer \u2013 a model that uses attention to boost the speed with which these models can be trained. The Transformers outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud\u2019s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let\u2019s try to break the model apart and look at how it functions."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p3", "contents": "The Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard\u2019s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p4", "contents": "Let\u2019s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p5", "contents": "Popping open that Optimus Prime goodness, we see an encoding component, a decoding component, and connections between them."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p6", "contents": "The encoding component is a stack of encoders (the paper stacks six of them on top of each other \u2013 there\u2019s nothing magical about the number six, one can definitely experiment with other arrangements). The decoding component is a stack of decoders of the same number."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p7", "contents": "The encoders are all identical in structure (yet they do not share weights). Each one is broken down into two sub-layers:"}
{"id": "http://jalammar.github.io//illustrated-transformer/_p8", "contents": "The encoder\u2019s inputs first flow through a self-attention layer \u2013 a layer that helps the encoder look at other words in the input sentence as it encodes a specific word. We\u2019ll look closer at self-attention later in the post."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p9", "contents": "The outputs of the self-attention layer are fed to a feed-forward neural network. The exact same feed-forward network is independently applied to each position."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p10", "contents": "The decoder has both those layers, but between them is an attention layer that helps the decoder focus on relevant parts of the input sentence (similar what attention does in seq2seq models)."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p11", "contents": "Now that we\u2019ve seen the major components of the model, let\u2019s start to look at the various vectors/tensors and how they flow between these components to turn the input of a trained model into an output."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p12", "contents": "As is the case in NLP applications in general, we begin by turning each input word into a vector using an embedding algorithm."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p14", "contents": "The embedding only happens in the bottom-most encoder. The abstraction that is common to all the encoders is that they receive a list of vectors each of the size 512 \u2013 In the bottom encoder that would be the word embeddings, but in other encoders, it would be the output of the encoder that\u2019s directly below. The size of this list is hyperparameter we can set \u2013 basically it would be the length of the longest sentence in our training dataset."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p15", "contents": "After embedding the words in our input sequence, each of them flows through each of the two layers of the encoder."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p16", "contents": "Here we begin to see one key property of the Transformer, which is that the word in each position flows through its own path in the encoder. There are dependencies between these paths in the self-attention layer. The feed-forward layer does not have those dependencies, however, and thus the various paths can be executed in parallel while flowing through the feed-forward layer."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p17", "contents": "Next, we\u2019ll switch up the example to a shorter sentence and we\u2019ll look at what happens in each sub-layer of the encoder."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p18", "contents": "As we\u2019ve mentioned already, an encoder receives a list of vectors as input. It processes this list by passing these vectors into a \u2018self-attention\u2019 layer, then into a feed-forward neural network, then sends out the output upwards to the next encoder."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p19", "contents": "Don\u2019t be fooled by me throwing around the word \u201cself-attention\u201d like it\u2019s a concept everyone should be familiar with. I had personally never came across the concept until reading the Attention is All You Need paper. Let us distill how it works."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p20", "contents": "Say the following sentence is an input sentence we want to translate:"}
{"id": "http://jalammar.github.io//illustrated-transformer/_p21", "contents": "\u201dThe animal didn't cross the street because it was too tired\u201d"}
{"id": "http://jalammar.github.io//illustrated-transformer/_p22", "contents": "What does \u201cit\u201d in this sentence refer to? Is it referring to the street or to the animal? It\u2019s a simple question to a human, but not as simple to an algorithm."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p23", "contents": "When the model is processing the word \u201cit\u201d, self-attention allows it to associate \u201cit\u201d with \u201canimal\u201d."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p24", "contents": "As the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p25", "contents": "If you\u2019re familiar with RNNs, think of how maintaining a hidden state allows an RNN to incorporate its representation of previous words/vectors it has processed with the current one it\u2019s processing. Self-attention is the method the Transformer uses to bake the \u201cunderstanding\u201d of other relevant words into the one we\u2019re currently processing."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p26", "contents": "Be sure to check out the Tensor2Tensor notebook where you can load a Transformer model, and examine it using this interactive visualization."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p27", "contents": "Let\u2019s first look at how to calculate self-attention using vectors, then proceed to look at how it\u2019s actually implemented \u2013 using matrices."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p28", "contents": "The first step in calculating self-attention is to create three vectors from each of the encoder\u2019s input vectors (in this case, the embedding of each word). So for each word, we create a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that we trained during the training process."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p29", "contents": "Notice that these new vectors are smaller in dimension than the embedding vector. Their dimensionality is 64, while the embedding and encoder input/output vectors have dimensionality of 512. They don\u2019t HAVE to be smaller, this is an architecture choice to make the computation of multiheaded attention (mostly) constant."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p32", "contents": "What are the \u201cquery\u201d, \u201ckey\u201d, and \u201cvalue\u201d vectors?\n\n\nThey\u2019re abstractions that are useful for calculating and thinking about attention. Once you proceed with reading how attention is calculated below, you\u2019ll know pretty much all you need to know about the role each of these vectors plays."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p33", "contents": "The second step in calculating self-attention is to calculate a score. Say we\u2019re calculating the self-attention for the first word in this example, \u201cThinking\u201d. We need to score each word of the input sentence against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p34", "contents": "The score is calculated by taking the dot product of the query vector with the key vector of the respective word we\u2019re scoring. So if we\u2019re processing the self-attention for the word in position #1, the first score would be the dot product of q1 and k1. The second score would be the dot product of q1 and k2."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p37", "contents": "The third and forth steps are to divide the scores by 8 (the square root of the dimension of the key vectors used in the paper \u2013 64. This leads to having more stable gradients. There could be other possible values here, but this is the default), then pass the result through a softmax operation. Softmax normalizes the scores so they\u2019re all positive and add up to 1."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p39", "contents": "This softmax score determines how much each word will be expressed at this position. Clearly the word at this position will have the highest softmax score, but sometimes it\u2019s useful to attend to another word that is relevant to the current word."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p41", "contents": "The fifth step is to multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example)."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p42", "contents": "The sixth step is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word)."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p44", "contents": "That concludes the self-attention calculation. The resulting vector is one we can send along to the feed-forward neural network. In the actual implementation, however, this calculation is done in matrix form for faster processing. So let\u2019s look at that now that we\u2019ve seen the intuition of the calculation on the word level."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p45", "contents": "The first step is to calculate the Query, Key, and Value matrices. We do that by packing our embeddings into a matrix X, and multiplying it by the weight matrices we\u2019ve trained (WQ, WK, WV)."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p47", "contents": "Finally, since we\u2019re dealing with matrices, we can condense steps two through six in one formula to calculate the outputs of the self-attention layer."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p50", "contents": "The paper further refined the self-attention layer by adding a mechanism called \u201cmulti-headed\u201d attention. This improves the performance of the attention layer in two ways:"}
{"id": "http://jalammar.github.io//illustrated-transformer/_p51", "contents": "It expands the model\u2019s ability to focus on different positions. Yes, in the example above, z1 contains a little bit of every other encoding, but it could be dominated by the the actual word itself. It would be useful if we\u2019re translating a sentence like \u201cThe animal didn\u2019t cross the street because it was too tired\u201d, we would want to know which word \u201cit\u201d refers to."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p52", "contents": "It gives the attention layer multiple \u201crepresentation subspaces\u201d. As we\u2019ll see next, with multi-headed attention we have not only one, but multiple sets of Query/Key/Value weight matrices (the Transformer uses eight attention heads, so we end up with eight sets for each encoder/decoder). Each of these sets is randomly initialized. Then, after training, each set is used to project the input embeddings (or vectors from lower encoders/decoders) into a different representation subspace."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p53", "contents": "\nIf we do the same self-attention calculation we outlined above, just eight different times with different weight matrices, we end up with eight different Z matrices"}
{"id": "http://jalammar.github.io//illustrated-transformer/_p55", "contents": "This leaves us with a bit of a challenge. The feed-forward layer is not expecting eight matrices \u2013 it\u2019s expecting a single matrix (a vector for each word). So we need a way to condense these eight down into a single matrix."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p56", "contents": "How do we do that? We concat the matrices then multiple them by an additional weights matrix WO."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p57", "contents": "That\u2019s pretty much all there is to multi-headed self-attention. It\u2019s quite a handful of matrices, I realize. Let me try to put them all in one visual so we can look at them in one place"}
{"id": "http://jalammar.github.io//illustrated-transformer/_p60", "contents": "Now that we have touched upon attention heads, let\u2019s revisit our example from before to see where the different attention heads are focusing as we encode the word \u201cit\u201d in our example sentence:"}
{"id": "http://jalammar.github.io//illustrated-transformer/_p62", "contents": "If we add all the attention heads to the picture, however, things can be harder to interpret:"}
{"id": "http://jalammar.github.io//illustrated-transformer/_p63", "contents": "One thing that\u2019s missing from the model as we have described it so far is a way to account for the order of the words in the input sequence."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p64", "contents": "To address this, the transformer adds a vector to each input embedding. These vectors follow a specific pattern that the model learns, which helps it determine the position of each word, or the distance between different words in the sequence. The intuition here is that adding these values to the embeddings provides meaningful distances between the embedding vectors once they\u2019re projected into Q/K/V vectors and during dot-product attention."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p67", "contents": "If we assumed the embedding has a dimensionality of 4, the actual positional encodings would look like this:"}
{"id": "http://jalammar.github.io//illustrated-transformer/_p69", "contents": "What might this pattern look like?"}
{"id": "http://jalammar.github.io//illustrated-transformer/_p70", "contents": "In the following figure, each row corresponds the a positional encoding of a vector. So the first row would be the vector we\u2019d add to the embedding of the first word in an input sequence. Each row contains 512 values \u2013 each with a value between 1 and -1. We\u2019ve color-coded them so the pattern is visible."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p71", "contents": "The formula for positional encoding is described in the paper (section 3.5). You can see the code for generating positional encodings in get_timing_signal_1d(). This is not the only possible method for positional encoding. It, however, gives the advantage of being able to scale to unseen lengths of sequences (e.g. if our trained model is asked to translate a sentence longer than any of those in our training set)."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p72", "contents": "July 2020 Update: \nThe positional encoding shown above is from the Tranformer2Transformer implementation of the Transformer. The method shown in the paper is slightly different in that it doesn\u2019t directly concatenate, but interweaves the two signals. The following figure shows what that looks like. Here\u2019s the code to generate it:"}
{"id": "http://jalammar.github.io//illustrated-transformer/_p73", "contents": "One detail in the architecture of the encoder that we need to mention before moving on, is that each sub-layer (self-attention, ffnn) in each encoder has a residual connection around it, and is followed by a layer-normalization step."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p74", "contents": "If we\u2019re to visualize the vectors and the layer-norm operation associated with self attention, it would look like this:"}
{"id": "http://jalammar.github.io//illustrated-transformer/_p75", "contents": "This goes for the sub-layers of the decoder as well. If we\u2019re to think of a Transformer of 2 stacked encoders and decoders, it would look something like this:"}
{"id": "http://jalammar.github.io//illustrated-transformer/_p76", "contents": "Now that we\u2019ve covered most of the concepts on the encoder side, we basically know how the components of decoders work as well. But let\u2019s take a look at how they work together."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p77", "contents": "The encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder in its \u201cencoder-decoder attention\u201d layer which helps the decoder focus on appropriate places in the input sequence:"}
{"id": "http://jalammar.github.io//illustrated-transformer/_p78", "contents": "The following steps repeat the process until a special  symbol is reached indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did. And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p79", "contents": "The self attention layers in the decoder operate in a slightly different way than the one in the encoder:"}
{"id": "http://jalammar.github.io//illustrated-transformer/_p80", "contents": "In the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions (setting them to -inf) before the softmax step in the self-attention calculation."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p81", "contents": "The \u201cEncoder-Decoder Attention\u201d layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p82", "contents": "The decoder stack outputs a vector of floats. How do we turn that into a word? That\u2019s the job of the final Linear layer which is followed by a Softmax Layer."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p83", "contents": "The Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p84", "contents": "Let\u2019s assume that our model knows 10,000 unique English words (our model\u2019s \u201coutput vocabulary\u201d) that it\u2019s learned from its training dataset. This would make the logits vector 10,000 cells wide \u2013 each cell corresponding to the score of a unique word. That is how we interpret the output of the model followed by the Linear layer."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p85", "contents": "The softmax layer then turns those scores into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p88", "contents": "Now that we\u2019ve covered the entire forward-pass process through a trained Transformer, it would be useful to glance at the intuition of training the model."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p89", "contents": "During training, an untrained model would go through the exact same forward pass. But since we are training it on a labeled training dataset, we can compare its output with the actual correct output."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p90", "contents": "To visualize this, let\u2019s assume our output vocabulary only contains six words(\u201ca\u201d, \u201cam\u201d, \u201ci\u201d, \u201cthanks\u201d, \u201cstudent\u201d, and \u201c<eos>\u201d (short for \u2018end of sentence\u2019))."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p91", "contents": "Once we define our output vocabulary, we can use a vector of the same width to indicate each word in our vocabulary. This also known as one-hot encoding. So for example, we can indicate the word \u201cam\u201d using the following vector:"}
{"id": "http://jalammar.github.io//illustrated-transformer/_p92", "contents": "Following this recap, let\u2019s discuss the model\u2019s loss function \u2013 the metric we are optimizing during the training phase to lead up to a trained and hopefully amazingly accurate model."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p93", "contents": "Say we are training our model. Say it\u2019s our first step in the training phase, and we\u2019re training it on a simple example \u2013 translating \u201cmerci\u201d into \u201cthanks\u201d."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p94", "contents": "What this means, is that we want the output to be a probability distribution indicating the word \u201cthanks\u201d. But since this model is not yet trained, that\u2019s unlikely to happen just yet."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p96", "contents": "How do you compare two probability distributions? We simply subtract one from the other. For more details, look at  cross-entropy and Kullback\u2013Leibler divergence."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p97", "contents": "But note that this is an oversimplified example. More realistically, we\u2019ll use a sentence longer than one word. For example \u2013 input: \u201cje suis \u00e9tudiant\u201d and expected output: \u201ci am a student\u201d. What this really means, is that we want our model to successively output probability distributions where:"}
{"id": "http://jalammar.github.io//illustrated-transformer/_p99", "contents": "After training the model for enough time on a large enough dataset, we would hope the produced probability distributions would look like this:"}
{"id": "http://jalammar.github.io//illustrated-transformer/_p100", "contents": "Now, because the model produces the outputs one at a time, we can assume that the model is selecting the word with the highest probability from that probability distribution and throwing away the rest. That\u2019s one way to do it (called greedy decoding). Another way to do it would be to hold on to, say, the top two words (say, \u2018I\u2019 and \u2018a\u2019 for example), then in the next step, run the model twice: once assuming the first output position was the word \u2018I\u2019, and another time assuming the first output position was the word \u2018a\u2019, and whichever version produced less error considering both positions #1 and #2 is kept. We repeat this for positions #2 and #3\u2026etc. This method is called \u201cbeam search\u201d, where in our example, beam_size was two (meaning that at all times, two partial hypotheses (unfinished translations) are kept in memory), and top_beams is also two (meaning we\u2019ll return two translations). These are both hyperparameters that you can experiment with."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p101", "contents": "I hope you\u2019ve found this a useful place to start to break the ice with the major concepts of the Transformer. If you want to go deeper, I\u2019d suggest these next steps:"}
{"id": "http://jalammar.github.io//illustrated-transformer/_p102", "contents": "Follow-up works:"}
{"id": "http://jalammar.github.io//illustrated-transformer/_p103", "contents": "Thanks to Illia Polosukhin, Jakob Uszkoreit, Llion Jones , Lukasz Kaiser, Niki Parmar, and Noam Shazeer for providing feedback on earlier versions of this post."}
{"id": "http://jalammar.github.io//illustrated-transformer/_p104", "contents": "Please hit me up on Twitter for any corrections or feedback."}
{"id": "http://jalammar.github.io//illustrated-bert/_p0", "contents": "Visualizing machine learning one concept at a time.@JayAlammar on Twitter."}
{"id": "http://jalammar.github.io//illustrated-bert/_p1", "contents": "Discussions:\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\n\n\nTranslations: Chinese (Simplified), Japanese, Korean, Persian, Russian"}
{"id": "http://jalammar.github.io//illustrated-bert/_p2", "contents": "The year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It\u2019s been referred to as NLP\u2019s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks)."}
{"id": "http://jalammar.github.io//illustrated-bert/_p3", "contents": "(ULM-FiT has nothing to do with Cookie Monster. But I couldn\u2019t think of anything else..)"}
{"id": "http://jalammar.github.io//illustrated-bert/_p4", "contents": "One of the latest milestones in this development is the release of BERT, an event described as marking the beginning of a new era in NLP. BERT is a model that broke several records for how well models can handle language-based tasks. Soon after the release of the paper describing the model, the team also open-sourced the code of the model, and made available for download versions of the model that were already pre-trained on massive datasets. This is a momentous development since it enables anyone building a machine learning model involving language processing to use this powerhouse as a readily-available component \u2013 saving the time, energy, knowledge, and resources that would have gone to training a language-processing model from scratch."}
{"id": "http://jalammar.github.io//illustrated-bert/_p5", "contents": "BERT builds on top of a number of clever ideas that have been bubbling up in the NLP community recently \u2013 including but not limited to Semi-supervised Sequence Learning (by Andrew Dai and Quoc Le), ELMo (by Matthew Peters and researchers from AI2 and UW CSE), ULMFiT (by fast.ai founder Jeremy Howard and Sebastian Ruder), the OpenAI transformer (by OpenAI researchers Radford, Narasimhan, Salimans, and Sutskever), and the Transformer (Vaswani et al)."}
{"id": "http://jalammar.github.io//illustrated-bert/_p6", "contents": "There are a number of concepts one needs to be aware of to properly wrap one\u2019s head around what BERT is. So let\u2019s start by looking at ways you can use BERT before looking at the concepts involved in the model itself."}
{"id": "http://jalammar.github.io//illustrated-bert/_p7", "contents": "The most straight-forward way to use BERT is to use it to classify a single piece of text. This model would look like this:"}
{"id": "http://jalammar.github.io//illustrated-bert/_p9", "contents": "To train such a model, you mainly have to train the classifier, with minimal changes happening to the BERT model during the training phase. This training process is called Fine-Tuning, and has roots in Semi-supervised Sequence Learning and ULMFiT."}
{"id": "http://jalammar.github.io//illustrated-bert/_p10", "contents": "For people not versed in the topic, since we\u2019re talking about classifiers, then we are in the supervised-learning domain of machine learning. Which would mean we need a labeled dataset to train such a model. For this spam classifier example, the labeled dataset would be a list of email messages and a label (\u201cspam\u201d or \u201cnot spam\u201d for each message)."}
{"id": "http://jalammar.github.io//illustrated-bert/_p11", "contents": "Other examples for such a use-case include:"}
{"id": "http://jalammar.github.io//illustrated-bert/_p12", "contents": "Now that you have an example use-case in your head for how BERT can be used, let\u2019s take a closer look at how it works."}
{"id": "http://jalammar.github.io//illustrated-bert/_p14", "contents": "The paper presents two model sizes for BERT:"}
{"id": "http://jalammar.github.io//illustrated-bert/_p15", "contents": "BERT is basically a trained Transformer Encoder stack. This is a good time to direct you to read my earlier post The Illustrated Transformer which explains the Transformer model \u2013 a foundational concept for BERT and the concepts we\u2019ll discuss next."}
{"id": "http://jalammar.github.io//illustrated-bert/_p17", "contents": "Both BERT model sizes have a large number of encoder layers (which the paper calls Transformer Blocks) \u2013 twelve for the Base version, and twenty four for the Large version. These also have larger feedforward-networks (768 and 1024 hidden units respectively), and more attention heads (12 and 16 respectively) than the default configuration in the reference implementation of the Transformer in the initial paper (6 encoder layers, 512 hidden units, and 8 attention heads)."}
{"id": "http://jalammar.github.io//illustrated-bert/_p19", "contents": "The first input token is supplied with a special [CLS] token for reasons that will become apparent later on. CLS here stands for Classification."}
{"id": "http://jalammar.github.io//illustrated-bert/_p20", "contents": "Just like the vanilla encoder of the transformer, BERT takes a sequence of words as input which keep flowing up the stack. Each layer applies self-attention, and passes its results through a feed-forward network, and then hands it off to the next encoder."}
{"id": "http://jalammar.github.io//illustrated-bert/_p22", "contents": "In terms of architecture, this has been identical to the Transformer up until this point (aside from size, which are just configurations we can set). It is at the output that we first start seeing how things diverge."}
{"id": "http://jalammar.github.io//illustrated-bert/_p23", "contents": "Each position outputs a vector of size hidden_size (768 in BERT Base). For the sentence classification example we\u2019ve looked at above, we focus on the output of only the first position (that we passed the special [CLS] token to)."}
{"id": "http://jalammar.github.io//illustrated-bert/_p25", "contents": "That vector can now be used as the input for a classifier of our choosing. The paper achieves great results by just using a single-layer neural network as the classifier."}
{"id": "http://jalammar.github.io//illustrated-bert/_p27", "contents": "If you have more labels (for example if you\u2019re an email service that tags emails with \u201cspam\u201d, \u201cnot spam\u201d, \u201csocial\u201d, and \u201cpromotion\u201d), you just tweak the classifier network to have more output neurons that then pass through softmax."}
{"id": "http://jalammar.github.io//illustrated-bert/_p28", "contents": "For those with a background in computer vision, this vector hand-off should be reminiscent of what happens between the convolution part of a network like VGGNet and the fully-connected classification portion at the end of the network."}
{"id": "http://jalammar.github.io//illustrated-bert/_p30", "contents": "These new developments carry with them a new shift in how words are encoded. Up until now, word-embeddings have been a major force in how leading NLP models deal with language. Methods like Word2Vec and Glove have been widely used for such tasks. Let\u2019s recap how those are used before pointing to what has now changed."}
{"id": "http://jalammar.github.io//illustrated-bert/_p31", "contents": "For words to be processed by machine learning models, they need some form of numeric representation that models can use in their calculation. Word2Vec showed that we can use a vector (a list of numbers) to properly represent words in a way that captures semantic or meaning-related relationships (e.g. the ability to tell if words are similar, or opposites, or that a pair of words like \u201cStockholm\u201d and \u201cSweden\u201d have the same relationship between them as \u201cCairo\u201d and \u201cEgypt\u201d have between them) as well as syntactic, or grammar-based, relationships (e.g. the relationship between \u201chad\u201d and \u201chas\u201d is the same as that between \u201cwas\u201d and \u201cis\u201d)."}
{"id": "http://jalammar.github.io//illustrated-bert/_p32", "contents": "The field quickly realized it\u2019s a great idea to use embeddings that were pre-trained on vast amounts of text data instead of training them alongside the model on what was frequently a small dataset.  So it became possible to download a list of words and their embeddings generated by pre-training with Word2Vec or GloVe. This is an example of the GloVe embedding of the word \u201cstick\u201d (with an embedding vector size of 200)"}
{"id": "http://jalammar.github.io//illustrated-bert/_p33", "contents": "Since these are large and full of numbers, I use the following basic shape in the figures in my posts to show vectors:"}
{"id": "http://jalammar.github.io//illustrated-bert/_p34", "contents": "If we\u2019re using this GloVe representation, then the word \u201cstick\u201d would be represented by this vector no-matter what the context was. \u201cWait a minute\u201d said a number of NLP researchers (Peters et. al., 2017, McCann et. al., 2017, and yet again Peters et. al., 2018 in the ELMo paper ), \u201cstick\u201d\u201d has multiple meanings depending on where it\u2019s used. Why not give it an embedding based on the context it\u2019s used in \u2013 to both capture the word meaning in that context as well as other contextual information?\u201d. And so, contextualized word-embeddings were born."}
{"id": "http://jalammar.github.io//illustrated-bert/_p35", "contents": "Instead of using a fixed embedding for each word, ELMo looks at the entire sentence before assigning each word in it an embedding. It uses a bi-directional LSTM trained on a specific task to be able to create those embeddings."}
{"id": "http://jalammar.github.io//illustrated-bert/_p36", "contents": "ELMo provided a significant step towards pre-training in the context of NLP. The ELMo LSTM would be trained on a massive dataset in the language of our dataset, and then we can use it as a component in other models that need to handle language."}
{"id": "http://jalammar.github.io//illustrated-bert/_p37", "contents": "What\u2019s ELMo\u2019s secret?"}
{"id": "http://jalammar.github.io//illustrated-bert/_p38", "contents": "ELMo gained its language understanding from being trained to predict the next word in a sequence of words - a task called Language Modeling. This is convenient because we have vast amounts of text data that such a model can learn from without needing labels."}
{"id": "http://jalammar.github.io//illustrated-bert/_p39", "contents": "\n\n  A step in the pre-training process of ELMo: Given \u201cLet\u2019s stick to\u201d as input, predict the next most likely word \u2013 a language modeling task. When trained on a large dataset, the model starts to pick up on language patterns. It\u2019s unlikely it\u2019ll accurately guess the next word in this example. More realistically, after a word such as \u201chang\u201d, it will assign a higher probability to a word like \u201cout\u201d (to spell \u201chang out\u201d) than to \u201ccamera\u201d."}
{"id": "http://jalammar.github.io//illustrated-bert/_p40", "contents": "We can see the hidden state of each unrolled-LSTM step peaking out from behind ELMo\u2019s head. Those come in handy in the embedding process after this pre-training is done."}
{"id": "http://jalammar.github.io//illustrated-bert/_p41", "contents": "ELMo actually goes a step further and trains a bi-directional LSTM \u2013 so that its language model doesn\u2019t only have a sense of the next word, but also the previous word."}
{"id": "http://jalammar.github.io//illustrated-bert/_p42", "contents": "ELMo comes up with the contextualized embedding through grouping together the hidden states (and initial embedding) in a certain way (concatenation followed by weighted summation)."}
{"id": "http://jalammar.github.io//illustrated-bert/_p43", "contents": "ULM-FiT introduced methods to effectively utilize a lot of what the model learns during pre-training \u2013 more than just embeddings, and more than contextualized embeddings. ULM-FiT introduced a language model and a process to effectively fine-tune that language model for various tasks."}
{"id": "http://jalammar.github.io//illustrated-bert/_p44", "contents": "NLP finally had a way to do transfer learning probably as well as Computer Vision could."}
{"id": "http://jalammar.github.io//illustrated-bert/_p45", "contents": "The release of the Transformer paper and code, and the results it achieved on tasks such as machine translation started to make some in the field think of them as a replacement to LSTMs. This was compounded by the fact that Transformers deal with long-term dependancies better than LSTMs."}
{"id": "http://jalammar.github.io//illustrated-bert/_p46", "contents": "The Encoder-Decoder structure of the transformer made it perfect for machine translation. But how would you use it for sentence classification? How would you use it to pre-train a language model that can be fine-tuned for other tasks (downstream tasks is what the field calls those supervised-learning tasks that utilize a pre-trained model or component)."}
{"id": "http://jalammar.github.io//illustrated-bert/_p47", "contents": "It turns out we don\u2019t need an entire Transformer to adopt transfer learning and a fine-tunable language model for NLP tasks. We can do with just the decoder of the transformer. The decoder is a good choice because it\u2019s a natural choice for language modeling (predicting the next word) since it\u2019s built to mask future tokens \u2013 a valuable feature when it\u2019s generating a translation word by word."}
{"id": "http://jalammar.github.io//illustrated-bert/_p48", "contents": "The model stacked twelve decoder layers. Since there is no encoder in this set up, these decoder layers would not have the encoder-decoder attention sublayer that vanilla transformer decoder layers have. It would still have the self-attention layer, however (masked so it doesn\u2019t peak at future tokens)."}
{"id": "http://jalammar.github.io//illustrated-bert/_p49", "contents": "With this structure, we can proceed to train the model on the same language modeling task: predict the next word using massive (unlabeled) datasets. Just, throw the text of 7,000 books at it and have it learn! Books are great for this sort of task since it allows the model to learn to associate related information even if they\u2019re separated by a lot of text \u2013 something you don\u2019t get for example, when you\u2019re training with tweets, or articles."}
{"id": "http://jalammar.github.io//illustrated-bert/_p50", "contents": "Now that the OpenAI transformer is pre-trained and its layers have been tuned to reasonably handle language, we can start using it for downstream tasks. Let\u2019s first look at sentence classification (classify an email message as \u201cspam\u201d or \u201cnot spam\u201d):"}
{"id": "http://jalammar.github.io//illustrated-bert/_p51", "contents": "The OpenAI paper outlines a number of input transformations to handle the inputs for different types of tasks. The following image from the paper shows the structures of the models and input transformations to carry out different tasks."}
{"id": "http://jalammar.github.io//illustrated-bert/_p52", "contents": "Isn\u2019t that clever?"}
{"id": "http://jalammar.github.io//illustrated-bert/_p53", "contents": "The openAI transformer gave us a fine-tunable pre-trained model based on the Transformer. But something went missing in this transition from LSTMs to Transformers. ELMo\u2019s language model was bi-directional, but the openAI transformer only trains a forward language model. Could we build a transformer-based model whose language model looks both forward and backwards (in the technical jargon \u2013 \u201cis conditioned on both left and right context\u201d)?"}
{"id": "http://jalammar.github.io//illustrated-bert/_p54", "contents": "\u201cHold my beer\u201d, said R-rated BERT."}
{"id": "http://jalammar.github.io//illustrated-bert/_p55", "contents": "\u201cWe\u2019ll use transformer encoders\u201d, said BERT."}
{"id": "http://jalammar.github.io//illustrated-bert/_p56", "contents": "\u201cThis is madness\u201d, replied Ernie, \u201cEverybody knows bidirectional conditioning would allow each word to indirectly see itself in a multi-layered context.\u201d"}
{"id": "http://jalammar.github.io//illustrated-bert/_p57", "contents": "\u201cWe\u2019ll use masks\u201d, said BERT confidently."}
{"id": "http://jalammar.github.io//illustrated-bert/_p58", "contents": "Finding the right task to train a Transformer stack of encoders is a complex hurdle that BERT resolves by adopting a \u201cmasked language model\u201d concept from earlier literature (where it\u2019s called a Cloze task)."}
{"id": "http://jalammar.github.io//illustrated-bert/_p59", "contents": "Beyond masking 15% of the input, BERT also mixes things a bit in order to improve how the model later fine-tunes. Sometimes it randomly replaces a word with another word and asks the model to predict the correct word in that position."}
{"id": "http://jalammar.github.io//illustrated-bert/_p60", "contents": "If you look back up at the input transformations the OpenAI transformer does to handle different tasks, you\u2019ll notice that some tasks require the model to say something intelligent about two sentences (e.g. are they simply paraphrased versions of each other? Given a wikipedia entry as input, and a question regarding that entry as another input, can we answer that question?)."}
{"id": "http://jalammar.github.io//illustrated-bert/_p61", "contents": "To make BERT better at handling relationships between multiple sentences, the pre-training process includes an additional task: Given two sentences (A and B), is B likely to be the sentence that follows A, or not?"}
{"id": "http://jalammar.github.io//illustrated-bert/_p62", "contents": "The BERT paper shows a number of ways to use BERT for different tasks."}
{"id": "http://jalammar.github.io//illustrated-bert/_p63", "contents": "The fine-tuning approach isn\u2019t the only way to use BERT. Just like ELMo, you can use the pre-trained BERT to create contextualized word embeddings. Then you can feed these embeddings to your existing model \u2013 a process the paper shows yield results not far behind fine-tuning BERT on a task such as named-entity recognition."}
{"id": "http://jalammar.github.io//illustrated-bert/_p64", "contents": "Which vector works best as a contextualized embedding? I would think it depends on the task. The paper examines six choices (Compared to the fine-tuned model which achieved a score of 96.4):"}
{"id": "http://jalammar.github.io//illustrated-bert/_p65", "contents": "The best way to try out BERT is through the BERT FineTuning with Cloud TPUs notebook hosted on Google Colab. If you\u2019ve never used Cloud TPUs before, this is also a good starting point to try them as well as the BERT code works on TPUs, CPUs and GPUs as well."}
{"id": "http://jalammar.github.io//illustrated-bert/_p66", "contents": "The next step would be to look at the code in the BERT repo:"}
{"id": "http://jalammar.github.io//illustrated-bert/_p67", "contents": "run_classifier.py is an example of the fine-tuning process. It also constructs the classification layer for the supervised model. If you want to construct your own classifier, check out the create_model() method in that file."}
{"id": "http://jalammar.github.io//illustrated-bert/_p68", "contents": "Several pre-trained models are available for download. These span BERT Base and BERT Large, as well as languages such as English, Chinese, and a multi-lingual model covering 102 languages trained on wikipedia."}
{"id": "http://jalammar.github.io//illustrated-bert/_p69", "contents": "You can also check out the PyTorch implementation of BERT. The AllenNLP library uses this implementation to allow using BERT embeddings with any model."}
{"id": "http://jalammar.github.io//illustrated-bert/_p70", "contents": "Thanks to Jacob Devlin, Matt Gardner, Kenton Lee,  Mark Neumann, and Matthew Peters for providing feedback on earlier drafts of this post."}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p0", "contents": "Visualizing machine learning one concept at a time.@JayAlammar on Twitter."}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p1", "contents": "Discussions:\nHacker News (63 points, 8 comments), Reddit r/programming (312 points, 37 comments)\n\nTranslations: Spanish\n"}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p2", "contents": "Update: Part 2 is now live: A Visual And Interactive Look at Basic Neural Network Math"}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p3", "contents": "I\u2019m not a machine learning expert. I\u2019m a software engineer by training and I\u2019ve had little interaction with AI. I had always wanted to delve deeper into machine learning, but never really found my \u201cin\u201d. That\u2019s why when Google open sourced TensorFlow in November 2015, I got super excited and knew it was time to jump in and start the learning journey. Not to sound dramatic, but to me, it actually felt kind of like Prometheus handing down fire to mankind from the Mount Olympus of machine learning. In the back of my head was the idea that the entire field of Big Data and technologies like Hadoop were vastly accelerated when Google researchers released their Map Reduce paper. This time it\u2019s not a paper \u2013 it\u2019s the actual software they use internally after years and years of evolution."}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p4", "contents": "So I started learning what I can about the basics of the topic, and saw the need for gentler resources for people with no experience in the field. This is my attempt at that."}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p5", "contents": "Let\u2019s start with a simple example. Say you\u2019re helping a friend who wants to buy a house. She was quoted $400,000 for a 2000 sq ft house (185 meters). Is this a good price or not?"}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p6", "contents": "It\u2019s not easy to tell without a frame of reference. So you ask your friends who have bought houses in that same neighborhoods, and you end up with three data points:"}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p7", "contents": "Personally, my first instinct would be to get the average price per sq ft. That comes to $180 per sq ft."}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p8", "contents": "Welcome to your first neural network! Now it\u2019s not quite at Siri level yet, but now you know the fundamental building block. And it looks like this:"}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p9", "contents": "Diagrams like this show you the structure of the network and how it calculates a prediction. The calculation starts from the input node at the left. The input value flows to the right. It gets multiplied by the weight and the result becomes our output."}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p10", "contents": "Multiplying 2,000 sq ft by 180 gives us $360,000. That\u2019s all there is to it at this level. Calculating the prediction is simple multiplication. But before that, we needed to think about the weight we\u2019ll be multiplying by. Here we started with an average, later we\u2019ll look at better algorithms that can scale as we get more inputs and more complicated models. Finding the weight is our \u201ctraining\u201d stage. So whenever you hear of someone \u201ctraining\u201d a neural network, it just means finding the weights we use to calculate the prediction."}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p11", "contents": "This is a form of prediction. This is a simple predictive model that takes an input, does a calculation, and gives an output (since the output can be of continuous values, the technical name for what we have would be a \u201cregression model\u201d)"}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p12", "contents": "Let us visualize this process (for simplicity, let\u2019s switch our price unit from $1 to $1000. Now our weight is 0.180 rather than 180):"}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p17", "contents": "Can we do better than estimate the price based on the average of our data points? Let\u2019s try. Let\u2019s first define what it means to be better in this scenario. If we apply our model to the three data points we have, how good of a job would it do?"}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p21", "contents": "That\u2019s quite a bit of yellow. Yellow is bad. Yellow is error. We want to shrink yellow as much as we can."}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p22", "contents": "Here we can see the actual price value, the predicted price value, and the difference between them. Then we\u2019ll need to average these differences so we have a number that tells us how much error there is in this prediction model. The problem is, the 3rd row has -63 as its value. We have to deal with this negative value if we want to use the difference between the prediction and price as our error measuring stick. That\u2019s one reason why we introduce an additional column that shows the error squared, thus getting rid of the negative value."}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p23", "contents": "This is now our definition of doing better \u2013 a better model is one that has less error. Error is measured as the average of the errors for each point in our data set. For each point, the error is measured by the difference between the actual value and the predicted value, raised to the power of 2. This is called Mean Square Error. Using it as a guide to train our model makes it our loss function (also, cost function)."}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p24", "contents": "Now that we defined our measuring stick for what makes a better model, let\u2019s experiment with a couple more weight values and compare them with our average pick:"}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p28", "contents": "Our lines can better approximate our values now that we have this b value added to the line formula. In this context, we call it a \u201cbias\u201d. This makes our neural network look like this:"}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p29", "contents": "We can generalize it by saying that a neural network with one input and one output (spoiler warning: and no hidden layers) looks like this:"}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p30", "contents": "In this graph, W and b are values we find during the training process. X is the input we plug into the formula (area in sq ft in our example). Y is the predicted price."}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p31", "contents": "Calculating a prediction now uses this formula:"}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p32", "contents": "So our current model calculates predictions by plugging in the area of house as x in this formula:"}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p33", "contents": "How about you take a crack at training our toy neural network? Minimize the loss function by tweaking the weight and bias dials. Can you get an error value below 799?"}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p34", "contents": "Congratulations on manually training your first neural network! Let\u2019s look at how to automate this training process. Below is another example with an additional autopilot-like functionality. These are the GD Step buttons. They use an algorithm called \u201cGradient Descent\u201d to try to step towards the correct weight and bias values that minimize the loss function."}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p35", "contents": "The two new graphs are to help you track the error values as you fiddle with the parameters (weight and bias) of the model. It\u2019s important to keep track of the error as the training process is all about reducing this error as much possible."}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p36", "contents": "How does gradient descent know where its next step should be? Calculus. You see, knowing the function we\u2019re minimizing (our loss function, the average of (y_ - y)\u00b2 for all our data points), and knowing the current inputs into it (the current weight and bias), the derivatives of the loss function tell us which direction to nudge W and b in order to minimize the error."}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p37", "contents": "Learn more about gradient descent and how to use it to calculate the new weights & bias in the first lectures of Coursera\u2019s Machine Learning course."}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p38", "contents": "Is the size of the house the only variable that goes into how much it costs? Obviously there are many other factors. Let\u2019s add another variable and see how we can adjust our neural network to it."}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p39", "contents": "Say your friend does a bit more research and finds a bunch more data points. She also finds out how many bathrooms each house has:"}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p40", "contents": "Our neural network with two variables looks like this:"}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p41", "contents": "We now have to find two weights (one for each input) and one bias to create our new model."}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p42", "contents": "Calculating Y looks like this:"}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p43", "contents": "But how do we find w1 and w2? This is a little trickier than when we only had to worry about one weight value. How much does having an extra bathroom change how we predict the value of a home?"}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p44", "contents": "Take a stab at finding the right weights and bias. You will start here to see the complexity we start getting into as the number of our inputs increase. We start losing the ability to create simple 2d shapes that allow us to visualize the model at a glance. Instead, we\u2019ll have to mainly rely on how the error value is evolving as we tweak our model parameters."}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p45", "contents": "Our trusty gradient descent is here to help once again. It still is valuable in helping us find the right weights and bias."}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p46", "contents": "Now that you\u2019ve seen neural networks with one and two features, you can sort of figure out how to add additional features and use them to calculate your predictions. The number of weights will continue to grow, and our implementation of gradient descent will have to be tweaked as we add each feature so that it can update the new weights associated with the new feature."}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p47", "contents": "It\u2019s important to note here that we don\u2019t blindly feed the network everything we know about our examples. We have to be selective about which features we feed the model. Feature selection/processing is an entire discipline with its own set of best practices and considerations. If you want to see an example of the process of examining a dataset to choose which features to feed a prediction model, check out  A Journey Through Titanic. It\u2019s a notebook where Omar EL Gabry narrates his process for solving Kaggle\u2019s Titanic challenge. Kaggle makes available the passenger\u2019s manifest of the Titanic including data like name, sex, age, cabin, and whether the person survived or not. The challenge is to build a model that predicts whether a person survived or not given their other information."}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p48", "contents": "Let\u2019s continue to tweak our example. Assume your friend gives you a list of houses. This time, she has labeled which ones she thinks have a good size and number of bathrooms:"}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p49", "contents": "She needs you to use this to create a model to predict whether she would like a house or not given its size and number of bathrooms. You will use this list above to build the model, then she will use the model to classify many other houses. One additional change in the process, is that she has another list of 10 houses she has labeled, but she\u2019s keeping it from you. That other list would be used to evaluate your model after you\u2019ve trained it \u2013 thus trying to ensure your model grasps the conditions that actually make her like the features of the house."}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p50", "contents": "The neural networks we\u2019ve been toying around with until now are all doing \u201cregression\u201d \u2013 they calculate and output a \u201ccontinuous\u201d value (the output can be 4, or 100.6, or 2143.342343). In practice, however, neural networks are more often used in \u201cclassification\u201d type problems. In these problems, the neural network\u2019s output has to be from a set of discrete values (or \u201cclasses\u201d) like \u201cGood\u201d or \u201cBad\u201d. How this works out in practice, is that we\u2019ll have a model that will say that it\u2019s 75% sure that a house is \u201cGood\u201d rather than just spit out \u201cgood\u201d or \u201cbad\u201d."}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p51", "contents": "One way we can transform the network we\u2019ve seen into a classification network is to have it output two values \u2013 one for each class (our classes now being \u201cgood\u201d and \u201cbad\u201d). We then pass these values through an operation called \u201csoftmax\u201d. The output of softmax is the probability of each class. For example, say that layer of the network outputs 2 for \u201cgood\u201d and 4 for \u201cbad\u201d, if we feed [2, 4] to softmax, it will return [0.11,  0.88] as the output. Which translates the values to say the network is 88% sure that the inputted value is \u201cbad\u201d and our friend would not like that house."}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p52", "contents": "Softmax takes an array and outputs an array of the same length. Notice that its outputs are all positive and sum up to 1 \u2013 which is useful when we\u2019re outputting a probability value. Also notice that even though 4 is double 2, its probability is not only double, but is eight times that of 2. This is a useful property that exaggerates the difference in output thus improving our training process."}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p53", "contents": "As you can see in the last two rows, softmax extends to any number of inputs. So now if our friend adds a third label (say \u201cGood, but I\u2019ll have to airbnb one room\u201d), softmax scales to accomedate that change."}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p54", "contents": "Take a second to explore the shape of the network as you vary the number of features (x1, x2, x3\u2026etc) (which can be area, number of bathrooms, price, proximity to school/work\u2026etc) and vary the number of classes (y1, y2, y3\u2026etc) (which can be \u201ctoo expensive\u201d, \u201cgood deal\u201d, \u201cgood if I airbnb\u201d, \u201ctoo small\u201d):"}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p55", "contents": "You can see an example of how to create and train this network using TensorFlow in this notebook I created to accompany this post."}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p56", "contents": "If you have reached this far, I have to reveal to you another motivation of mine to write this post. This post is meant as an even gentler intro to TensorFlow tutorials. If you start working through MNIST For ML Beginners now, and come across this graph:"}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p57", "contents": "I hope you would feel prepared and that you have an understanding of this system and how it works. If you want to start tinkering with code, feel free to pick up from the intro tutorial and teach a neural network how to detect handwritten digits."}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p58", "contents": "You should also continue your education by learning the theoretical and mathematical underpinnings of the concepts we discussed here. Good questions to ask now include:"}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p59", "contents": "Great learning resources include:"}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p60", "contents": "Thanks to Yasmine Alfouzan, Ammar Alammar, Khalid Alnuaim, Fahad Alhazmi, Mazen Melibari, and Hadeel Al-Negheimish for their assistance in reviewing previous versions of this post."}
{"id": "http://jalammar.github.io//visual-interactive-guide-basics-neural-networks/_p61", "contents": "Please contact me on Twitter with any corrections or feedback."}
{"id": "http://jalammar.github.io//about_p0", "contents": "Visualizing machine learning one concept at a time.@JayAlammar on Twitter."}
{"id": "http://jalammar.github.io//about_p1", "contents": "Hello! I\u2019m Jay and this is my English tech blog. The most popular posts here are:"}
{"id": "http://jalammar.github.io//about_p2", "contents": "As this is my technical blog, it\u2019s probably fitting to point out some aspects of technology that fascinate me:"}
{"id": "http://jalammar.github.io//about_p3", "contents": "\u0628\u0627\u0644\u0627\u0636\u0627\u0641\u0629 \u0625\u0644\u0649 \u0645\u062f\u0648\u0646\u062a\u064a \u0627\u0644\u062a\u0642\u0646\u064a\u0629 \u0647\u0630\u0647\u060c \u0644\u062f\u064a \u0645\u062f\u0648\u0646\u0627\u062a \u0633\u0627\u0628\u0642\u0629 \u0645\u062a\u0646\u0627\u062b\u0631\u0629 \u0641\u064a \u0627\u0644\u0627\u0646\u062a\u0631\u0646\u062a. \u0627\u062d\u062f\u062b\u0647\u0627 \u0647\u064a \u0645\u062f\u0648\u0646\u062a\u064a \u0641\u064a \u0643\u0627\u0631\u0627\u0645\u064a\u0644\u0627. \u0628\u0639\u0636 \u0643\u062a\u0627\u0628\u0627\u062a\u064a \u0627\u0644\u0633\u0627\u0628\u0642\u0629 \u0647\u064a:"}
{"id": "http://jalammar.github.io//about_p4", "contents": "alammar at gmail"}
{"id": "http://jalammar.github.io//jays-intro-to-ai/_p0", "contents": "Visualizing machine learning one concept at a time.@JayAlammar on Twitter."}
{"id": "http://jalammar.github.io//jays-intro-to-ai/_p1", "contents": "Check out the first video in my new series introducing the general public to AI and machine learning."}
{"id": "http://jalammar.github.io//jays-intro-to-ai/_p2", "contents": "My aim for this series is to help people integrate ML into their world-view away from all the hype and overpromises that plauge the topic."}
{"id": "http://jalammar.github.io//jays-intro-to-ai/_p3", "contents": "This first video is a gentle visual introduction to Artificial Intelligence (AI) and some of its key commercial applications. In this first video, we explain the simple trick that lies at the heart of the majority of AI/machine learning applications. Learn how to think about prediction, and how major companies apply it to make trillions of dollars."}
{"id": "http://jalammar.github.io//jays-intro-to-ai/_p5", "contents": "0:28 Why learn about AI?"}
{"id": "http://jalammar.github.io//jays-intro-to-ai/_p6", "contents": "2:47 What is AI?"}
{"id": "http://jalammar.github.io//jays-intro-to-ai/_p7", "contents": "3:44 Where machine learning intersects with business"}
{"id": "http://jalammar.github.io//jays-intro-to-ai/_p8", "contents": "4:02 The simple trick at the heart of it all"}
{"id": "http://jalammar.github.io//jays-intro-to-ai/_p9", "contents": "4:45 Simple prediction example using machine learning"}
{"id": "http://jalammar.github.io//jays-intro-to-ai/_p10", "contents": "7:07 The ingredients of prediction: Features, labels, datasets, models, and weights"}
{"id": "http://jalammar.github.io//jays-intro-to-ai/_p11", "contents": "7:56 Examples of prediction in business that generated over a $trillion"}
{"id": "http://jalammar.github.io//jays-intro-to-ai/_p12", "contents": "I\u2019ve been wanting to create YouTube videos for a while. I rely on YouTube for a lot of my own learning in ML and various other topics. I intend to cover the overlap of AI and business as a lot of people consider the topic for how it might effect their careers (or for technical people, how they may gain the skills of the field)."}
{"id": "http://jalammar.github.io//jays-intro-to-ai/_p13", "contents": "I would really love to dive into some advanced topics later on the life of the channel. But my first priority now is to get a few introductory videos out. There\u2019s a lot of needless complexity that people need to sort through to get a sense of what ML really does. I want to do something about that."}
{"id": "http://jalammar.github.io//jays-intro-to-ai/_p14", "contents": "Exciting times ahead!"}
{"id": "http://jalammar.github.io//qcon-2020-intro-to-ai/_p0", "contents": "Visualizing machine learning one concept at a time.@JayAlammar on Twitter."}
{"id": "http://jalammar.github.io//qcon-2020-intro-to-ai/_p1", "contents": "I had an incredible time organizing and speaking at the AI/machine learning track at QCon London 2020 where I invited and shared the stage with incredible speakers Vincent Warmerdam, Susanne Groothuis, Peter Elger, and Hien Luu."}
{"id": "http://jalammar.github.io//qcon-2020-intro-to-ai/_p2", "contents": "QCon is a global software conference for software engineers, architects, and team leaders, with over 1,600 attendees in London. All speakers have a software background."}
{"id": "http://jalammar.github.io//qcon-2020-intro-to-ai/_p3", "contents": "In my talk, I distilled ten central concepts in applied machine learning that should help software engineers break into the field."}
{"id": "http://jalammar.github.io//qcon-2020-intro-to-ai/_p4", "contents": "I learned quite a bunch from the talks and discussions. I agree with Vincent that we have to build business rules into chatbots (especially when considering where the technology is at the moment). Susanne\u2019s approach to having different groups label the same sentiment-analysis dataset, and then comparing group labels was quite interesting. Peter\u2019s advice to experiment with cloud ML service has merit in plenty of circumstances. And Hien\u2019s introduction to MLflow showed clear value in establishing a rigorous ML infrastructure that allows companies to roll out (and track) ML experiments quickly."}
{"id": "http://jalammar.github.io//qcon-2020-intro-to-ai/_p5", "contents": "I\u2019d like to thank them for sharing their time and insights. The videos should roll out publicly to the infoQ website over the next few weeks (then to YouTube later)."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p0", "contents": "Visualizing machine learning one concept at a time.@JayAlammar on Twitter."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p1", "contents": " Discussions:\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\n\n\n\nTranslations: Chinese (Simplified), Korean, Portuguese, Russian\n"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p2", "contents": "I find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you\u2019ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you\u2019ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2)."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p3", "contents": "Word2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p4", "contents": "In this post, we\u2019ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let\u2019s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p5", "contents": "On a scale of 0 to 100, how introverted/extraverted are you (where 0 is the most introverted, and 100 is the most extraverted)?\nHave you ever taken a personality test like MBTI \u2013 or even better, the Big Five Personality Traits test? If you haven\u2019t, these are tests that ask you a list of questions, then score you on a number of axes, introversion/extraversion being one of them."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p6", "contents": "Imagine I\u2019ve scored 38/100 as my introversion/extraversion score. we can plot that in this way:"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p7", "contents": "Let\u2019s switch the range to be from -1 to 1:"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p8", "contents": "How well do you feel you know a person knowing only this one piece of information about them? Not much. People are complex. So let\u2019s add another dimension \u2013 the score of one other trait from the test."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p9", "contents": "I\u2019ve hidden which traits we\u2019re plotting just so you get used to not knowing what each dimension represents \u2013 but still getting a lot of value from the vector representation of a person\u2019s personality."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p10", "contents": "We can now say that this vector partially represents my personality. The usefulness of such representation comes when you want to compare two other people to me. Say I get hit by a bus and I need to be replaced by someone with a similar personality. In the following figure, which of the two people is more similar to me?"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p11", "contents": "When dealing with vectors, a common way to calculate a similarity score is cosine_similarity:"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p12", "contents": "Yet again, two dimensions aren\u2019t enough to capture enough information about how different people are. Decades of psychology research have led to five major traits (and plenty of sub-traits). So let\u2019s use all five dimensions in our comparison:"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p13", "contents": "The problem with five dimensions is that we lose the ability to draw neat little arrows in two dimensions. This is a common challenge in machine learning where we often have to think in higher-dimensional space. The good thing is, though, that cosine_similarity still works. It works with any number of dimensions:"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p14", "contents": "At the end of this section, I want us to come out with two central ideas:"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p15", "contents": "With this understanding, we can proceed to look at trained word-vector examples (also called word embeddings) and start looking at some of their interesting properties."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p16", "contents": "This is a word embedding for the word \u201cking\u201d (GloVe vector trained on Wikipedia):"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p17", "contents": "\n[ 0.50451 ,  0.68607 , -0.59517 , -0.022801,  0.60046 , -0.13498 ,\n -0.08813 ,  0.47377 , -0.61798 , -0.31012 , -0.076666,  1.493   ,\n -0.034189, -0.98173 ,  0.68229 ,  0.81722 , -0.51874 , -0.31503 ,\n -0.55809 ,  0.66421 ,  0.1961  , -0.13495 , -0.11476 , -0.30344 ,\n  0.41177 , -2.223   , -1.0756  , -1.0783  , -0.34354 ,  0.33505 ,\n  1.9927  , -0.04234 , -0.64319 ,  0.71125 ,  0.49159 ,  0.16754 ,\n  0.34344 , -0.25663 , -0.8523  ,  0.1661  ,  0.40102 ,  1.1685  ,\n -1.0137  , -0.21585 , -0.15155 ,  0.78321 , -0.91241 , -1.6106  ,\n -0.64426 , -0.51042 ]\n "}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p18", "contents": "It\u2019s a list of 50 numbers. We can\u2019t tell much by looking at the values. But let\u2019s visualize it a bit so we can compare it other word vectors. Let\u2019s put all these numbers in one row:"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p19", "contents": "Let\u2019s color code the cells based on their values (red if they\u2019re close to 2, white if they\u2019re close to 0, blue if they\u2019re close to -2):"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p20", "contents": "We\u2019ll proceed by ignoring the numbers and only looking at the colors to indicate the values of the cells. Let\u2019s now contrast \u201cKing\u201d against other words:"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p21", "contents": "See how \u201cMan\u201d and \u201cWoman\u201d are much more similar to each other than either of them is to \u201cking\u201d? This tells you something. These vector representations capture quite a bit of the information/meaning/associations of these words."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p22", "contents": "Here\u2019s another list of examples (compare by vertically scanning the columns looking for columns with similar colors):"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p23", "contents": "A few things to point out:"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p24", "contents": "The famous examples that show an incredible property of embeddings is the concept of analogies. We can add and subtract word embeddings and arrive at interesting results. The most famous example is the formula: \u201cking\u201d - \u201cman\u201d + \u201cwoman\u201d:"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p25", "contents": "We can visualize this analogy as we did previously:"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p26", "contents": "Now that we\u2019ve looked at trained word embeddings, let\u2019s learn more about the training process. But before we get to word2vec, we need to look at a conceptual parent of word embeddings: the neural language model."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p27", "contents": "If one wanted to give an example of an NLP application, one of the best examples would be the next-word prediction feature of a smartphone keyboard. It\u2019s a feature that billions of people use hundreds of times every day."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p28", "contents": "Next-word prediction is a task that can be addressed by a language model. A language model can take a list of words (let\u2019s say two words), and attempt to predict the word that follows them."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p29", "contents": "In the screenshot above, we can think of the model as one that took in these two green words (thou shalt) and returned a list of suggestions (\u201cnot\u201d being the one with the highest probability):"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p31", "contents": "We can think of the model as looking like this black box:"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p34", "contents": "But in practice, the model doesn\u2019t output only one word. It actually outputs a probability score for all the words it knows (the model\u2019s \u201cvocabulary\u201d, which can range from a few thousand to over a million words). The keyboard application then has to find the words with the highest scores, and present those to the user."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p37", "contents": "After being trained, early neural language models (Bengio 2003) would calculate a prediction in three steps:"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p40", "contents": "The first step is the most relevant for us as we discuss embeddings. One of the results of the training process was this matrix that contains an embedding for each word in our vocabulary. During prediction time, we just look up the embeddings of the input word, and use them to calculate the prediction:"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p41", "contents": "Let\u2019s now turn to the training process to learn more about how this embedding matrix was developed."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p42", "contents": "Language models have a huge advantage over most other machine learning models. That advantage is that we are able to train them on running text \u2013 which we have an abundance of. Think of all the books, articles, Wikipedia content, and other forms of text data we have lying around. Contrast this with a lot of other machine learning models which need hand-crafted features and specially-collected data."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p43", "contents": "\u201cYou shall know a word by the company it keeps\u201d J.R. Firth"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p44", "contents": "Words get their embeddings by us looking at which other words they tend to appear next to. The mechanics of that is that"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p45", "contents": "As this window slides against the text, we (virtually) generate a dataset that we use to train a model. To look exactly at how that\u2019s done, let\u2019s see how the sliding window processes this phrase:"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p46", "contents": "\u201cThou shalt not make a machine in the likeness of a human mind\u201d ~Dune"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p47", "contents": "When we start, the window is on the first three words of the sentence:"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p50", "contents": "We take the first two words to be features, and the third word to be a label:"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p53", "contents": "We then slide our window to the next position and create a second sample:"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p56", "contents": "And pretty soon we have a larger dataset of which words tend to appear after different pairs of words:"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p59", "contents": "In practice, models tend to be trained while we\u2019re sliding the window. But I find it clearer to logically separate the \u201cdataset generation\u201d phase from the training phase. Aside from neural-network-based approaches to language modeling, a technique called N-grams was commonly used to train language models (see: Chapter 3 of Speech and Language Processing). To see how this switch from N-grams to neural models reflects on real-world products, here\u2019s a 2015 blog post from Swiftkey, my favorite Android keyboard, introducing their neural language model and comparing it with their previous N-gram model. I like this example because it shows you how the algorithmic properties of embeddings can be described in marketing speech."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p60", "contents": "Knowing what you know from earlier in the post, fill in the blank:"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p61", "contents": "The context I gave you here is five words before the blank word (and an earlier mention of \u201cbus\u201d). I\u2019m sure most people would guess the word bus goes into the blank. But what if I gave you one more piece of information \u2013 a word after the blank, would that change your answer?"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p62", "contents": "This completely changes what should go in the blank. the word red is now the most likely to go into the blank. What we learn from this is the words both before and after a specific word carry informational value. It turns out that accounting for both directions (words to the left and to the right of the word we\u2019re guessing) leads to better word embeddings. Let\u2019s see how we can adjust the way we\u2019re training the model to account for this."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p63", "contents": "Instead of only looking two words before the target word, we can also look at two words after it."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p64", "contents": "If we do this, the dataset we\u2019re virtually building and training the model against would look like this:"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p65", "contents": "This is called a Continuous Bag of Words architecture and is described in one of the word2vec papers [pdf]. Another architecture that also tended to show great results does things a little differently."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p66", "contents": "Instead of guessing a word based on its context (the words before and after it), this other architecture tries to guess neighboring words using the current word. We can think of the window it slides against the training text as looking like this:"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p67", "contents": "The pink boxes are in different shades because this sliding window actually creates four separate samples in our training dataset:"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p70", "contents": "This method is called the skipgram architecture. We can visualize the sliding window as doing the following:"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p73", "contents": "This would add these four samples to our training dataset:"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p75", "contents": "We then slide our window to the next position:"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p78", "contents": "Which generates our next four examples:"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p80", "contents": "A couple of positions later, we have a lot more examples:"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p82", "contents": "Now that we have our skipgram training dataset that we extracted from existing running text, let\u2019s glance at how we use it to train a basic neural language model that predicts the neighboring word."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p84", "contents": "We start with the first sample in our dataset. We grab the feature and feed to the untrained model asking it to predict an appropriate neighboring word."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p86", "contents": "The model conducts the three steps and outputs a prediction vector (with a probability assigned to each word in its vocabulary). Since the model is untrained, it\u2019s prediction is sure to be wrong at this stage. But that\u2019s okay. We know what word it should have guessed \u2013 the label/output cell in the row we\u2019re currently using to train the model:"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p89", "contents": "How far off was the model? We subtract the two vectors resulting in an error vector:"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p92", "contents": "This error vector can now be used to update the model so the next time, it\u2019s a little more likely to guess thou when it gets not as input."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p95", "contents": "And that concludes the first step of the training. We proceed to do the same process with the next sample in our dataset, and then the next, until we\u2019ve covered all the samples in the dataset. That concludes one epoch of training. We do it over again for a number of epochs, and then we\u2019d have our trained model and we can extract the embedding matrix from it and use it for any other application."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p96", "contents": "While this extends our understanding of the process, it\u2019s still not how word2vec is actually trained. We\u2019re missing a couple of key ideas."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p97", "contents": "Recall the three steps of how this neural language model calculates its prediction:\n"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p99", "contents": "The third step is very expensive from a computational point of view \u2013 especially knowing that we will do it once for every training sample in our dataset (easily tens of millions of times). We need to do something to improve performance."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p100", "contents": "One way is to split our target into two steps:"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p101", "contents": "We\u2019ll focus on step 1. in this post as we\u2019re focusing on embeddings. To generate high-quality embeddings using a high-performance model, we can switch the model\u2019s task from predicting a neighboring word:"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p102", "contents": "And switch it to a model that takes the input and output word, and outputs a score indicating if they\u2019re neighbors or not (0 for \u201cnot neighbors\u201d, 1 for \u201cneighbors\u201d)."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p103", "contents": "This simple switch changes the model we need from a neural network, to a logistic regression model \u2013 thus it becomes much simpler and much faster to calculate."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p104", "contents": "This switch requires we switch the structure of our dataset \u2013 the label is now a new column with values 0 or 1. They will be all 1 since all the words we added are neighbors."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p107", "contents": "This can now be computed at blazing speed \u2013 processing millions of examples in minutes. But there\u2019s one loophole we need to close. If all of our examples are positive (target: 1), we open ourself to the possibility of a smartass model that always returns 1 \u2013 achieving 100% accuracy, but learning nothing and generating garbage embeddings."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p108", "contents": "To address this, we need to introduce negative samples to our dataset \u2013 samples of words that are not neighbors.  Our model needs to return 0 for those samples. Now that\u2019s a challenge that the model has to work hard to solve \u2013 but still at blazing fast speed."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p110", "contents": "But what do we fill in as output words? We randomly sample words from our vocabulary"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p112", "contents": "This idea is inspired by Noise-contrastive estimation [pdf]. We are contrasting the actual signal (positive examples of neighboring words) with noise (randomly selected words that are not neighbors). This leads to a great tradeoff of computational and statistical efficiency."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p113", "contents": "We have now covered two of the central ideas in word2vec: as a pair, they\u2019re called skipgram with negative sampling."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p114", "contents": "Now that we\u2019ve established the two central ideas of skipgram and negative sampling, we can proceed to look closer at the actual word2vec training process."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p115", "contents": "Before the training process starts, we pre-process the text we\u2019re training the model against. In this step, we determine the size of our vocabulary (we\u2019ll call this vocab_size, think of it as, say, 10,000) and which words belong to it."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p116", "contents": "At the start of the training phase, we create two matrices \u2013 an Embedding matrix and a Context matrix. These two matrices have an embedding for each word in our vocabulary (So vocab_size is one of their dimensions). The second dimension is how long we want each embedding to be (embedding_size \u2013 300 is a common value, but we\u2019ve looked at an example of 50 earlier in this post)."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p117", "contents": "At the start of the training process, we initialize these matrices with random values. Then we start the training process. In each training step, we take one positive example and its associated negative examples. Let\u2019s take our first group:"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p118", "contents": "Now we have four words: the input word not and output/context words: thou (the actual neighbor), aaron, and taco (the negative examples). We proceed to look up their embeddings \u2013 for the input word, we look in the Embedding matrix. For the context words, we look in the Context matrix (even though both matrices have an embedding for every word in our vocabulary)."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p119", "contents": "Then, we take the dot product of the input embedding with each of the context embeddings. In each case, that would result in a number, that number indicates the similarity of the input and context embeddings"}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p120", "contents": "Now we need a way to turn these scores into something that looks like probabilities \u2013 we need them to all be positive and have values between zero and one. This is a great task for sigmoid, the logistic operation."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p121", "contents": "And we can now treat the output of the sigmoid operations as the model\u2019s output for these examples. You can see that taco has the highest score and aaron still has the lowest score both before and after the sigmoid operations."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p122", "contents": "Now that the untrained model has made a prediction, and seeing as though we have an actual target label to compare against, let\u2019s calculate how much error is in the model\u2019s prediction. To do that, we just subtract the sigmoid scores from the target labels."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p124", "contents": "Here comes the \u201clearning\u201d part of \u201cmachine learning\u201d. We can now use this error score to adjust the embeddings of not, thou, aaron, and taco so that the next time we make this calculation, the result would be closer to the target scores."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p125", "contents": "This concludes the training step. We emerge from it with slightly better embeddings for the words involved in this step (not, thou, aaron, and taco). We now proceed to our next step (the next positive sample and its associated negative samples) and do the same process again."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p126", "contents": "The embeddings continue to be improved while we cycle through our entire dataset for a number of times. We can then stop the training process, discard the Context matrix, and use the Embeddings matrix as our pre-trained embeddings for the next task."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p127", "contents": "Two key hyperparameters in the word2vec training process are the window size and the number of negative samples."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p128", "contents": "Different tasks are served better by different window sizes. One heuristic is that smaller window sizes (2-15) lead to embeddings where high similarity scores between two embeddings indicates that the words are interchangeable (notice that antonyms are often interchangable if we\u2019re only looking at their surrounding words \u2013 e.g. good and bad often appear in similar contexts). Larger window sizes (15-50, or even more) lead to embeddings where similarity is more indicative of relatedness of the words. In practice, you\u2019ll often have to provide annotations that guide the embedding process leading to a useful similarity sense for your task. The Gensim default window size is 5 (two words before and two words after the input word, in addition to the input word itself)."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p129", "contents": "The number of negative samples is another factor of the training process. The original paper prescribes 5-20 as being a good number of negative samples. It also states that 2-5 seems to be enough when you have a large enough dataset. The Gensim default is 5 negative samples."}
{"id": "http://jalammar.github.io//illustrated-word2vec/_p130", "contents": "I hope that you now have a sense for word embeddings and the word2vec algorithm. I also hope that now when you read a paper mentioning \u201cskip gram with negative sampling\u201d (SGNS) (like the recommendation system papers at the top), that you have a better sense for these concepts. As always, all feedback is appreciated @JayAlammar."}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p0", "contents": "Visualizing machine learning one concept at a time.@JayAlammar on Twitter."}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p1", "contents": "Translations: Russian"}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p2", "contents": "Progress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents \u201cthe biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search\u201d."}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p3", "contents": "This post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved."}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p4", "contents": "Alongside this post, I\u2019ve prepared a notebook. You can see it here the notebook or run it on colab.\n"}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p5", "contents": "The dataset we will use in this example is SST2, which contains sentences from movie reviews, each labeled as either positive (has the value 1) or negative (has the value 0):"}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p6", "contents": "Our goal is to create a model that takes a sentence (just like the ones in our dataset) and produces either 1 (indicating the sentence carries a positive sentiment) or a 0 (indicating the sentence carries a negative sentiment). We can think of it as looking like this:"}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p7", "contents": "Under the hood, the model is actually made up of two model."}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p8", "contents": "The data we pass between the two models is a vector of size 768. We can think of this of vector as an embedding for the sentence that we can use for classification."}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p9", "contents": "If you\u2019ve read my previous post, Illustrated BERT, this vector is the result of the first position (which receives the [CLS] token as input)."}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p10", "contents": "While we\u2019ll be using two models, we will only train the logistic regression model. For DistillBERT, we\u2019ll use a model that\u2019s already pre-trained and has a grasp on the English language. This model, however is neither trained not fine-tuned to do sentence classification. We get some sentence classification capability, however, from the general objectives BERT is trained on. This is especially the case with BERT\u2019s output for the first position (associated with the [CLS] token). I believe that\u2019s due to BERT\u2019s second training object \u2013 Next sentence classification. That objective seemingly trains the model to encapsulate a sentence-wide sense to the output at the first position. The transformers library provides us with an implementation of DistilBERT as well as pretrained versions of the model."}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p11", "contents": "So here\u2019s the game plan with this tutorial. We will first use the trained distilBERT to generate sentence embeddings for 2,000 sentences."}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p12", "contents": "We will not touch distilBERT after this step. It\u2019s all Scikit Learn from here. We do the usual train/test split on this dataset:"}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p13", "contents": "Then we train the logistic regression model on the training set:"}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p14", "contents": "Before we dig into the code and explain how to train the model, let\u2019s look at how a trained model calculates its prediction."}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p15", "contents": "Let\u2019s try to classify the sentence \u201ca visually stunning rumination on love\u201d. The first step is to use the BERT tokenizer to first split the word into tokens. Then, we add the special tokens needed for sentence classifications (these are [CLS] at the first position, and [SEP] at the end of the sentence)."}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p16", "contents": "The third step the tokenizer does is to replace each token with its id from the embedding table which is a component we get with the trained model. Read The Illustrated Word2vec for a background on word embeddings."}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p17", "contents": "Note that the tokenizer does all these steps in a single line of code:"}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p18", "contents": "Our input sentence is now the proper shape to be passed to DistilBERT."}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p19", "contents": "If you\u2019ve read Illustrated BERT, this step can also be visualized in this manner:"}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p20", "contents": "Passing the input vector through DistilBERT works just like BERT. The output would be a vector for each input token. each vector is made up of 768 numbers (floats)."}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p21", "contents": "Because this is a sentence classification task, we ignore all except the first vector (the one associated with the [CLS] token). The one vector we pass as the input to the logistic regression model."}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p22", "contents": "From here, it\u2019s the logistic regression model\u2019s job to classify this vector based on what it learned from its training phase. We can think of a prediction calculation as looking like this:"}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p23", "contents": "The training is what we\u2019ll discuss in the next section, along with the code of the entire process."}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p24", "contents": "In this section we\u2019ll highlight the code to train this sentence classification model. A notebook containing all this code is available on colab and github."}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p25", "contents": "Let\u2019s start by importing the tools of the trade"}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p26", "contents": "The dataset is available as a file on github, so we just import it directly into a pandas dataframe"}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p27", "contents": "We can use df.head() to look at the first five rows of the dataframe to see how the data looks."}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p28", "contents": "Which outputs:"}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p29", "contents": "We can now tokenize the dataset. Note that we\u2019re going to do things a little differently here from the example above. The example above tokenized and processed only one sentence. Here, we\u2019ll tokenize and process all sentences together as a batch (the notebook processes a smaller group of examples just for resource considerations, let\u2019s say 2000 examples)."}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p30", "contents": "This turns every sentence into the list of ids."}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p31", "contents": "The dataset is currently a list (or pandas Series/DataFrame) of lists. Before DistilBERT can process this as input, we\u2019ll need to make all the vectors the same size by padding shorter sentences with the token id 0. You can refer to the notebook for the padding step, it\u2019s basic python string and array manipulation."}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p32", "contents": "After the padding, we have a matrix/tensor that is ready to be passed to BERT:"}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p33", "contents": "We now create an input tensor out of the padded token matrix, and send that to DistilBERT"}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p34", "contents": "After running this step, last_hidden_states holds the outputs of DistilBERT. It is a tuple with the shape (number of examples, max number of tokens in the sequence, number of hidden units in the DistilBERT model). In our case, this will be 2000 (since we only limited ourselves to 2000 examples), 66 (which is the number of tokens in the longest sequence from the 2000 examples), 768 (the number of hidden units in the DistilBERT model)."}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p35", "contents": "Let\u2019s unpack this 3-d output tensor. We can first start by examining its dimensions:"}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p36", "contents": "Each row is associated with a sentence from our dataset. To recap the processing path of the first sentence, we can think of it as looking like this:"}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p37", "contents": "For sentence classification, we\u2019re only only interested in BERT\u2019s output for the [CLS] token, so we select that slice of the cube and discard everything else."}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p38", "contents": "This is how we slice that 3d tensor to get the 2d tensor we\u2019re interested in:"}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p39", "contents": "And now features is a 2d numpy array containing the sentence embeddings of all the sentences in our dataset."}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p40", "contents": "Now that we have the output of BERT, we have assembled the dataset we need to train our logistic regression model. The 768 columns are the features, and the labels we just get from our initial dataset."}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p41", "contents": "After doing the traditional train/test split of machine learning, we can declare our Logistic Regression model and train it against the dataset."}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p42", "contents": "Which splits the dataset into training/testing sets:"}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p43", "contents": "Next, we train the Logistic Regression model on the training set."}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p44", "contents": "Now that the model is trained, we can score it against the test set:"}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p45", "contents": "Which shows the model achieves around 81% accuracy."}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p46", "contents": "For reference, the highest accuracy score for this dataset is currently 96.8. DistilBERT can be trained to improve its score on this task \u2013 a process called fine-tuning which updates BERT\u2019s weights to make it achieve a better performance in the sentence classification (which we can call the downstream task). The fine-tuned DistilBERT turns out to achieve an accuracy score of 90.7. The full size BERT model achieves 94.9."}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p47", "contents": "Dive right into the notebook or run it on colab."}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p48", "contents": "And that\u2019s it! That\u2019s a good first contact with BERT. The next step would be to head over to the documentation and try your hand at fine-tuning. You can also go back and switch from distilBERT to BERT and see how that works."}
{"id": "http://jalammar.github.io//a-visual-guide-to-using-bert-for-the-first-time/_p49", "contents": "Thanks to Cl\u00e9ment Delangue, Victor Sanh, and the Huggingface team for providing feedback to earlier versions of this tutorial."}
{"id": "http://jalammar.github.io//mit-analytics-lab-talk/_p0", "contents": "Visualizing machine learning one concept at a time.@JayAlammar on Twitter."}
{"id": "http://jalammar.github.io//mit-analytics-lab-talk/_p1", "contents": "I had a great time speaking at the MIT Analytics Lab about some of my favorite ideas in natural language processing and their practical applications."}
{"id": "http://jalammar.github.io//mit-analytics-lab-talk/_p2", "contents": "The talk built on top of my explanations of the word2vec algorithm, and then how embeddings are used to do product recommendations."}
{"id": "http://jalammar.github.io//mit-analytics-lab-talk/_p3", "contents": "I demoed three basic Jupyter notebooks as hand-on practice:"}
{"id": "http://jalammar.github.io//mit-analytics-lab-talk/_p4", "contents": "Exploring Word Embeddings (Colab)\n  This notebook provides the quickest way to experiment with word embeddings, visualize them, and compare them."}
{"id": "http://jalammar.github.io//mit-analytics-lab-talk/_p5", "contents": "Song Embeddings - Skipgram Recommender (Colab)\n   In this notebook, we train a word2vec model against song playlists to generate music recommendations."}
{"id": "http://jalammar.github.io//mit-analytics-lab-talk/_p6", "contents": "Sentence Classification with BERT (Colab)\n  This notebook is a super fast way to use a pre-trained BERT model (using the wonderful Huggingface transformers package) use it for sentiment analysis.  I later expanded this in its own blog post - A Visual Guide to Using BERT for the First Time with an update notebook (Colab)"}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p0", "contents": "Visualizing machine learning one concept at a time.@JayAlammar on Twitter."}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p1", "contents": "Check out the first video in my new series introducing the general public to AI and machine learning."}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p2", "contents": "My aim for this series is to help people integrate ML into their world-view away from all the hype and overpromises that plauge the topic."}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p3", "contents": "I had an incredible time organizing and speaking at the AI/machine learning track at QCon London 2020 where I invited and shared the stage with incredible speakers Vincent Warmerdam, Susanne Groothuis, Peter Elger, and Hien Luu."}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p4", "contents": "QCon is a global software conference for software engineers, architects, and team leaders, with over 1,600 attendees in London. All speakers have a software background."}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p5", "contents": "Translations: Russian"}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p6", "contents": "Progress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents \u201cthe biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search\u201d."}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p7", "contents": "This post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved."}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p8", "contents": "Alongside this post, I\u2019ve prepared a notebook. You can see it here the notebook or run it on colab."}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p9", "contents": "I had a great time speaking at the MIT Analytics Lab about some of my favorite ideas in natural language processing and their practical applications."}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p10", "contents": "Discussions:\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\n"}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p11", "contents": "Translations: Russian"}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p12", "contents": "This year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn\u2019t a particularly novel architecture \u2013 it\u2019s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we\u2019ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we\u2019ll look at applications for the decoder-only transformer beyond language modeling."}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p13", "contents": "My goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they\u2019ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve."}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p14", "contents": "Discussions:\nHacker News (366 points, 21 comments), Reddit r/MachineLearning (256 points, 18 comments)\n\n\nTranslations: Chinese 1, Chinese 2, Japanese"}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p15", "contents": "The NumPy package is the workhorse of data analysis, machine learning, and scientific computing in the python ecosystem. It vastly simplifies manipulating and crunching vectors and matrices. Some of python\u2019s leading package rely on NumPy as a fundamental piece of their infrastructure (examples include scikit-learn, SciPy, pandas, and tensorflow). Beyond the ability to slice and dice numeric data, mastering numpy will give you an edge when dealing and debugging with advanced usecases in these libraries."}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p16", "contents": "In this post, we\u2019ll look at some of the main ways to use NumPy and how it can represent different types of data (tables, images, text\u2026etc) before we can serve them to machine learning models."}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p17", "contents": "I gave a talk at Qcon London this year. Watch it here:"}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p18", "contents": "Intuition & Use-Cases of Embeddings in NLP & beyond [YouTube]"}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p19", "contents": "https://www.infoq.com/presentations/nlp-word-embedding/ [infoQ]"}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p20", "contents": "In this video, I introduced word embeddings and the word2vec algorithm.  I then proceeded to discuss how the word2vec algorithm is used to create recommendation engines in companies like Airbnb and Alibaba. I close by glancing at real-world consequences of popular recommendation systems like those of YouTube and Facebook."}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p21", "contents": "My Illustrated Word2vec post used and built on the materials I created for this talk (but didn\u2019t include anything on the recommender application of word2vec). This was my first talk at a technical conference and I spent quite a bit of time preparing for it. In the six weeks prior to the conference I spent about 100 hours working on the presentation and ended up with 200 slides. It was an interesting balancing act of trying to make it introductory but not shallow, suitable for senior engineers and architects yet not necessarily ones who have machine learning experience."}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p22", "contents": " Discussions:\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\n\n\n\nTranslations: Chinese (Simplified), Korean, Portuguese, Russian\n"}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p23", "contents": "I find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you\u2019ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you\u2019ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2)."}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p24", "contents": "Word2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines."}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p25", "contents": "In this post, we\u2019ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let\u2019s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?"}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p26", "contents": "Discussions:\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\n\n\nTranslations: Chinese (Simplified), Japanese, Korean, Persian, Russian"}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p27", "contents": "The year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It\u2019s been referred to as NLP\u2019s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks)."}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p28", "contents": "Discussions:\nHacker News (195 points, 51 comments), Reddit r/Python (140 points, 18 comments)\n"}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p29", "contents": "If you\u2019re planning to learn data analysis, machine learning, or data science tools in python, you\u2019re most likely going to be using the wonderful pandas library. Pandas is an open source library for data manipulation and analysis in python."}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p30", "contents": "One of the easiest ways to think about that, is that you can load tables (and excel files) and then slice and dice them in multiple ways:"}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p32", "contents": "Discussions:\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\n\n\nTranslations: Chinese (Simplified), Japanese, Korean, Russian\n\nWatch: MIT\u2019s Deep Learning State of the Art lecture referencing this post"}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p33", "contents": "In the previous post, we looked at Attention \u2013 a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer \u2013 a model that uses attention to boost the speed with which these models can be trained. The Transformers outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud\u2019s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let\u2019s try to break the model apart and look at how it functions."}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p34", "contents": "The Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard\u2019s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter."}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p35", "contents": "Let\u2019s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another."}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p36", "contents": "Translations: Chinese (Simplified), Japanese, Korean, Russian\n\nWatch: MIT\u2019s Deep Learning State of the Art lecture referencing this post"}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p37", "contents": "May 25th update: New graphics (RNN animation, word embedding graph), color coding, elaborated on the final attention example."}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p38", "contents": "Note: The animations below are videos. Touch or hover on them (if you\u2019re using a mouse) to get play controls so you can pause if needed."}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p39", "contents": "Sequence-to-sequence models are deep learning models that have achieved a lot of success in tasks like machine translation, text summarization, and image captioning. Google Translate started using such a model in production in late 2016. These models are explained in the two pioneering papers (Sutskever et al., 2014, Cho et al., 2014)."}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p40", "contents": "I found, however, that understanding the model well enough to implement it requires unraveling a series of concepts that build on top of each other. I thought that a bunch of these ideas would be more accessible if expressed visually. That\u2019s what I aim to do in this post. You\u2019ll need some previous understanding of deep learning to get through this post. I hope it can be a useful companion to reading the papers mentioned above (and the attention papers linked later in the post)."}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p41", "contents": "A sequence-to-sequence model is a model that takes a sequence of items (words, letters, features of an images\u2026etc) and outputs another sequence of items. A trained model would work like this:"}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p42", "contents": "I love using python\u2019s Pandas package for data analysis. The 10 Minutes to pandas is a great place to start learning how to use it for data analysis."}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p43", "contents": "Things get a lot more interesting once you\u2019re comfortable with the fundamentals and start with Reshaping and Pivot Tables. That guide shows some of the more interesting functions of reshaping data. Below are some visualizations to go along with the Pandas reshaping guide."}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p44", "contents": "In the previous post, we looked at the basic concepts of neural networks. Let us now take another example as an excuse to guide us to explore some of the basic mathematical ideas involved in prediction with neural networks."}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p45", "contents": "Discussions:\nHacker News (63 points, 8 comments), Reddit r/programming (312 points, 37 comments)\n\nTranslations: Spanish\n"}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p46", "contents": "Update: Part 2 is now live: A Visual And Interactive Look at Basic Neural Network Math"}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p47", "contents": "I\u2019m not a machine learning expert. I\u2019m a software engineer by training and I\u2019ve had little interaction with AI. I had always wanted to delve deeper into machine learning, but never really found my \u201cin\u201d. That\u2019s why when Google open sourced TensorFlow in November 2015, I got super excited and knew it was time to jump in and start the learning journey. Not to sound dramatic, but to me, it actually felt kind of like Prometheus handing down fire to mankind from the Mount Olympus of machine learning. In the back of my head was the idea that the entire field of Big Data and technologies like Hadoop were vastly accelerated when Google researchers released their Map Reduce paper. This time it\u2019s not a paper \u2013 it\u2019s the actual software they use internally after years and years of evolution."}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p48", "contents": "So I started learning what I can about the basics of the topic, and saw the need for gentler resources for people with no experience in the field. This is my attempt at that."}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p49", "contents": "Discussion:\nReddit r/Android (80 points, 16 comments)\n"}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p51", "contents": "In November 2015, Google announced and open sourced TensorFlow, its latest and greatest machine learning library. This is a big deal for three reasons:"}
{"id": "http://jalammar.github.io/#part-3-beyond-language-modeling_p52", "contents": "This last reason is the operating reason for this post since we\u2019ll be focusing on Android. If you examine the tensorflow repo on GitHub, you\u2019ll find a little  tensorflow/examples/android directory. I\u2019ll try to shed some light on the Android TensorFlow example and some of the things going on under the hood."}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p0", "contents": "Visualizing machine learning one concept at a time.@JayAlammar on Twitter."}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p1", "contents": "Check out the first video in my new series introducing the general public to AI and machine learning."}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p2", "contents": "My aim for this series is to help people integrate ML into their world-view away from all the hype and overpromises that plauge the topic."}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p3", "contents": "I had an incredible time organizing and speaking at the AI/machine learning track at QCon London 2020 where I invited and shared the stage with incredible speakers Vincent Warmerdam, Susanne Groothuis, Peter Elger, and Hien Luu."}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p4", "contents": "QCon is a global software conference for software engineers, architects, and team leaders, with over 1,600 attendees in London. All speakers have a software background."}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p5", "contents": "Translations: Russian"}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p6", "contents": "Progress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents \u201cthe biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search\u201d."}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p7", "contents": "This post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved."}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p8", "contents": "Alongside this post, I\u2019ve prepared a notebook. You can see it here the notebook or run it on colab."}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p9", "contents": "I had a great time speaking at the MIT Analytics Lab about some of my favorite ideas in natural language processing and their practical applications."}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p10", "contents": "Discussions:\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\n"}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p11", "contents": "Translations: Russian"}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p12", "contents": "This year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn\u2019t a particularly novel architecture \u2013 it\u2019s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we\u2019ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we\u2019ll look at applications for the decoder-only transformer beyond language modeling."}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p13", "contents": "My goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they\u2019ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve."}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p14", "contents": "Discussions:\nHacker News (366 points, 21 comments), Reddit r/MachineLearning (256 points, 18 comments)\n\n\nTranslations: Chinese 1, Chinese 2, Japanese"}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p15", "contents": "The NumPy package is the workhorse of data analysis, machine learning, and scientific computing in the python ecosystem. It vastly simplifies manipulating and crunching vectors and matrices. Some of python\u2019s leading package rely on NumPy as a fundamental piece of their infrastructure (examples include scikit-learn, SciPy, pandas, and tensorflow). Beyond the ability to slice and dice numeric data, mastering numpy will give you an edge when dealing and debugging with advanced usecases in these libraries."}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p16", "contents": "In this post, we\u2019ll look at some of the main ways to use NumPy and how it can represent different types of data (tables, images, text\u2026etc) before we can serve them to machine learning models."}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p17", "contents": "I gave a talk at Qcon London this year. Watch it here:"}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p18", "contents": "Intuition & Use-Cases of Embeddings in NLP & beyond [YouTube]"}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p19", "contents": "https://www.infoq.com/presentations/nlp-word-embedding/ [infoQ]"}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p20", "contents": "In this video, I introduced word embeddings and the word2vec algorithm.  I then proceeded to discuss how the word2vec algorithm is used to create recommendation engines in companies like Airbnb and Alibaba. I close by glancing at real-world consequences of popular recommendation systems like those of YouTube and Facebook."}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p21", "contents": "My Illustrated Word2vec post used and built on the materials I created for this talk (but didn\u2019t include anything on the recommender application of word2vec). This was my first talk at a technical conference and I spent quite a bit of time preparing for it. In the six weeks prior to the conference I spent about 100 hours working on the presentation and ended up with 200 slides. It was an interesting balancing act of trying to make it introductory but not shallow, suitable for senior engineers and architects yet not necessarily ones who have machine learning experience."}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p22", "contents": " Discussions:\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\n\n\n\nTranslations: Chinese (Simplified), Korean, Portuguese, Russian\n"}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p23", "contents": "I find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you\u2019ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you\u2019ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2)."}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p24", "contents": "Word2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines."}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p25", "contents": "In this post, we\u2019ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let\u2019s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?"}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p26", "contents": "Discussions:\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\n\n\nTranslations: Chinese (Simplified), Japanese, Korean, Persian, Russian"}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p27", "contents": "The year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It\u2019s been referred to as NLP\u2019s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks)."}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p28", "contents": "Discussions:\nHacker News (195 points, 51 comments), Reddit r/Python (140 points, 18 comments)\n"}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p29", "contents": "If you\u2019re planning to learn data analysis, machine learning, or data science tools in python, you\u2019re most likely going to be using the wonderful pandas library. Pandas is an open source library for data manipulation and analysis in python."}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p30", "contents": "One of the easiest ways to think about that, is that you can load tables (and excel files) and then slice and dice them in multiple ways:"}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p32", "contents": "Discussions:\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\n\n\nTranslations: Chinese (Simplified), Japanese, Korean, Russian\n\nWatch: MIT\u2019s Deep Learning State of the Art lecture referencing this post"}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p33", "contents": "In the previous post, we looked at Attention \u2013 a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer \u2013 a model that uses attention to boost the speed with which these models can be trained. The Transformers outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud\u2019s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let\u2019s try to break the model apart and look at how it functions."}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p34", "contents": "The Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard\u2019s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter."}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p35", "contents": "Let\u2019s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another."}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p36", "contents": "Translations: Chinese (Simplified), Japanese, Korean, Russian\n\nWatch: MIT\u2019s Deep Learning State of the Art lecture referencing this post"}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p37", "contents": "May 25th update: New graphics (RNN animation, word embedding graph), color coding, elaborated on the final attention example."}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p38", "contents": "Note: The animations below are videos. Touch or hover on them (if you\u2019re using a mouse) to get play controls so you can pause if needed."}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p39", "contents": "Sequence-to-sequence models are deep learning models that have achieved a lot of success in tasks like machine translation, text summarization, and image captioning. Google Translate started using such a model in production in late 2016. These models are explained in the two pioneering papers (Sutskever et al., 2014, Cho et al., 2014)."}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p40", "contents": "I found, however, that understanding the model well enough to implement it requires unraveling a series of concepts that build on top of each other. I thought that a bunch of these ideas would be more accessible if expressed visually. That\u2019s what I aim to do in this post. You\u2019ll need some previous understanding of deep learning to get through this post. I hope it can be a useful companion to reading the papers mentioned above (and the attention papers linked later in the post)."}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p41", "contents": "A sequence-to-sequence model is a model that takes a sequence of items (words, letters, features of an images\u2026etc) and outputs another sequence of items. A trained model would work like this:"}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p42", "contents": "I love using python\u2019s Pandas package for data analysis. The 10 Minutes to pandas is a great place to start learning how to use it for data analysis."}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p43", "contents": "Things get a lot more interesting once you\u2019re comfortable with the fundamentals and start with Reshaping and Pivot Tables. That guide shows some of the more interesting functions of reshaping data. Below are some visualizations to go along with the Pandas reshaping guide."}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p44", "contents": "In the previous post, we looked at the basic concepts of neural networks. Let us now take another example as an excuse to guide us to explore some of the basic mathematical ideas involved in prediction with neural networks."}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p45", "contents": "Discussions:\nHacker News (63 points, 8 comments), Reddit r/programming (312 points, 37 comments)\n\nTranslations: Spanish\n"}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p46", "contents": "Update: Part 2 is now live: A Visual And Interactive Look at Basic Neural Network Math"}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p47", "contents": "I\u2019m not a machine learning expert. I\u2019m a software engineer by training and I\u2019ve had little interaction with AI. I had always wanted to delve deeper into machine learning, but never really found my \u201cin\u201d. That\u2019s why when Google open sourced TensorFlow in November 2015, I got super excited and knew it was time to jump in and start the learning journey. Not to sound dramatic, but to me, it actually felt kind of like Prometheus handing down fire to mankind from the Mount Olympus of machine learning. In the back of my head was the idea that the entire field of Big Data and technologies like Hadoop were vastly accelerated when Google researchers released their Map Reduce paper. This time it\u2019s not a paper \u2013 it\u2019s the actual software they use internally after years and years of evolution."}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p48", "contents": "So I started learning what I can about the basics of the topic, and saw the need for gentler resources for people with no experience in the field. This is my attempt at that."}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p49", "contents": "Discussion:\nReddit r/Android (80 points, 16 comments)\n"}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p51", "contents": "In November 2015, Google announced and open sourced TensorFlow, its latest and greatest machine learning library. This is a big deal for three reasons:"}
{"id": "http://jalammar.github.io/#part-2-illustrated-self-attention_p52", "contents": "This last reason is the operating reason for this post since we\u2019ll be focusing on Android. If you examine the tensorflow repo on GitHub, you\u2019ll find a little  tensorflow/examples/android directory. I\u2019ll try to shed some light on the Android TensorFlow example and some of the things going on under the hood."}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p0", "contents": "Visualizing machine learning one concept at a time.@JayAlammar on Twitter."}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p1", "contents": "Check out the first video in my new series introducing the general public to AI and machine learning."}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p2", "contents": "My aim for this series is to help people integrate ML into their world-view away from all the hype and overpromises that plauge the topic."}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p3", "contents": "I had an incredible time organizing and speaking at the AI/machine learning track at QCon London 2020 where I invited and shared the stage with incredible speakers Vincent Warmerdam, Susanne Groothuis, Peter Elger, and Hien Luu."}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p4", "contents": "QCon is a global software conference for software engineers, architects, and team leaders, with over 1,600 attendees in London. All speakers have a software background."}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p5", "contents": "Translations: Russian"}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p6", "contents": "Progress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents \u201cthe biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search\u201d."}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p7", "contents": "This post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved."}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p8", "contents": "Alongside this post, I\u2019ve prepared a notebook. You can see it here the notebook or run it on colab."}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p9", "contents": "I had a great time speaking at the MIT Analytics Lab about some of my favorite ideas in natural language processing and their practical applications."}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p10", "contents": "Discussions:\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\n"}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p11", "contents": "Translations: Russian"}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p12", "contents": "This year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn\u2019t a particularly novel architecture \u2013 it\u2019s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we\u2019ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we\u2019ll look at applications for the decoder-only transformer beyond language modeling."}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p13", "contents": "My goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they\u2019ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve."}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p14", "contents": "Discussions:\nHacker News (366 points, 21 comments), Reddit r/MachineLearning (256 points, 18 comments)\n\n\nTranslations: Chinese 1, Chinese 2, Japanese"}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p15", "contents": "The NumPy package is the workhorse of data analysis, machine learning, and scientific computing in the python ecosystem. It vastly simplifies manipulating and crunching vectors and matrices. Some of python\u2019s leading package rely on NumPy as a fundamental piece of their infrastructure (examples include scikit-learn, SciPy, pandas, and tensorflow). Beyond the ability to slice and dice numeric data, mastering numpy will give you an edge when dealing and debugging with advanced usecases in these libraries."}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p16", "contents": "In this post, we\u2019ll look at some of the main ways to use NumPy and how it can represent different types of data (tables, images, text\u2026etc) before we can serve them to machine learning models."}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p17", "contents": "I gave a talk at Qcon London this year. Watch it here:"}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p18", "contents": "Intuition & Use-Cases of Embeddings in NLP & beyond [YouTube]"}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p19", "contents": "https://www.infoq.com/presentations/nlp-word-embedding/ [infoQ]"}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p20", "contents": "In this video, I introduced word embeddings and the word2vec algorithm.  I then proceeded to discuss how the word2vec algorithm is used to create recommendation engines in companies like Airbnb and Alibaba. I close by glancing at real-world consequences of popular recommendation systems like those of YouTube and Facebook."}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p21", "contents": "My Illustrated Word2vec post used and built on the materials I created for this talk (but didn\u2019t include anything on the recommender application of word2vec). This was my first talk at a technical conference and I spent quite a bit of time preparing for it. In the six weeks prior to the conference I spent about 100 hours working on the presentation and ended up with 200 slides. It was an interesting balancing act of trying to make it introductory but not shallow, suitable for senior engineers and architects yet not necessarily ones who have machine learning experience."}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p22", "contents": " Discussions:\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\n\n\n\nTranslations: Chinese (Simplified), Korean, Portuguese, Russian\n"}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p23", "contents": "I find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you\u2019ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you\u2019ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2)."}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p24", "contents": "Word2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines."}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p25", "contents": "In this post, we\u2019ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let\u2019s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?"}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p26", "contents": "Discussions:\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\n\n\nTranslations: Chinese (Simplified), Japanese, Korean, Persian, Russian"}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p27", "contents": "The year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It\u2019s been referred to as NLP\u2019s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks)."}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p28", "contents": "Discussions:\nHacker News (195 points, 51 comments), Reddit r/Python (140 points, 18 comments)\n"}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p29", "contents": "If you\u2019re planning to learn data analysis, machine learning, or data science tools in python, you\u2019re most likely going to be using the wonderful pandas library. Pandas is an open source library for data manipulation and analysis in python."}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p30", "contents": "One of the easiest ways to think about that, is that you can load tables (and excel files) and then slice and dice them in multiple ways:"}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p32", "contents": "Discussions:\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\n\n\nTranslations: Chinese (Simplified), Japanese, Korean, Russian\n\nWatch: MIT\u2019s Deep Learning State of the Art lecture referencing this post"}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p33", "contents": "In the previous post, we looked at Attention \u2013 a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer \u2013 a model that uses attention to boost the speed with which these models can be trained. The Transformers outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud\u2019s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let\u2019s try to break the model apart and look at how it functions."}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p34", "contents": "The Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard\u2019s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter."}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p35", "contents": "Let\u2019s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another."}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p36", "contents": "Translations: Chinese (Simplified), Japanese, Korean, Russian\n\nWatch: MIT\u2019s Deep Learning State of the Art lecture referencing this post"}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p37", "contents": "May 25th update: New graphics (RNN animation, word embedding graph), color coding, elaborated on the final attention example."}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p38", "contents": "Note: The animations below are videos. Touch or hover on them (if you\u2019re using a mouse) to get play controls so you can pause if needed."}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p39", "contents": "Sequence-to-sequence models are deep learning models that have achieved a lot of success in tasks like machine translation, text summarization, and image captioning. Google Translate started using such a model in production in late 2016. These models are explained in the two pioneering papers (Sutskever et al., 2014, Cho et al., 2014)."}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p40", "contents": "I found, however, that understanding the model well enough to implement it requires unraveling a series of concepts that build on top of each other. I thought that a bunch of these ideas would be more accessible if expressed visually. That\u2019s what I aim to do in this post. You\u2019ll need some previous understanding of deep learning to get through this post. I hope it can be a useful companion to reading the papers mentioned above (and the attention papers linked later in the post)."}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p41", "contents": "A sequence-to-sequence model is a model that takes a sequence of items (words, letters, features of an images\u2026etc) and outputs another sequence of items. A trained model would work like this:"}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p42", "contents": "I love using python\u2019s Pandas package for data analysis. The 10 Minutes to pandas is a great place to start learning how to use it for data analysis."}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p43", "contents": "Things get a lot more interesting once you\u2019re comfortable with the fundamentals and start with Reshaping and Pivot Tables. That guide shows some of the more interesting functions of reshaping data. Below are some visualizations to go along with the Pandas reshaping guide."}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p44", "contents": "In the previous post, we looked at the basic concepts of neural networks. Let us now take another example as an excuse to guide us to explore some of the basic mathematical ideas involved in prediction with neural networks."}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p45", "contents": "Discussions:\nHacker News (63 points, 8 comments), Reddit r/programming (312 points, 37 comments)\n\nTranslations: Spanish\n"}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p46", "contents": "Update: Part 2 is now live: A Visual And Interactive Look at Basic Neural Network Math"}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p47", "contents": "I\u2019m not a machine learning expert. I\u2019m a software engineer by training and I\u2019ve had little interaction with AI. I had always wanted to delve deeper into machine learning, but never really found my \u201cin\u201d. That\u2019s why when Google open sourced TensorFlow in November 2015, I got super excited and knew it was time to jump in and start the learning journey. Not to sound dramatic, but to me, it actually felt kind of like Prometheus handing down fire to mankind from the Mount Olympus of machine learning. In the back of my head was the idea that the entire field of Big Data and technologies like Hadoop were vastly accelerated when Google researchers released their Map Reduce paper. This time it\u2019s not a paper \u2013 it\u2019s the actual software they use internally after years and years of evolution."}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p48", "contents": "So I started learning what I can about the basics of the topic, and saw the need for gentler resources for people with no experience in the field. This is my attempt at that."}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p49", "contents": "Discussion:\nReddit r/Android (80 points, 16 comments)\n"}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p51", "contents": "In November 2015, Google announced and open sourced TensorFlow, its latest and greatest machine learning library. This is a big deal for three reasons:"}
{"id": "http://jalammar.github.io/#part-1-got-and-language-modeling_p52", "contents": "This last reason is the operating reason for this post since we\u2019ll be focusing on Android. If you examine the tensorflow repo on GitHub, you\u2019ll find a little  tensorflow/examples/android directory. I\u2019ll try to shed some light on the Android TensorFlow example and some of the things going on under the hood."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p0", "contents": "Visualizing machine learning one concept at a time.@JayAlammar on Twitter."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p1", "contents": "Discussions:\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\n"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p2", "contents": "Translations: Russian"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p3", "contents": "This year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn\u2019t a particularly novel architecture \u2013 it\u2019s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we\u2019ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we\u2019ll look at applications for the decoder-only transformer beyond language modeling."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p4", "contents": "My goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they\u2019ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p6", "contents": "So what exactly is a language model?"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p7", "contents": "In The Illustrated Word2vec, we\u2019ve looked at what a language model is \u2013 basically a machine learning model that is able to look at part of a sentence and predict the next word. The most famous language models are smartphone keyboards that suggest the next word based on what you\u2019ve currently typed."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p8", "contents": "In this sense, we can say that the GPT-2 is basically the next word prediction feature of a keyboard app, but one that is much larger and more sophisticated than what your phone has. The GPT-2 was trained on a massive 40GB dataset called WebText that the OpenAI researchers crawled from the internet as part of the research effort. To compare in terms of storage size, the keyboard app I use, SwiftKey, takes up 78MBs of space. The smallest variant of the trained GPT-2, takes up 500MBs of storage to store all of its parameters. The largest GPT-2 variant is 13 times the size so it could take up more than 6.5 GBs of storage space."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p9", "contents": "One great way to experiment with GPT-2 is using the AllenAI GPT-2 Explorer. It uses GPT-2 to display ten possible predictions for the next word (alongside their probability score). You can select a word then see the next list of predictions to continue writing the passage."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p10", "contents": "As we\u2019ve seen in The Illustrated Transformer, the original transformer model is made up of an encoder and decoder \u2013 each is a stack of what we can call transformer blocks. That architecture was appropriate because the model tackled machine translation  \u2013 a problem where encoder-decoder architectures have been successful in the past."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p11", "contents": "A lot of the subsequent research work saw the architecture shed either the encoder or decoder, and use just one stack of transformer blocks \u2013 stacking them up as high as practically possible, feeding them massive amounts of training text, and throwing vast amounts of compute at them (hundreds of thousands of dollars to train some of these language models, likely millions in the case of AlphaStar)."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p12", "contents": "How high can we stack up these blocks? It turns out that\u2019s one of the main distinguishing factors between the different GPT2 model sizes:"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p13", "contents": "The GPT-2 is built using transformer decoder blocks. BERT, on the other hand, uses transformer encoder blocks. We will examine the difference in a following section. But one key difference between the two is that GPT2, like traditional language models, outputs one token at a time. Let\u2019s for example prompt a well-trained GPT-2 to recite the first law of robotics:"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p14", "contents": "The way these models actually work is that after each token is produced, that token is added to the sequence of inputs. And that new sequence becomes the input to the model in its next step. This is an idea called \u201cauto-regression\u201d. This is one of the ideas that made RNNs unreasonably effective."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p15", "contents": "The GPT2, and some later models like TransformerXL and XLNet are auto-regressive in nature. BERT is not. That is a trade off. In losing auto-regression, BERT gained the ability to incorporate the context on both sides of a word to gain better results. XLNet brings back autoregression while finding an alternative way to incorporate the context on both sides."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p16", "contents": "The initial transformer paper introduced two types of transformer blocks:"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p17", "contents": "First is the encoder block:"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p18", "contents": "Second, there\u2019s the decoder block which has a small architectural variation from the encoder block \u2013 a layer to allow it to pay attention to specific segments from the encoder:"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p19", "contents": "One key difference in the self-attention layer here, is that it masks future tokens \u2013 not by changing the word to [mask] like BERT, but by interfering in the self-attention calculation blocking information from tokens that are to the right of the position being calculated."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p20", "contents": "If, for example, we\u2019re to highlight the path of position #4, we can see that it is only allowed to attend to the present and previous tokens:"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p21", "contents": "It\u2019s important that the distinction between self-attention (what BERT uses) and masked self-attention (what GPT-2 uses) is clear. A normal self-attention block allows a position to peak at tokens to its right. Masked self-attention prevents that from happening:"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p22", "contents": "Subsequent to the original paper, Generating Wikipedia by Summarizing Long Sequences proposed another arrangement of the transformer block that is capable of doing language modeling. This model threw away the Transformer encoder. For that reason, let\u2019s call the model the \u201cTransformer-Decoder\u201d. This early transformer-based language model was made up of a stack of six transformer decoder blocks:"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p23", "contents": "These blocks were very similar to the original decoder blocks, except they did away with that second self-attention layer. A similar architecture was examined in Character-Level Language Modeling with Deeper Self-Attention to create a language model that predicts one letter/character at a time."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p24", "contents": "The OpenAI GPT-2 model uses these decoder-only blocks."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p25", "contents": "Look inside and you will see,\nThe words are cutting deep inside my brain.\nThunder burning, quickly burning,\nKnife of words is driving me insane, insane yeah.\n~Budgie"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p26", "contents": "Let\u2019s lay a trained GPT-2 on our surgery table and look at how it works."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p27", "contents": "The simplest way to run a trained GPT-2 is to allow it to ramble on its own (which is technically called generating unconditional samples) \u2013 alternatively, we can give it a prompt to have it speak about a certain topic (a.k.a generating interactive conditional samples). In the rambling case, we can simply hand it the start token and have it start generating words (the trained model uses <|endoftext|> as its start token. Let\u2019s call it <s> instead)."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p29", "contents": "The model only has one input token, so that path would be the only active one. The token is processed successively through all the layers, then a vector is produced along that path. That vector can be scored against the model\u2019s vocabulary (all the words the model knows, 50,000 words in the case of GPT-2). In this case we selected the token with the highest probability, \u2018the\u2019. But we can certainly mix things up \u2013 you know how if you keep clicking the suggested word in your keyboard app, it sometimes can stuck in repetitive loops where the only way out is if you click the second or third suggested word. The same can happen here. GPT-2 has a parameter called top-k that we can use to have the model consider sampling words other than the top word (which is the case when top-k = 1)."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p30", "contents": "In the next step, we add the output from the first step to our input sequence, and have the model make its next prediction:"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p31", "contents": "Notice that the second path is the only that\u2019s active in this calculation. Each layer of GPT-2 has retained its own interpretation of the first token and will use it in processing the second token (we\u2019ll get into more detail about this in the following section about self-attention). GPT-2 does not re-interpret the first token in light of the second token."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p32", "contents": "Let\u2019s look at more details to get to know the model more intimately. Let\u2019s start from the input. As in other NLP models we\u2019ve discussed before, the model looks up the embedding of the input word in its embedding matrix \u2013 one of the components we get as part of a trained model."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p33", "contents": "So in the beginning, we look up the embedding of the start token <s> in the embedding matrix. Before handing that to the first block in the model, we need to incorporate positional encoding \u2013 a signal that indicates the order of the words in the sequence to the transformer blocks. Part of the trained model is a matrix that contains a positional encoding vector for each of the 1024 positions in the input."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p34", "contents": "With this, we\u2019ve covered how input words are processed before being handed to the first transformer block. We also know two of the weight matrices that constitute the trained GPT-2."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p35", "contents": "The first block can now process the token by first passing it through the self-attention process, then passing it through its neural network layer. Once the first transformer block processes the token, it sends its resulting vector up the stack to be processed by the next block. The process is identical in each block, but each block has its own weights in both self-attention and the neural network sublayers."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p36", "contents": "Language heavily relies on context. For example, look at the second law:"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p37", "contents": "I have highlighted three places in the sentence where the words are referring to other words. There is no way to understand or process these words without incorporating the context they are referring to. When a model processes this sentence, it has to be able to know that:"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p38", "contents": "This is what self-attention does. It bakes in the model\u2019s understanding of relevant and associated words that explain the context of a certain word before processing that word (passing it through a neural network). It does that by assigning scores to how relevant each word in the segment is, and adding up their vector representation."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p39", "contents": "As an example, this self-attention layer in the top block is paying attention to \u201ca robot\u201d when it processes the word \u201cit\u201d. The vector it will pass to its neural network is a sum of the vectors for each of the three words multiplied by their scores."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p40", "contents": "Self-attention is processed along the path of each token in the segment. The significant components are three vectors:"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p41", "contents": "A crude analogy is to think of it like searching through a filing cabinet. The query is like a sticky note with the topic you\u2019re researching. The keys are like the labels of the folders inside the cabinet. When you match the tag with a sticky note, we take out the contents of that folder, these contents are the value vector. Except you\u2019re not only looking for one value, but a blend of values from a blend of folders."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p42", "contents": "Multiplying the query vector by each key vector produces a score for each folder (technically: dot product followed by softmax)."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p43", "contents": "We multiply each value by its score and sum up \u2013 resulting in our self-attention outcome."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p44", "contents": "This weighted blend of value vectors results in a vector that paid 50% of its \u201cattention\u201d to the word robot, 30% to the word a, and 19% to the word it. Later in the post, we\u2019ll got deeper into self-attention. But first, let\u2019s continue our journey up the stack towards the output of the model."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p45", "contents": "When the top block in the model produces its output vector (the result of its own self-attention followed by its own neural network), the model multiplies that vector by the embedding matrix."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p46", "contents": "Recall that each row in the embedding matrix corresponds to the embedding of a word in the model\u2019s vocabulary. The result of this multiplication is interpreted as a score for each word in the model\u2019s vocabulary."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p47", "contents": "We can simply select the token with the highest score (top_k = 1). But better results are achieved if the model considers other words as well. So a better strategy is to sample a word from the entire list using the score as the probability of selecting that word (so words with a higher score have a higher chance of being selected). A middle ground is setting top_k to 40, and having the model consider the 40 words with the highest scores."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p48", "contents": "With that, the model has completed an iteration resulting in outputting a single word. The model continues iterating until the entire context is generated (1024 tokens) or until an end-of-sequence token is produced."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p49", "contents": "And there we have it. A run down of how the GPT2 works. If you\u2019re curious to know exactly what happens inside the self-attention layer, then the following bonus section is for you. I created it to introduce more visual language to describe self-attention in order to make describing later transformer models easier to examine and describe (looking at you, TransformerXL and XLNet)."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p50", "contents": "I\u2019d like to note a few oversimplifications in this post:"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p51", "contents": "Earlier in the post we showed this image to showcase self-attention being applied in a layer that is processing the word it:"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p52", "contents": "In this section, we\u2019ll look at the details of how that is done. Note that we\u2019ll look at it in a way to try to make sense of what happens to individual words. That\u2019s why we\u2019ll be showing many single vectors. The actual implementations are done by multiplying giant matrices together. But I want to focus on the intuition of what happens on a word-level here."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p53", "contents": "Let\u2019s start by looking at the original self-attention as it\u2019s calculated in an encoder block. Let\u2019s look at a toy transformer block that can only process four tokens at a time."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p54", "contents": "Self-attention is applied through three main steps:"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p55", "contents": "Let\u2019s focus on the first path. We\u2019ll take its query, and compare against all the keys. That produces a score for each key. The first step in self-attention is to calculate the three vectors for each token path (let\u2019s ignore attention heads for now):"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p56", "contents": "Now that we have the vectors, we use the query and key vectors only for step #2. Since we\u2019re focused on the first token, we multiply its query by all the other key vectors resulting in a score for each of the four tokens."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p57", "contents": "We can now multiply the scores by the value vectors. A value with a high score will constitute a large portion of the resulting vector after we sum them up."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p58", "contents": "If we do the same operation for each path, we end up with a vector representing each token containing the appropriate context of that token. Those are then presented to the next sublayer in the transformer block (the feed-forward neural network):"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p59", "contents": "Now that we\u2019ve looked inside a transformer\u2019s self-attention step, let\u2019s proceed to look at masked self-attention. Masked self-attention is identical to self-attention except when it comes to step #2. Assuming the model only has two tokens as input and we\u2019re observing the second token. In this case, the last two tokens are masked. So the model interferes in the scoring step. It basically always scores the future tokens as 0 so the model can\u2019t peak to future words:"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p60", "contents": "This masking is often implemented as a matrix called an attention mask. Think of a sequence of four words (\u201crobot must obey orders\u201d, for example). In a language modeling scenario, this sequence is absorbed in four steps \u2013 one per word (assuming for now that every word is a token). As these models work in batches, we can assume a batch size of 4 for this toy model that will process the entire sequence (with its four steps) as one batch."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p61", "contents": "In matrix form, we calculate the scores by multiplying a queries matrix by a keys matrix. Let\u2019s visualize it as follows, except instead of the word, there would be the query (or key) vector associated with that word in that cell:"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p62", "contents": "After the multiplication, we slap on our attention mask triangle. It set the cells we want to mask to -infinity or a very large negative number (e.g. -1 billion in GPT2):"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p63", "contents": "Then, applying softmax on each row produces the actual scores we use for self-attention:"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p64", "contents": "What this scores table means is the following:"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p65", "contents": "Let\u2019s get into more detail on GPT-2\u2019s masked attention."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p66", "contents": "We can make the GPT-2 operate exactly as masked self-attention works. But during evaluation, when our model is only adding one new word after each iteration, it would be inefficient to recalculate self-attention along earlier paths for tokens which have already been processed."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p67", "contents": "In this case, we process the first token (ignoring <s> for now)."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p68", "contents": "GPT-2 holds on to the key and value vectors of the the a token. Every self-attention layer holds on to its respective key and value vectors for that token:"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p69", "contents": "Now in the next iteration, when the model processes the word robot, it does not need to generate query, key, and value queries for the a token. It just reuses the ones it saved from the first iteration:"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p70", "contents": "Let\u2019s assume the model is processing the word it. If we\u2019re talking about the bottom block, then its input for that token would be the embedding of it + the positional encoding for slot #9:"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p71", "contents": "Every block in a transformer has its own weights (broken down later in the post). The first we encounter is the weight matrix that we use to create the queries, keys, and values."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p72", "contents": "The multiplication results in a vector that\u2019s basically a concatenation of the query, key, and value vectors for the word it."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p73", "contents": "In the previous examples, we dove straight into self-attention ignoring the \u201cmulti-head\u201d part. It would be useful to shed some light on that concept now. Self attention is conducted multiple times on different parts of the Q,K,V vectors. \u201cSplitting\u201d attention heads is simply reshaping the long vector into a matrix. The small GPT2 has 12 attention heads, so that would be the first dimension of the reshaped matrix:"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p74", "contents": "In the previous examples, we\u2019ve looked at what happens inside one attention head. One way to think of multiple attention-heads is like this (if we\u2019re to only visualize three of the twelve attention heads):"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p75", "contents": "We can now proceed to scoring \u2013 knowing that we\u2019re only looking at one attention head (and that all the others are conducting a similar operation):"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p76", "contents": "Now the token can get scored against all of keys of the other tokens (that were calculated in attention head #1 in previous iterations):"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p77", "contents": "As we\u2019ve seen before, we now multiply each value with its score, then sum them up, producing the result of self-attention for attention-head #1:"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p78", "contents": "The way we deal with the various attention heads is that we first concatenate them into one vector:"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p79", "contents": "But the vector isn\u2019t ready to be sent to the next sublayer just yet. We need to first turn this Frankenstein\u2019s-monster of hidden states into a homogenous representation."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p80", "contents": "We\u2019ll let the model learn how to best map concatenated self-attention results into a vector that the feed-forward neural network can deal with. Here comes our second large weight matrix that projects the results of the attention heads into the output vector of the self-attention sublayer:"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p81", "contents": "And with this, we have produced the vector we can send along to the next layer:"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p82", "contents": "The fully-connected neural network is where the block processes its input token after self-attention has included the appropriate context in its representation. It is made up of two layers. The first layer is four times the size of the model (Since GPT2 small is 768, this network would have 768*4 = 3072 units). Why four times? That\u2019s just the size the original transformer rolled with (model dimension was 512 and layer #1 in that model was 2048). This seems to give transformer models enough representational capacity to handle the tasks that have been thrown at them so far."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p83", "contents": "The second layer projects the result from the first layer back into model dimension (768 for the small GPT2). The result of this multiplication is the result of the transformer block for this token."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p84", "contents": "That\u2019s the most detailed version of the transformer block we\u2019ll get into! You now pretty much have the vast majority of the picture of what happens inside of a transformer language model. To recap, our brave input vector encounters these weight matrices:"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p85", "contents": "And each block has its own set of these weights. On the other hand, the model has only one token embedding matrix and one positional encoding matrix:"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p86", "contents": "If you want to see all the parameters of the model, then I have tallied them here:"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p87", "contents": "They add up to 124M parameters instead of 117M for some reason. I\u2019m not sure why, but that\u2019s how many of them seems to be in the published code (please correct me if I\u2019m wrong)."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p88", "contents": "The decoder-only transformer keeps showing promise beyond language modeling. There are plenty of applications where it has shown success which can be described by similar visuals as the above. Let\u2019s close this post by looking at some of these applications"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p89", "contents": "An encoder is not required to conduct translation. The same task can be addressed by a decoder-only transformer:"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p90", "contents": "This is the task that the first decoder-only transformer was trained on. Namely, it was trained to read a wikipedia article (without the opening section before the table of contents), and to summarize it. The actual opening sections of the articles were used as the labels in the training datasest:"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p91", "contents": "The paper trained the model against wikipedia articles, and thus the trained model was able to summarize articles:"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p92", "contents": "In Sample Efficient Text Summarization Using a Single Pre-Trained Transformer, a decoder-only transformer is first pre-trained on language modeling, then finetuned to do summarization. It turns out to achieve better results than a pre-trained encoder-decoder transformer in limited data settings."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p93", "contents": "The GPT2 paper also shows results of summarization after pre-training the model on language modeling."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p94", "contents": "The Music Transformer uses a decoder-only transformer to generate music with expressive timing and dynamics. \u201cMusic Modeling\u201d is just like language modeling \u2013 just let the model learn music in an unsupervised way, then have it sample outputs (what we called \u201crambling\u201d, earlier)."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p95", "contents": "You might be curious as to how music is represented in this scenario. Remember that language modeling can be done through vector representations of either characters, words, or tokens that are parts of words. With a musical performance (let\u2019s think about the piano for now), we have to represent the notes, but also velocity \u2013 a measure of how hard the piano key is pressed."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p96", "contents": "A performance is just a series of these one-hot vectors. A midi file can be converted into such a format. The paper has the following example input sequence:"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p97", "contents": "The one-hot vector representation for this input sequence would look like this:"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p98", "contents": "I love a visual in the paper that showcases self-attention in the Music Transformer. I\u2019ve added some annotations to it here:"}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p99", "contents": "If you\u2019re unclear on this representation of musical notes, check out this video."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p100", "contents": "This concludes our journey into the GPT2, and our exploration of its parent model, the decoder-only transformer. I hope that you come out of this post with a better understanding of self-attention and more comfort that you understand more of what goes on inside a transformer."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p101", "contents": "Thanks to Lukasz Kaiser, Mathias M\u00fcller, Peter J. Liu, Ryan Sepassi and Mohammad Saleh for feedback on earlier versions of this post."}
{"id": "http://jalammar.github.io//illustrated-gpt2/_p102", "contents": "Comments or corrections? Please tweet me at @JayAlammar"}
{"id": "http://jalammar.github.io//gentle-visual-intro-to-data-analysis-python-pandas/_p0", "contents": "Visualizing machine learning one concept at a time.@JayAlammar on Twitter."}
{"id": "http://jalammar.github.io//gentle-visual-intro-to-data-analysis-python-pandas/_p1", "contents": "Discussions:\nHacker News (195 points, 51 comments), Reddit r/Python (140 points, 18 comments)\n"}
{"id": "http://jalammar.github.io//gentle-visual-intro-to-data-analysis-python-pandas/_p2", "contents": "If you\u2019re planning to learn data analysis, machine learning, or data science tools in python, you\u2019re most likely going to be using the wonderful pandas library. Pandas is an open source library for data manipulation and analysis in python."}
{"id": "http://jalammar.github.io//gentle-visual-intro-to-data-analysis-python-pandas/_p3", "contents": "One of the easiest ways to think about that, is that you can load tables (and excel files) and then slice and dice them in multiple ways:"}
{"id": "http://jalammar.github.io//gentle-visual-intro-to-data-analysis-python-pandas/_p5", "contents": "Pandas allows us to load a spreadsheet and manipulate it programmatically in python. The central concept in pandas is the type of object called a DataFrame \u2013 basically a table of values which has a label for each row and column. Let\u2019s load this basic CSV file containing data from a music streaming service:"}
{"id": "http://jalammar.github.io//gentle-visual-intro-to-data-analysis-python-pandas/_p6", "contents": "Now the variable df is a pandas DataFrame:"}
{"id": "http://jalammar.github.io//gentle-visual-intro-to-data-analysis-python-pandas/_p8", "contents": "We can select any column using its label:"}
{"id": "http://jalammar.github.io//gentle-visual-intro-to-data-analysis-python-pandas/_p10", "contents": "We can select one or multiple rows using their numbers:"}
{"id": "http://jalammar.github.io//gentle-visual-intro-to-data-analysis-python-pandas/_p12", "contents": "We can select any slice of the table using a both column label and row numbers using loc (but here it would be inclusive of both bounding row numbers):"}
{"id": "http://jalammar.github.io//gentle-visual-intro-to-data-analysis-python-pandas/_p14", "contents": "Now it gets more interesting. We can easily filter rows using the values of a specific row. For example, here are our jazz musicians:"}
{"id": "http://jalammar.github.io//gentle-visual-intro-to-data-analysis-python-pandas/_p16", "contents": "Here are the artists who have more than 1,800,000 listeners:"}
{"id": "http://jalammar.github.io//gentle-visual-intro-to-data-analysis-python-pandas/_p18", "contents": "Many datasets you\u2019ll deal with in your data science journey will have missing values. Let\u2019s say our data frame has a missing value:"}
{"id": "http://jalammar.github.io//gentle-visual-intro-to-data-analysis-python-pandas/_p20", "contents": "Pandas provides multiple ways to deal with this. The easiest is to just drop rows with missing values:"}
{"id": "http://jalammar.github.io//gentle-visual-intro-to-data-analysis-python-pandas/_p22", "contents": "Another way would be to fill-in the missing value using fillna() (with 0, for example)."}
{"id": "http://jalammar.github.io//gentle-visual-intro-to-data-analysis-python-pandas/_p23", "contents": "Things start to get really interesting when you start grouping rows with certain criteria and aggregating their data. For example, let\u2019s group our dataset by genre and see how many listeners and plays each genre has:"}
{"id": "http://jalammar.github.io//gentle-visual-intro-to-data-analysis-python-pandas/_p25", "contents": "Pandas grouped the the two \u201cJazz\u201d rows into one, and since we used sum() for aggregation, it added together the listeners and plays for the two Jazz artists and shows the sums in the combined Jazz column."}
{"id": "http://jalammar.github.io//gentle-visual-intro-to-data-analysis-python-pandas/_p26", "contents": "This is not only nifty, but is an extremely powerful data analysis method. Now that you know groupby(), you wield immense power to fold datasets and uncover insights from them. Aggregation is the first pillar of statistical wisdom, and so is one of the foundational tools of statistics."}
{"id": "http://jalammar.github.io//gentle-visual-intro-to-data-analysis-python-pandas/_p27", "contents": "In addition to sum(), pandas provides multiple aggregation functions including mean() to compute the average value, min(), max(), and multiple other functions. More on groupyby() in the Group By User Guide."}
{"id": "http://jalammar.github.io//gentle-visual-intro-to-data-analysis-python-pandas/_p28", "contents": "If you use groupby() to its full potential, and use nothing else in pandas, then you\u2019d be putting pandas to great use. But the library can still offer you much, much more."}
{"id": "http://jalammar.github.io//gentle-visual-intro-to-data-analysis-python-pandas/_p29", "contents": "Often in the data analysis process, we find ourselves needing to create new columns from existing ones. Pandas makes this a breeze."}
{"id": "http://jalammar.github.io//gentle-visual-intro-to-data-analysis-python-pandas/_p31", "contents": "By telling Pandas to divide a column by another column, it realizes that we want to do is divide the individual values respectively (i.e. each row\u2019s \u201cPlays\u201d value by that row\u2019s \u201cListeners\u201d value)."}
{"id": "http://jalammar.github.io//gentle-visual-intro-to-data-analysis-python-pandas/_p32", "contents": "You can get started playing with Pandas in your browser right now through this basic notebook hosted in Google Colab. The notebook is also available on Github if you have your local environment set up."}
{"id": "http://jalammar.github.io//gentle-visual-intro-to-data-analysis-python-pandas/_p33", "contents": "Want to learn more? Be sure to check out the 10 Minutes to pandas tutorial in the official Pandas docs. Thanks to Marc Garcia for initiating the thoughts for these visualizations and continuing to improve the pandas documentation."}
{"id": "http://jalammar.github.io//gentle-visual-intro-to-data-analysis-python-pandas/_p34", "contents": "Did you find this tutorial helpful? Any suggestions for improvement? Please let me know (@JayAlammar) know on Twitter. Thanks!"}
{"id": "http://jalammar.github.io//visual-numpy/_p0", "contents": "Visualizing machine learning one concept at a time.@JayAlammar on Twitter."}
{"id": "http://jalammar.github.io//visual-numpy/_p1", "contents": "Discussions:\nHacker News (366 points, 21 comments), Reddit r/MachineLearning (256 points, 18 comments)\n\n\nTranslations: Chinese 1, Chinese 2, Japanese"}
{"id": "http://jalammar.github.io//visual-numpy/_p2", "contents": "The NumPy package is the workhorse of data analysis, machine learning, and scientific computing in the python ecosystem. It vastly simplifies manipulating and crunching vectors and matrices. Some of python\u2019s leading package rely on NumPy as a fundamental piece of their infrastructure (examples include scikit-learn, SciPy, pandas, and tensorflow). Beyond the ability to slice and dice numeric data, mastering numpy will give you an edge when dealing and debugging with advanced usecases in these libraries."}
{"id": "http://jalammar.github.io//visual-numpy/_p3", "contents": "In this post, we\u2019ll look at some of the main ways to use NumPy and how it can represent different types of data (tables, images, text\u2026etc) before we can serve them to machine learning models."}
{"id": "http://jalammar.github.io//visual-numpy/_p4", "contents": "We can create a NumPy array (a.k.a. the mighty ndarray) by passing a python list to it and using ` np.array()`. In this case, python creates the array we can see on the right here:"}
{"id": "http://jalammar.github.io//visual-numpy/_p5", "contents": "There are often cases when we want NumPy to initialize the values of the array for us. NumPy provides methods like ones(), zeros(), and random.random() for these cases. We just pass them the number of elements we want it to generate:"}
{"id": "http://jalammar.github.io//visual-numpy/_p6", "contents": "Once we\u2019ve created our arrays, we can start to manipulate them in interesting ways."}
{"id": "http://jalammar.github.io//visual-numpy/_p7", "contents": "Let\u2019s create two NumPy arrays to showcase their usefulness. We\u2019ll call them data and ones:"}
{"id": "http://jalammar.github.io//visual-numpy/_p9", "contents": "Adding them up position-wise (i.e. adding the values of each row) is as simple as typing data + ones:"}
{"id": "http://jalammar.github.io//visual-numpy/_p11", "contents": "When I started learning such tools, I found it refreshing that an abstraction like this makes me not have to program such a calculation in loops. It\u2019s a wonderful abstraction that allows you to think about problems at a higher level."}
{"id": "http://jalammar.github.io//visual-numpy/_p12", "contents": "And it\u2019s not only addition that we can do this way:"}
{"id": "http://jalammar.github.io//visual-numpy/_p14", "contents": "There are often cases when we want to carry out an operation between an array and a single number (we can also call this an operation between a vector and a scalar). Say, for example, our array represents distance in miles, and we want to convert it to kilometers. We simply say data * 1.6:"}
{"id": "http://jalammar.github.io//visual-numpy/_p16", "contents": "See how NumPy understood that operation to mean that the multiplication should happen with each cell? That concept is called broadcasting, and it\u2019s very useful."}
{"id": "http://jalammar.github.io//visual-numpy/_p17", "contents": "We can index and slice NumPy arrays in all the ways we can slice python lists:"}
{"id": "http://jalammar.github.io//visual-numpy/_p18", "contents": "Additional benefits NumPy gives us are aggregation functions:"}
{"id": "http://jalammar.github.io//visual-numpy/_p19", "contents": "In addition to min, max, and sum, you get all the greats like mean to get the average, prod to get the result of multiplying all the elements together, std to get standard deviation, and plenty of others."}
{"id": "http://jalammar.github.io//visual-numpy/_p20", "contents": "All the examples we\u2019ve looked at deal with vectors in one dimension. A key part of the beauty of NumPy is its ability to apply everything we\u2019ve looked at so far to any number of dimensions."}
{"id": "http://jalammar.github.io//visual-numpy/_p21", "contents": "We can pass python lists of lists in the following shape to have NumPy create a matrix to represent them:"}
{"id": "http://jalammar.github.io//visual-numpy/_p22", "contents": "We can also use the same methods we mentioned above (ones(), zeros(), and random.random()) as long as we give them a tuple describing the dimensions of the matrix we are creating:"}
{"id": "http://jalammar.github.io//visual-numpy/_p24", "contents": "We can add and multiply matrices using arithmetic operators (+-*/) if the two matrices are the same size. NumPy handles those as position-wise operations:"}
{"id": "http://jalammar.github.io//visual-numpy/_p26", "contents": "We can get away with doing these arithmetic operations on matrices of different size only if the different dimension is one (e.g. the matrix has only one column or one row), in which case NumPy uses its broadcast rules for that operation:"}
{"id": "http://jalammar.github.io//visual-numpy/_p28", "contents": "A key distinction to make with arithmetic is the case of matrix multiplication using the dot product. NumPy gives every matrix a dot() method we can use to carry-out dot product operations with other matrices:"}
{"id": "http://jalammar.github.io//visual-numpy/_p30", "contents": "I\u2019ve added matrix dimensions at the bottom of this figure to stress that the two matrices have to have the same dimension on the side they face each other with. You can visualize this operation as looking like this:"}
{"id": "http://jalammar.github.io//visual-numpy/_p32", "contents": "Indexing and slicing operations become even more useful when we\u2019re manipulating matrices:"}
{"id": "http://jalammar.github.io//visual-numpy/_p34", "contents": "We can aggregate matrices the same way we aggregated vectors:"}
{"id": "http://jalammar.github.io//visual-numpy/_p36", "contents": "Not only can we aggregate all the values in a matrix, but we can also aggregate across the rows or columns by using the axis parameter:"}
{"id": "http://jalammar.github.io//visual-numpy/_p38", "contents": "A common need when dealing with matrices is the need to rotate them. This is often the case when we need to take the dot product of two matrices and need to align the dimension they share. NumPy arrays have a convenient property called T to get the transpose of a matrix:"}
{"id": "http://jalammar.github.io//visual-numpy/_p40", "contents": "In more advanced use case, you may find yourself needing to switch the dimensions of a certain matrix. This is often the case in machine learning applications where a certain model expects a certain shape for the inputs that is different from your dataset. NumPy\u2019s reshape() method is useful in these cases. You just pass it the new dimensions you want for the matrix. You can pass -1 for a dimension and NumPy can infer the correct dimension based on your matrix:"}
{"id": "http://jalammar.github.io//visual-numpy/_p42", "contents": "NumPy can do everything we\u2019ve mentioned in any number of dimensions. Its central data structure is called ndarray (N-Dimensional Array) for a reason."}
{"id": "http://jalammar.github.io//visual-numpy/_p43", "contents": "In a lot of ways, dealing with a new dimension is just adding a comma to the parameters of a NumPy function:"}
{"id": "http://jalammar.github.io//visual-numpy/_p44", "contents": "Note: Keep in mind that when you print a 3-dimensional NumPy array, the text output visualizes the array differently than shown here.  NumPy\u2019s order for printing n-dimensional arrays is that the last axis is looped over the fastest, while the first is the slowest. Which means that np.ones((4,3,2)) would be printed as:"}
{"id": "http://jalammar.github.io//visual-numpy/_p45", "contents": "And now for the payoff. Here are some examples of the useful things NumPy will help you through."}
{"id": "http://jalammar.github.io//visual-numpy/_p46", "contents": "Implementing mathematical formulas that work on matrices and vectors is a key use case to consider NumPy for. It\u2019s why NumPy is the darling of the scientific python community. For example, consider the mean square error formula that is central to supervised machine learning models tackling regression problems:"}
{"id": "http://jalammar.github.io//visual-numpy/_p48", "contents": "Implementing this is a breeze in NumPy:"}
{"id": "http://jalammar.github.io//visual-numpy/_p50", "contents": "The beauty of this is that numpy does not care if predictions and labels contain one or a thousand values (as long as they\u2019re both the same size). We can walk through an example stepping sequentially through the four operations in that line of code:"}
{"id": "http://jalammar.github.io//visual-numpy/_p52", "contents": "Both the predictions and labels vectors contain three values. Which means n has a value of three. After we carry out the subtraction, we end up with the values looking like this:"}
{"id": "http://jalammar.github.io//visual-numpy/_p54", "contents": "Then we can square the values in the vector:"}
{"id": "http://jalammar.github.io//visual-numpy/_p56", "contents": "Now we sum these values:"}
{"id": "http://jalammar.github.io//visual-numpy/_p58", "contents": "Which results in the error value for that prediction and a score for the quality of the model."}
{"id": "http://jalammar.github.io//visual-numpy/_p59", "contents": "Think of all the data types you\u2019ll need to crunch and build models around (spreadsheets, images, audio\u2026etc). So many of them are perfectly suited for representation in an n-dimensional array:"}
{"id": "http://jalammar.github.io//visual-numpy/_p60", "contents": "Here\u2019s a look at a slice of an audio file:"}
{"id": "http://jalammar.github.io//visual-numpy/_p61", "contents": "The same goes for time-series data (for example, the price of a stock over time)."}
{"id": "http://jalammar.github.io//visual-numpy/_p62", "contents": "An image is a matrix of pixels of size (height x width)."}
{"id": "http://jalammar.github.io//visual-numpy/_p63", "contents": "Here\u2019s a look at a slice of an image file:"}
{"id": "http://jalammar.github.io//visual-numpy/_p64", "contents": "If the image is colored, then each pixel is represented by three numbers - a value for each of red, green, and blue. In that case we need a 3rd dimension (because each cell can only contain one number). So a colored image is represented by an ndarray of dimensions: (height x width x 3)."}
{"id": "http://jalammar.github.io//visual-numpy/_p65", "contents": "If we\u2019re dealing with text, the story is a little different. The numeric representation of text requires a step of building a vocabulary (an inventory of all the unique words the model knows) and an embedding step. Let us see the steps of numerically representing this (translated) quote by an ancient spirit:"}
{"id": "http://jalammar.github.io//visual-numpy/_p66", "contents": "\u201cHave the bards who preceded me left any theme unsung?\u201d"}
{"id": "http://jalammar.github.io//visual-numpy/_p67", "contents": "A model needs to look at a large amount of text before it can numerically represent the anxious words of this warrior poet. We can proceed to have it process a small dataset and use it to build a vocabulary (of 71,290 words):"}
{"id": "http://jalammar.github.io//visual-numpy/_p68", "contents": "The sentence can then be broken into an array of tokens (words or parts of words based on common rules):"}
{"id": "http://jalammar.github.io//visual-numpy/_p69", "contents": "We then replace each word by its id in the vocabulary table:"}
{"id": "http://jalammar.github.io//visual-numpy/_p70", "contents": "These ids still don\u2019t provide much information value to a model. So before feeding a sequence of words to a model, the tokens/words need to be replaced with their embeddings (50 dimension word2vec embedding in this case):"}
{"id": "http://jalammar.github.io//visual-numpy/_p71", "contents": "You can see that this NumPy array has the dimensions [embedding_dimension x sequence_length]. In practice these would be the other way around, but I\u2019m presenting it this way for visual consistency. For performance reasons, deep learning models tend to preserve the first dimension for batch size (because the model can be trained faster if multiple examples are trained in parallel). This is a clear case where reshape() becomes super useful. A model like BERT, for example, would expect its inputs in the shape: [batch_size, sequence_length, embedding_size]."}
{"id": "http://jalammar.github.io//visual-numpy/_p72", "contents": "This is now a numeric volume that a model can crunch and do useful things with. I left the other rows empty, but they\u2019d be filled with other examples for the model to train on (or predict)."}
{"id": "http://jalammar.github.io//visual-numpy/_p73", "contents": "(It turned out the poet\u2019s words in our example were immortalized more so than those of the other poets which trigger his anxieties. Born a slave owned by his father, Antarah\u2019s valor and command of language gained him his freedom and the mythical status of having his poem as one of seven poems suspended in the kaaba in pre-Islamic Arabia)."}
{"id": "http://jalammar.github.io//skipgram-recommender-talk/_p0", "contents": "Visualizing machine learning one concept at a time.@JayAlammar on Twitter."}
{"id": "http://jalammar.github.io//skipgram-recommender-talk/_p1", "contents": "I gave a talk at Qcon London this year. Watch it here:"}
{"id": "http://jalammar.github.io//skipgram-recommender-talk/_p2", "contents": "Intuition & Use-Cases of Embeddings in NLP & beyond [YouTube]"}
{"id": "http://jalammar.github.io//skipgram-recommender-talk/_p3", "contents": "https://www.infoq.com/presentations/nlp-word-embedding/ [infoQ]"}
{"id": "http://jalammar.github.io//skipgram-recommender-talk/_p4", "contents": "In this video, I introduced word embeddings and the word2vec algorithm.  I then proceeded to discuss how the word2vec algorithm is used to create recommendation engines in companies like Airbnb and Alibaba. I close by glancing at real-world consequences of popular recommendation systems like those of YouTube and Facebook."}
{"id": "http://jalammar.github.io//skipgram-recommender-talk/_p5", "contents": "My Illustrated Word2vec post used and built on the materials I created for this talk (but didn\u2019t include anything on the recommender application of word2vec). This was my first talk at a technical conference and I spent quite a bit of time preparing for it. In the six weeks prior to the conference I spent about 100 hours working on the presentation and ended up with 200 slides. It was an interesting balancing act of trying to make it introductory but not shallow, suitable for senior engineers and architects yet not necessarily ones who have machine learning experience."}
{"id": "http://jalammar.github.io//visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/_p0", "contents": "Visualizing machine learning one concept at a time.@JayAlammar on Twitter."}
{"id": "http://jalammar.github.io//visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/_p1", "contents": "Translations: Chinese (Simplified), Japanese, Korean, Russian\n\nWatch: MIT\u2019s Deep Learning State of the Art lecture referencing this post"}
{"id": "http://jalammar.github.io//visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/_p2", "contents": "May 25th update: New graphics (RNN animation, word embedding graph), color coding, elaborated on the final attention example."}
{"id": "http://jalammar.github.io//visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/_p3", "contents": "Note: The animations below are videos. Touch or hover on them (if you\u2019re using a mouse) to get play controls so you can pause if needed."}
{"id": "http://jalammar.github.io//visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/_p4", "contents": "Sequence-to-sequence models are deep learning models that have achieved a lot of success in tasks like machine translation, text summarization, and image captioning. Google Translate started using such a model in production in late 2016. These models are explained in the two pioneering papers (Sutskever et al., 2014, Cho et al., 2014)."}
{"id": "http://jalammar.github.io//visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/_p5", "contents": "I found, however, that understanding the model well enough to implement it requires unraveling a series of concepts that build on top of each other. I thought that a bunch of these ideas would be more accessible if expressed visually. That\u2019s what I aim to do in this post. You\u2019ll need some previous understanding of deep learning to get through this post. I hope it can be a useful companion to reading the papers mentioned above (and the attention papers linked later in the post)."}
{"id": "http://jalammar.github.io//visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/_p6", "contents": "A sequence-to-sequence model is a model that takes a sequence of items (words, letters, features of an images\u2026etc) and outputs another sequence of items. A trained model would work like this:"}
{"id": "http://jalammar.github.io//visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/_p8", "contents": "In neural machine translation, a sequence is a series of words, processed one after another. The output is, likewise, a series of words:"}
{"id": "http://jalammar.github.io//visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/_p9", "contents": "Under the hood, the model is composed of an encoder and a decoder."}
{"id": "http://jalammar.github.io//visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/_p10", "contents": "The encoder processes each item in the input sequence, it compiles the information it captures into a vector (called the context). After processing the entire input sequence, the encoder sends the context  over to the decoder, which begins producing the output sequence item by item."}
{"id": "http://jalammar.github.io//visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/_p12", "contents": "The same applies in the case of machine translation."}
{"id": "http://jalammar.github.io//visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/_p13", "contents": "The context  is a vector (an array of numbers, basically) in the case of machine translation. The encoder and decoder  tend to both be recurrent neural networks (Be sure to check out Luis Serrano\u2019s A friendly introduction to Recurrent Neural Networks for an intro to RNNs)."}
{"id": "http://jalammar.github.io//visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/_p14", "contents": "You can set the size of the context  vector when you set up your model. It is basically the number of hidden units in the encoder RNN. These visualizations show a vector of size 4, but in real world applications the context vector would be of a size like 256, 512, or 1024."}
{"id": "http://jalammar.github.io//visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/_p16", "contents": "By design, a RNN takes two inputs at each time step: an input (in the case of the encoder, one word from the input sentence), and a hidden state. The word, however, needs to be represented by a vector. To transform a word into a vector, we turn to the class of methods called \u201cword embedding\u201d algorithms. These turn words into vector spaces that capture a lot of the meaning/semantic information of the words (e.g. king - man + woman = queen)."}
{"id": "http://jalammar.github.io//visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/_p18", "contents": "Now that we\u2019ve introduced our main vectors/tensors, let\u2019s recap the mechanics of an RNN and establish a visual language to describe these models:"}
{"id": "http://jalammar.github.io//visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/_p20", "contents": "The next RNN step takes the second input vector and hidden state #1 to create the output of that time step. Later in the post, we\u2019ll use an animation like this to describe the vectors inside a neural machine translation model."}
{"id": "http://jalammar.github.io//visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/_p22", "contents": "In the following visualization, each pulse for the encoder or decoder  is that RNN processing its inputs and generating an output for that time step. Since the encoder and decoder  are both RNNs, each time step one of the RNNs does some processing, it updates its hidden state  based on its inputs and previous inputs it has seen."}
{"id": "http://jalammar.github.io//visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/_p23", "contents": "Let\u2019s look at the hidden states  for the encoder. Notice how the last hidden state  is actually the context  we pass along to the decoder."}
{"id": "http://jalammar.github.io//visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/_p25", "contents": "The decoder  also maintains a hidden states  that it passes from one time step to the next. We just didn\u2019t visualize it in this graphic because we\u2019re concerned with the major parts of the model for now."}
{"id": "http://jalammar.github.io//visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/_p26", "contents": "Let\u2019s now look at another way to visualize a sequence-to-sequence model. This animation will make it easier to understand the static graphics that describe these models. This is called an \u201cunrolled\u201d view where instead of showing the one decoder, we show a copy of it for each time step. This way we can look at the inputs and outputs of each time step."}
{"id": "http://jalammar.github.io//visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/_p28", "contents": "The context  vector turned out to be a bottleneck for these types of models. It made it challenging for the models to deal with long sentences. A solution was proposed in Bahdanau et al., 2014 and Luong et al., 2015. These papers introduced and refined a technique called \u201cAttention\u201d, which highly improved the quality of machine translation systems. Attention allows the model to focus on the relevant parts of the input sequence as needed."}
{"id": "http://jalammar.github.io//visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/_p31", "contents": "Let\u2019s continue looking at attention models at this high level of abstraction. An attention model differs from a classic sequence-to-sequence model in two main ways:"}
{"id": "http://jalammar.github.io//visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/_p32", "contents": "First, the encoder passes a lot more data to the decoder. Instead of passing the last hidden state of the encoding stage, the encoder passes all the hidden states  to the decoder:"}
{"id": "http://jalammar.github.io//visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/_p34", "contents": "Second, an attention decoder  does an extra step before producing its output. In order to focus on the parts of the input that are relevant to this decoding time step, the decoder  does the following:"}
{"id": "http://jalammar.github.io//visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/_p36", "contents": "This scoring exercise is done at each time step on the decoder side."}
{"id": "http://jalammar.github.io//visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/_p37", "contents": "Let us now bring the whole thing together in the following visualization and look at how the attention process works:"}
{"id": "http://jalammar.github.io//visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/_p39", "contents": "This is another way to look at which part of the input sentence we\u2019re paying attention to at each decoding step:"}
{"id": "http://jalammar.github.io//visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/_p40", "contents": "Note that the model isn\u2019t just mindless aligning the first word at the output with the first word from the input. It actually learned from the training phase how to align words in that language pair (French and English in our example). An example for how precise this mechanism can be comes from the attention papers listed above:"}
{"id": "http://jalammar.github.io//visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/_p42", "contents": "If you feel you\u2019re ready to learn the implementation, be sure to check TensorFlow\u2019s Neural Machine Translation (seq2seq) Tutorial."}
{"id": "http://jalammar.github.io//visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/_p44", "contents": "I hope you\u2019ve found this useful. These visuals are early iterations of a lesson on attention that is part of the Udacity Natural Language Processing Nanodegree Program. We go into more details in the lesson, including discussing applications and touching on more recent attention methods like the Transformer model from  Attention Is All You Need."}
{"id": "http://jalammar.github.io//visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/_p45", "contents": "Check out the trailer of the NLP Nanodegree Program:"}
{"id": "http://jalammar.github.io//visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/_p46", "contents": "I\u2019ve also created a few lessons as a part of Udacity\u2019s Machine Learning Nanodegree Program. The lessons I\u2019ve created cover Unsupervised Learning, as well as a jupyter notebook on movie recommendations using collaborative filtering."}
{"id": "http://jalammar.github.io//visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/_p47", "contents": "I\u2019d love any feedback you may have. Please reach me at @JayAlammmar."}
{"id": "http://jalammar.github.io//visualizing-pandas-pivoting-and-reshaping/_p0", "contents": "Visualizing machine learning one concept at a time.@JayAlammar on Twitter."}
{"id": "http://jalammar.github.io//visualizing-pandas-pivoting-and-reshaping/_p1", "contents": "I love using python\u2019s Pandas package for data analysis. The 10 Minutes to pandas is a great place to start learning how to use it for data analysis."}
{"id": "http://jalammar.github.io//visualizing-pandas-pivoting-and-reshaping/_p2", "contents": "Things get a lot more interesting once you\u2019re comfortable with the fundamentals and start with Reshaping and Pivot Tables. That guide shows some of the more interesting functions of reshaping data. Below are some visualizations to go along with the Pandas reshaping guide."}
{"id": "http://jalammar.github.io//visualizing-pandas-pivoting-and-reshaping/_p4", "contents": "(Thanks to /u/Xylon- for the correction)"}
{"id": "http://jalammar.github.io//visualizing-pandas-pivoting-and-reshaping/_p7", "contents": "Example #1 \u2013 without a parameter"}
{"id": "http://jalammar.github.io/#relu-visualization_p0", "contents": "Visualizing machine learning one concept at a time.@JayAlammar on Twitter."}
{"id": "http://jalammar.github.io/#relu-visualization_p1", "contents": "Check out the first video in my new series introducing the general public to AI and machine learning."}
{"id": "http://jalammar.github.io/#relu-visualization_p2", "contents": "My aim for this series is to help people integrate ML into their world-view away from all the hype and overpromises that plauge the topic."}
{"id": "http://jalammar.github.io/#relu-visualization_p3", "contents": "I had an incredible time organizing and speaking at the AI/machine learning track at QCon London 2020 where I invited and shared the stage with incredible speakers Vincent Warmerdam, Susanne Groothuis, Peter Elger, and Hien Luu."}
{"id": "http://jalammar.github.io/#relu-visualization_p4", "contents": "QCon is a global software conference for software engineers, architects, and team leaders, with over 1,600 attendees in London. All speakers have a software background."}
{"id": "http://jalammar.github.io/#relu-visualization_p5", "contents": "Translations: Russian"}
{"id": "http://jalammar.github.io/#relu-visualization_p6", "contents": "Progress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents \u201cthe biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search\u201d."}
{"id": "http://jalammar.github.io/#relu-visualization_p7", "contents": "This post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved."}
{"id": "http://jalammar.github.io/#relu-visualization_p8", "contents": "Alongside this post, I\u2019ve prepared a notebook. You can see it here the notebook or run it on colab."}
{"id": "http://jalammar.github.io/#relu-visualization_p9", "contents": "I had a great time speaking at the MIT Analytics Lab about some of my favorite ideas in natural language processing and their practical applications."}
{"id": "http://jalammar.github.io/#relu-visualization_p10", "contents": "Discussions:\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\n"}
{"id": "http://jalammar.github.io/#relu-visualization_p11", "contents": "Translations: Russian"}
{"id": "http://jalammar.github.io/#relu-visualization_p12", "contents": "This year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn\u2019t a particularly novel architecture \u2013 it\u2019s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we\u2019ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we\u2019ll look at applications for the decoder-only transformer beyond language modeling."}
{"id": "http://jalammar.github.io/#relu-visualization_p13", "contents": "My goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they\u2019ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve."}
{"id": "http://jalammar.github.io/#relu-visualization_p14", "contents": "Discussions:\nHacker News (366 points, 21 comments), Reddit r/MachineLearning (256 points, 18 comments)\n\n\nTranslations: Chinese 1, Chinese 2, Japanese"}
{"id": "http://jalammar.github.io/#relu-visualization_p15", "contents": "The NumPy package is the workhorse of data analysis, machine learning, and scientific computing in the python ecosystem. It vastly simplifies manipulating and crunching vectors and matrices. Some of python\u2019s leading package rely on NumPy as a fundamental piece of their infrastructure (examples include scikit-learn, SciPy, pandas, and tensorflow). Beyond the ability to slice and dice numeric data, mastering numpy will give you an edge when dealing and debugging with advanced usecases in these libraries."}
{"id": "http://jalammar.github.io/#relu-visualization_p16", "contents": "In this post, we\u2019ll look at some of the main ways to use NumPy and how it can represent different types of data (tables, images, text\u2026etc) before we can serve them to machine learning models."}
{"id": "http://jalammar.github.io/#relu-visualization_p17", "contents": "I gave a talk at Qcon London this year. Watch it here:"}
{"id": "http://jalammar.github.io/#relu-visualization_p18", "contents": "Intuition & Use-Cases of Embeddings in NLP & beyond [YouTube]"}
{"id": "http://jalammar.github.io/#relu-visualization_p19", "contents": "https://www.infoq.com/presentations/nlp-word-embedding/ [infoQ]"}
{"id": "http://jalammar.github.io/#relu-visualization_p20", "contents": "In this video, I introduced word embeddings and the word2vec algorithm.  I then proceeded to discuss how the word2vec algorithm is used to create recommendation engines in companies like Airbnb and Alibaba. I close by glancing at real-world consequences of popular recommendation systems like those of YouTube and Facebook."}
{"id": "http://jalammar.github.io/#relu-visualization_p21", "contents": "My Illustrated Word2vec post used and built on the materials I created for this talk (but didn\u2019t include anything on the recommender application of word2vec). This was my first talk at a technical conference and I spent quite a bit of time preparing for it. In the six weeks prior to the conference I spent about 100 hours working on the presentation and ended up with 200 slides. It was an interesting balancing act of trying to make it introductory but not shallow, suitable for senior engineers and architects yet not necessarily ones who have machine learning experience."}
{"id": "http://jalammar.github.io/#relu-visualization_p22", "contents": " Discussions:\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\n\n\n\nTranslations: Chinese (Simplified), Korean, Portuguese, Russian\n"}
{"id": "http://jalammar.github.io/#relu-visualization_p23", "contents": "I find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you\u2019ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you\u2019ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2)."}
{"id": "http://jalammar.github.io/#relu-visualization_p24", "contents": "Word2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines."}
{"id": "http://jalammar.github.io/#relu-visualization_p25", "contents": "In this post, we\u2019ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let\u2019s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?"}
{"id": "http://jalammar.github.io/#relu-visualization_p26", "contents": "Discussions:\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\n\n\nTranslations: Chinese (Simplified), Japanese, Korean, Persian, Russian"}
{"id": "http://jalammar.github.io/#relu-visualization_p27", "contents": "The year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It\u2019s been referred to as NLP\u2019s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks)."}
{"id": "http://jalammar.github.io/#relu-visualization_p28", "contents": "Discussions:\nHacker News (195 points, 51 comments), Reddit r/Python (140 points, 18 comments)\n"}
{"id": "http://jalammar.github.io/#relu-visualization_p29", "contents": "If you\u2019re planning to learn data analysis, machine learning, or data science tools in python, you\u2019re most likely going to be using the wonderful pandas library. Pandas is an open source library for data manipulation and analysis in python."}
{"id": "http://jalammar.github.io/#relu-visualization_p30", "contents": "One of the easiest ways to think about that, is that you can load tables (and excel files) and then slice and dice them in multiple ways:"}
{"id": "http://jalammar.github.io/#relu-visualization_p32", "contents": "Discussions:\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\n\n\nTranslations: Chinese (Simplified), Japanese, Korean, Russian\n\nWatch: MIT\u2019s Deep Learning State of the Art lecture referencing this post"}
{"id": "http://jalammar.github.io/#relu-visualization_p33", "contents": "In the previous post, we looked at Attention \u2013 a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer \u2013 a model that uses attention to boost the speed with which these models can be trained. The Transformers outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud\u2019s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let\u2019s try to break the model apart and look at how it functions."}
{"id": "http://jalammar.github.io/#relu-visualization_p34", "contents": "The Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard\u2019s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter."}
{"id": "http://jalammar.github.io/#relu-visualization_p35", "contents": "Let\u2019s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another."}
{"id": "http://jalammar.github.io/#relu-visualization_p36", "contents": "Translations: Chinese (Simplified), Japanese, Korean, Russian\n\nWatch: MIT\u2019s Deep Learning State of the Art lecture referencing this post"}
{"id": "http://jalammar.github.io/#relu-visualization_p37", "contents": "May 25th update: New graphics (RNN animation, word embedding graph), color coding, elaborated on the final attention example."}
{"id": "http://jalammar.github.io/#relu-visualization_p38", "contents": "Note: The animations below are videos. Touch or hover on them (if you\u2019re using a mouse) to get play controls so you can pause if needed."}
{"id": "http://jalammar.github.io/#relu-visualization_p39", "contents": "Sequence-to-sequence models are deep learning models that have achieved a lot of success in tasks like machine translation, text summarization, and image captioning. Google Translate started using such a model in production in late 2016. These models are explained in the two pioneering papers (Sutskever et al., 2014, Cho et al., 2014)."}
{"id": "http://jalammar.github.io/#relu-visualization_p40", "contents": "I found, however, that understanding the model well enough to implement it requires unraveling a series of concepts that build on top of each other. I thought that a bunch of these ideas would be more accessible if expressed visually. That\u2019s what I aim to do in this post. You\u2019ll need some previous understanding of deep learning to get through this post. I hope it can be a useful companion to reading the papers mentioned above (and the attention papers linked later in the post)."}
{"id": "http://jalammar.github.io/#relu-visualization_p41", "contents": "A sequence-to-sequence model is a model that takes a sequence of items (words, letters, features of an images\u2026etc) and outputs another sequence of items. A trained model would work like this:"}
{"id": "http://jalammar.github.io/#relu-visualization_p42", "contents": "I love using python\u2019s Pandas package for data analysis. The 10 Minutes to pandas is a great place to start learning how to use it for data analysis."}
{"id": "http://jalammar.github.io/#relu-visualization_p43", "contents": "Things get a lot more interesting once you\u2019re comfortable with the fundamentals and start with Reshaping and Pivot Tables. That guide shows some of the more interesting functions of reshaping data. Below are some visualizations to go along with the Pandas reshaping guide."}
{"id": "http://jalammar.github.io/#relu-visualization_p44", "contents": "In the previous post, we looked at the basic concepts of neural networks. Let us now take another example as an excuse to guide us to explore some of the basic mathematical ideas involved in prediction with neural networks."}
{"id": "http://jalammar.github.io/#relu-visualization_p45", "contents": "Discussions:\nHacker News (63 points, 8 comments), Reddit r/programming (312 points, 37 comments)\n\nTranslations: Spanish\n"}
{"id": "http://jalammar.github.io/#relu-visualization_p46", "contents": "Update: Part 2 is now live: A Visual And Interactive Look at Basic Neural Network Math"}
{"id": "http://jalammar.github.io/#relu-visualization_p47", "contents": "I\u2019m not a machine learning expert. I\u2019m a software engineer by training and I\u2019ve had little interaction with AI. I had always wanted to delve deeper into machine learning, but never really found my \u201cin\u201d. That\u2019s why when Google open sourced TensorFlow in November 2015, I got super excited and knew it was time to jump in and start the learning journey. Not to sound dramatic, but to me, it actually felt kind of like Prometheus handing down fire to mankind from the Mount Olympus of machine learning. In the back of my head was the idea that the entire field of Big Data and technologies like Hadoop were vastly accelerated when Google researchers released their Map Reduce paper. This time it\u2019s not a paper \u2013 it\u2019s the actual software they use internally after years and years of evolution."}
{"id": "http://jalammar.github.io/#relu-visualization_p48", "contents": "So I started learning what I can about the basics of the topic, and saw the need for gentler resources for people with no experience in the field. This is my attempt at that."}
{"id": "http://jalammar.github.io/#relu-visualization_p49", "contents": "Discussion:\nReddit r/Android (80 points, 16 comments)\n"}
{"id": "http://jalammar.github.io/#relu-visualization_p51", "contents": "In November 2015, Google announced and open sourced TensorFlow, its latest and greatest machine learning library. This is a big deal for three reasons:"}
{"id": "http://jalammar.github.io/#relu-visualization_p52", "contents": "This last reason is the operating reason for this post since we\u2019ll be focusing on Android. If you examine the tensorflow repo on GitHub, you\u2019ll find a little  tensorflow/examples/android directory. I\u2019ll try to shed some light on the Android TensorFlow example and some of the things going on under the hood."}
{"id": "http://jalammar.github.io/#relu_p0", "contents": "Visualizing machine learning one concept at a time.@JayAlammar on Twitter."}
{"id": "http://jalammar.github.io/#relu_p1", "contents": "Check out the first video in my new series introducing the general public to AI and machine learning."}
{"id": "http://jalammar.github.io/#relu_p2", "contents": "My aim for this series is to help people integrate ML into their world-view away from all the hype and overpromises that plauge the topic."}
{"id": "http://jalammar.github.io/#relu_p3", "contents": "I had an incredible time organizing and speaking at the AI/machine learning track at QCon London 2020 where I invited and shared the stage with incredible speakers Vincent Warmerdam, Susanne Groothuis, Peter Elger, and Hien Luu."}
{"id": "http://jalammar.github.io/#relu_p4", "contents": "QCon is a global software conference for software engineers, architects, and team leaders, with over 1,600 attendees in London. All speakers have a software background."}
{"id": "http://jalammar.github.io/#relu_p5", "contents": "Translations: Russian"}
{"id": "http://jalammar.github.io/#relu_p6", "contents": "Progress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents \u201cthe biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search\u201d."}
{"id": "http://jalammar.github.io/#relu_p7", "contents": "This post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved."}
{"id": "http://jalammar.github.io/#relu_p8", "contents": "Alongside this post, I\u2019ve prepared a notebook. You can see it here the notebook or run it on colab."}
{"id": "http://jalammar.github.io/#relu_p9", "contents": "I had a great time speaking at the MIT Analytics Lab about some of my favorite ideas in natural language processing and their practical applications."}
{"id": "http://jalammar.github.io/#relu_p10", "contents": "Discussions:\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\n"}
{"id": "http://jalammar.github.io/#relu_p11", "contents": "Translations: Russian"}
{"id": "http://jalammar.github.io/#relu_p12", "contents": "This year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn\u2019t a particularly novel architecture \u2013 it\u2019s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we\u2019ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we\u2019ll look at applications for the decoder-only transformer beyond language modeling."}
{"id": "http://jalammar.github.io/#relu_p13", "contents": "My goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they\u2019ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve."}
{"id": "http://jalammar.github.io/#relu_p14", "contents": "Discussions:\nHacker News (366 points, 21 comments), Reddit r/MachineLearning (256 points, 18 comments)\n\n\nTranslations: Chinese 1, Chinese 2, Japanese"}
{"id": "http://jalammar.github.io/#relu_p15", "contents": "The NumPy package is the workhorse of data analysis, machine learning, and scientific computing in the python ecosystem. It vastly simplifies manipulating and crunching vectors and matrices. Some of python\u2019s leading package rely on NumPy as a fundamental piece of their infrastructure (examples include scikit-learn, SciPy, pandas, and tensorflow). Beyond the ability to slice and dice numeric data, mastering numpy will give you an edge when dealing and debugging with advanced usecases in these libraries."}
{"id": "http://jalammar.github.io/#relu_p16", "contents": "In this post, we\u2019ll look at some of the main ways to use NumPy and how it can represent different types of data (tables, images, text\u2026etc) before we can serve them to machine learning models."}
{"id": "http://jalammar.github.io/#relu_p17", "contents": "I gave a talk at Qcon London this year. Watch it here:"}
{"id": "http://jalammar.github.io/#relu_p18", "contents": "Intuition & Use-Cases of Embeddings in NLP & beyond [YouTube]"}
{"id": "http://jalammar.github.io/#relu_p19", "contents": "https://www.infoq.com/presentations/nlp-word-embedding/ [infoQ]"}
{"id": "http://jalammar.github.io/#relu_p20", "contents": "In this video, I introduced word embeddings and the word2vec algorithm.  I then proceeded to discuss how the word2vec algorithm is used to create recommendation engines in companies like Airbnb and Alibaba. I close by glancing at real-world consequences of popular recommendation systems like those of YouTube and Facebook."}
{"id": "http://jalammar.github.io/#relu_p21", "contents": "My Illustrated Word2vec post used and built on the materials I created for this talk (but didn\u2019t include anything on the recommender application of word2vec). This was my first talk at a technical conference and I spent quite a bit of time preparing for it. In the six weeks prior to the conference I spent about 100 hours working on the presentation and ended up with 200 slides. It was an interesting balancing act of trying to make it introductory but not shallow, suitable for senior engineers and architects yet not necessarily ones who have machine learning experience."}
{"id": "http://jalammar.github.io/#relu_p22", "contents": " Discussions:\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\n\n\n\nTranslations: Chinese (Simplified), Korean, Portuguese, Russian\n"}
{"id": "http://jalammar.github.io/#relu_p23", "contents": "I find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you\u2019ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you\u2019ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2)."}
{"id": "http://jalammar.github.io/#relu_p24", "contents": "Word2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines."}
{"id": "http://jalammar.github.io/#relu_p25", "contents": "In this post, we\u2019ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let\u2019s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?"}
{"id": "http://jalammar.github.io/#relu_p26", "contents": "Discussions:\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\n\n\nTranslations: Chinese (Simplified), Japanese, Korean, Persian, Russian"}
{"id": "http://jalammar.github.io/#relu_p27", "contents": "The year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It\u2019s been referred to as NLP\u2019s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks)."}
{"id": "http://jalammar.github.io/#relu_p28", "contents": "Discussions:\nHacker News (195 points, 51 comments), Reddit r/Python (140 points, 18 comments)\n"}
{"id": "http://jalammar.github.io/#relu_p29", "contents": "If you\u2019re planning to learn data analysis, machine learning, or data science tools in python, you\u2019re most likely going to be using the wonderful pandas library. Pandas is an open source library for data manipulation and analysis in python."}
{"id": "http://jalammar.github.io/#relu_p30", "contents": "One of the easiest ways to think about that, is that you can load tables (and excel files) and then slice and dice them in multiple ways:"}
{"id": "http://jalammar.github.io/#relu_p32", "contents": "Discussions:\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\n\n\nTranslations: Chinese (Simplified), Japanese, Korean, Russian\n\nWatch: MIT\u2019s Deep Learning State of the Art lecture referencing this post"}
{"id": "http://jalammar.github.io/#relu_p33", "contents": "In the previous post, we looked at Attention \u2013 a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer \u2013 a model that uses attention to boost the speed with which these models can be trained. The Transformers outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud\u2019s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let\u2019s try to break the model apart and look at how it functions."}
{"id": "http://jalammar.github.io/#relu_p34", "contents": "The Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard\u2019s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter."}
{"id": "http://jalammar.github.io/#relu_p35", "contents": "Let\u2019s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another."}
{"id": "http://jalammar.github.io/#relu_p36", "contents": "Translations: Chinese (Simplified), Japanese, Korean, Russian\n\nWatch: MIT\u2019s Deep Learning State of the Art lecture referencing this post"}
{"id": "http://jalammar.github.io/#relu_p37", "contents": "May 25th update: New graphics (RNN animation, word embedding graph), color coding, elaborated on the final attention example."}
{"id": "http://jalammar.github.io/#relu_p38", "contents": "Note: The animations below are videos. Touch or hover on them (if you\u2019re using a mouse) to get play controls so you can pause if needed."}
{"id": "http://jalammar.github.io/#relu_p39", "contents": "Sequence-to-sequence models are deep learning models that have achieved a lot of success in tasks like machine translation, text summarization, and image captioning. Google Translate started using such a model in production in late 2016. These models are explained in the two pioneering papers (Sutskever et al., 2014, Cho et al., 2014)."}
{"id": "http://jalammar.github.io/#relu_p40", "contents": "I found, however, that understanding the model well enough to implement it requires unraveling a series of concepts that build on top of each other. I thought that a bunch of these ideas would be more accessible if expressed visually. That\u2019s what I aim to do in this post. You\u2019ll need some previous understanding of deep learning to get through this post. I hope it can be a useful companion to reading the papers mentioned above (and the attention papers linked later in the post)."}
{"id": "http://jalammar.github.io/#relu_p41", "contents": "A sequence-to-sequence model is a model that takes a sequence of items (words, letters, features of an images\u2026etc) and outputs another sequence of items. A trained model would work like this:"}
{"id": "http://jalammar.github.io/#relu_p42", "contents": "I love using python\u2019s Pandas package for data analysis. The 10 Minutes to pandas is a great place to start learning how to use it for data analysis."}
{"id": "http://jalammar.github.io/#relu_p43", "contents": "Things get a lot more interesting once you\u2019re comfortable with the fundamentals and start with Reshaping and Pivot Tables. That guide shows some of the more interesting functions of reshaping data. Below are some visualizations to go along with the Pandas reshaping guide."}
{"id": "http://jalammar.github.io/#relu_p44", "contents": "In the previous post, we looked at the basic concepts of neural networks. Let us now take another example as an excuse to guide us to explore some of the basic mathematical ideas involved in prediction with neural networks."}
{"id": "http://jalammar.github.io/#relu_p45", "contents": "Discussions:\nHacker News (63 points, 8 comments), Reddit r/programming (312 points, 37 comments)\n\nTranslations: Spanish\n"}
{"id": "http://jalammar.github.io/#relu_p46", "contents": "Update: Part 2 is now live: A Visual And Interactive Look at Basic Neural Network Math"}
{"id": "http://jalammar.github.io/#relu_p47", "contents": "I\u2019m not a machine learning expert. I\u2019m a software engineer by training and I\u2019ve had little interaction with AI. I had always wanted to delve deeper into machine learning, but never really found my \u201cin\u201d. That\u2019s why when Google open sourced TensorFlow in November 2015, I got super excited and knew it was time to jump in and start the learning journey. Not to sound dramatic, but to me, it actually felt kind of like Prometheus handing down fire to mankind from the Mount Olympus of machine learning. In the back of my head was the idea that the entire field of Big Data and technologies like Hadoop were vastly accelerated when Google researchers released their Map Reduce paper. This time it\u2019s not a paper \u2013 it\u2019s the actual software they use internally after years and years of evolution."}
{"id": "http://jalammar.github.io/#relu_p48", "contents": "So I started learning what I can about the basics of the topic, and saw the need for gentler resources for people with no experience in the field. This is my attempt at that."}
{"id": "http://jalammar.github.io/#relu_p49", "contents": "Discussion:\nReddit r/Android (80 points, 16 comments)\n"}
{"id": "http://jalammar.github.io/#relu_p51", "contents": "In November 2015, Google announced and open sourced TensorFlow, its latest and greatest machine learning library. This is a big deal for three reasons:"}
{"id": "http://jalammar.github.io/#relu_p52", "contents": "This last reason is the operating reason for this post since we\u2019ll be focusing on Android. If you examine the tensorflow repo on GitHub, you\u2019ll find a little  tensorflow/examples/android directory. I\u2019ll try to shed some light on the Android TensorFlow example and some of the things going on under the hood."}
{"id": "http://jalammar.github.io/#prediction-calculation_p0", "contents": "Visualizing machine learning one concept at a time.@JayAlammar on Twitter."}
{"id": "http://jalammar.github.io/#prediction-calculation_p1", "contents": "Check out the first video in my new series introducing the general public to AI and machine learning."}
{"id": "http://jalammar.github.io/#prediction-calculation_p2", "contents": "My aim for this series is to help people integrate ML into their world-view away from all the hype and overpromises that plauge the topic."}
{"id": "http://jalammar.github.io/#prediction-calculation_p3", "contents": "I had an incredible time organizing and speaking at the AI/machine learning track at QCon London 2020 where I invited and shared the stage with incredible speakers Vincent Warmerdam, Susanne Groothuis, Peter Elger, and Hien Luu."}
{"id": "http://jalammar.github.io/#prediction-calculation_p4", "contents": "QCon is a global software conference for software engineers, architects, and team leaders, with over 1,600 attendees in London. All speakers have a software background."}
{"id": "http://jalammar.github.io/#prediction-calculation_p5", "contents": "Translations: Russian"}
{"id": "http://jalammar.github.io/#prediction-calculation_p6", "contents": "Progress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents \u201cthe biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search\u201d."}
{"id": "http://jalammar.github.io/#prediction-calculation_p7", "contents": "This post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved."}
{"id": "http://jalammar.github.io/#prediction-calculation_p8", "contents": "Alongside this post, I\u2019ve prepared a notebook. You can see it here the notebook or run it on colab."}
{"id": "http://jalammar.github.io/#prediction-calculation_p9", "contents": "I had a great time speaking at the MIT Analytics Lab about some of my favorite ideas in natural language processing and their practical applications."}
{"id": "http://jalammar.github.io/#prediction-calculation_p10", "contents": "Discussions:\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\n"}
{"id": "http://jalammar.github.io/#prediction-calculation_p11", "contents": "Translations: Russian"}
{"id": "http://jalammar.github.io/#prediction-calculation_p12", "contents": "This year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn\u2019t a particularly novel architecture \u2013 it\u2019s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we\u2019ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we\u2019ll look at applications for the decoder-only transformer beyond language modeling."}
{"id": "http://jalammar.github.io/#prediction-calculation_p13", "contents": "My goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they\u2019ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve."}
{"id": "http://jalammar.github.io/#prediction-calculation_p14", "contents": "Discussions:\nHacker News (366 points, 21 comments), Reddit r/MachineLearning (256 points, 18 comments)\n\n\nTranslations: Chinese 1, Chinese 2, Japanese"}
{"id": "http://jalammar.github.io/#prediction-calculation_p15", "contents": "The NumPy package is the workhorse of data analysis, machine learning, and scientific computing in the python ecosystem. It vastly simplifies manipulating and crunching vectors and matrices. Some of python\u2019s leading package rely on NumPy as a fundamental piece of their infrastructure (examples include scikit-learn, SciPy, pandas, and tensorflow). Beyond the ability to slice and dice numeric data, mastering numpy will give you an edge when dealing and debugging with advanced usecases in these libraries."}
{"id": "http://jalammar.github.io/#prediction-calculation_p16", "contents": "In this post, we\u2019ll look at some of the main ways to use NumPy and how it can represent different types of data (tables, images, text\u2026etc) before we can serve them to machine learning models."}
{"id": "http://jalammar.github.io/#prediction-calculation_p17", "contents": "I gave a talk at Qcon London this year. Watch it here:"}
{"id": "http://jalammar.github.io/#prediction-calculation_p18", "contents": "Intuition & Use-Cases of Embeddings in NLP & beyond [YouTube]"}
{"id": "http://jalammar.github.io/#prediction-calculation_p19", "contents": "https://www.infoq.com/presentations/nlp-word-embedding/ [infoQ]"}
{"id": "http://jalammar.github.io/#prediction-calculation_p20", "contents": "In this video, I introduced word embeddings and the word2vec algorithm.  I then proceeded to discuss how the word2vec algorithm is used to create recommendation engines in companies like Airbnb and Alibaba. I close by glancing at real-world consequences of popular recommendation systems like those of YouTube and Facebook."}
{"id": "http://jalammar.github.io/#prediction-calculation_p21", "contents": "My Illustrated Word2vec post used and built on the materials I created for this talk (but didn\u2019t include anything on the recommender application of word2vec). This was my first talk at a technical conference and I spent quite a bit of time preparing for it. In the six weeks prior to the conference I spent about 100 hours working on the presentation and ended up with 200 slides. It was an interesting balancing act of trying to make it introductory but not shallow, suitable for senior engineers and architects yet not necessarily ones who have machine learning experience."}
{"id": "http://jalammar.github.io/#prediction-calculation_p22", "contents": " Discussions:\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\n\n\n\nTranslations: Chinese (Simplified), Korean, Portuguese, Russian\n"}
{"id": "http://jalammar.github.io/#prediction-calculation_p23", "contents": "I find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you\u2019ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you\u2019ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2)."}
{"id": "http://jalammar.github.io/#prediction-calculation_p24", "contents": "Word2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines."}
{"id": "http://jalammar.github.io/#prediction-calculation_p25", "contents": "In this post, we\u2019ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let\u2019s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?"}
{"id": "http://jalammar.github.io/#prediction-calculation_p26", "contents": "Discussions:\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\n\n\nTranslations: Chinese (Simplified), Japanese, Korean, Persian, Russian"}
{"id": "http://jalammar.github.io/#prediction-calculation_p27", "contents": "The year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It\u2019s been referred to as NLP\u2019s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks)."}
{"id": "http://jalammar.github.io/#prediction-calculation_p28", "contents": "Discussions:\nHacker News (195 points, 51 comments), Reddit r/Python (140 points, 18 comments)\n"}
{"id": "http://jalammar.github.io/#prediction-calculation_p29", "contents": "If you\u2019re planning to learn data analysis, machine learning, or data science tools in python, you\u2019re most likely going to be using the wonderful pandas library. Pandas is an open source library for data manipulation and analysis in python."}
{"id": "http://jalammar.github.io/#prediction-calculation_p30", "contents": "One of the easiest ways to think about that, is that you can load tables (and excel files) and then slice and dice them in multiple ways:"}
{"id": "http://jalammar.github.io/#prediction-calculation_p32", "contents": "Discussions:\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\n\n\nTranslations: Chinese (Simplified), Japanese, Korean, Russian\n\nWatch: MIT\u2019s Deep Learning State of the Art lecture referencing this post"}
{"id": "http://jalammar.github.io/#prediction-calculation_p33", "contents": "In the previous post, we looked at Attention \u2013 a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer \u2013 a model that uses attention to boost the speed with which these models can be trained. The Transformers outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud\u2019s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let\u2019s try to break the model apart and look at how it functions."}
{"id": "http://jalammar.github.io/#prediction-calculation_p34", "contents": "The Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard\u2019s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter."}
{"id": "http://jalammar.github.io/#prediction-calculation_p35", "contents": "Let\u2019s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another."}
{"id": "http://jalammar.github.io/#prediction-calculation_p36", "contents": "Translations: Chinese (Simplified), Japanese, Korean, Russian\n\nWatch: MIT\u2019s Deep Learning State of the Art lecture referencing this post"}
{"id": "http://jalammar.github.io/#prediction-calculation_p37", "contents": "May 25th update: New graphics (RNN animation, word embedding graph), color coding, elaborated on the final attention example."}
{"id": "http://jalammar.github.io/#prediction-calculation_p38", "contents": "Note: The animations below are videos. Touch or hover on them (if you\u2019re using a mouse) to get play controls so you can pause if needed."}
{"id": "http://jalammar.github.io/#prediction-calculation_p39", "contents": "Sequence-to-sequence models are deep learning models that have achieved a lot of success in tasks like machine translation, text summarization, and image captioning. Google Translate started using such a model in production in late 2016. These models are explained in the two pioneering papers (Sutskever et al., 2014, Cho et al., 2014)."}
{"id": "http://jalammar.github.io/#prediction-calculation_p40", "contents": "I found, however, that understanding the model well enough to implement it requires unraveling a series of concepts that build on top of each other. I thought that a bunch of these ideas would be more accessible if expressed visually. That\u2019s what I aim to do in this post. You\u2019ll need some previous understanding of deep learning to get through this post. I hope it can be a useful companion to reading the papers mentioned above (and the attention papers linked later in the post)."}
{"id": "http://jalammar.github.io/#prediction-calculation_p41", "contents": "A sequence-to-sequence model is a model that takes a sequence of items (words, letters, features of an images\u2026etc) and outputs another sequence of items. A trained model would work like this:"}
{"id": "http://jalammar.github.io/#prediction-calculation_p42", "contents": "I love using python\u2019s Pandas package for data analysis. The 10 Minutes to pandas is a great place to start learning how to use it for data analysis."}
{"id": "http://jalammar.github.io/#prediction-calculation_p43", "contents": "Things get a lot more interesting once you\u2019re comfortable with the fundamentals and start with Reshaping and Pivot Tables. That guide shows some of the more interesting functions of reshaping data. Below are some visualizations to go along with the Pandas reshaping guide."}
{"id": "http://jalammar.github.io/#prediction-calculation_p44", "contents": "In the previous post, we looked at the basic concepts of neural networks. Let us now take another example as an excuse to guide us to explore some of the basic mathematical ideas involved in prediction with neural networks."}
{"id": "http://jalammar.github.io/#prediction-calculation_p45", "contents": "Discussions:\nHacker News (63 points, 8 comments), Reddit r/programming (312 points, 37 comments)\n\nTranslations: Spanish\n"}
{"id": "http://jalammar.github.io/#prediction-calculation_p46", "contents": "Update: Part 2 is now live: A Visual And Interactive Look at Basic Neural Network Math"}
{"id": "http://jalammar.github.io/#prediction-calculation_p47", "contents": "I\u2019m not a machine learning expert. I\u2019m a software engineer by training and I\u2019ve had little interaction with AI. I had always wanted to delve deeper into machine learning, but never really found my \u201cin\u201d. That\u2019s why when Google open sourced TensorFlow in November 2015, I got super excited and knew it was time to jump in and start the learning journey. Not to sound dramatic, but to me, it actually felt kind of like Prometheus handing down fire to mankind from the Mount Olympus of machine learning. In the back of my head was the idea that the entire field of Big Data and technologies like Hadoop were vastly accelerated when Google researchers released their Map Reduce paper. This time it\u2019s not a paper \u2013 it\u2019s the actual software they use internally after years and years of evolution."}
{"id": "http://jalammar.github.io/#prediction-calculation_p48", "contents": "So I started learning what I can about the basics of the topic, and saw the need for gentler resources for people with no experience in the field. This is my attempt at that."}
{"id": "http://jalammar.github.io/#prediction-calculation_p49", "contents": "Discussion:\nReddit r/Android (80 points, 16 comments)\n"}
{"id": "http://jalammar.github.io/#prediction-calculation_p51", "contents": "In November 2015, Google announced and open sourced TensorFlow, its latest and greatest machine learning library. This is a big deal for three reasons:"}
{"id": "http://jalammar.github.io/#prediction-calculation_p52", "contents": "This last reason is the operating reason for this post since we\u2019ll be focusing on Android. If you examine the tensorflow repo on GitHub, you\u2019ll find a little  tensorflow/examples/android directory. I\u2019ll try to shed some light on the Android TensorFlow example and some of the things going on under the hood."}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p0", "contents": "Visualizing machine learning one concept at a time.@JayAlammar on Twitter."}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p1", "contents": "Check out the first video in my new series introducing the general public to AI and machine learning."}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p2", "contents": "My aim for this series is to help people integrate ML into their world-view away from all the hype and overpromises that plauge the topic."}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p3", "contents": "I had an incredible time organizing and speaking at the AI/machine learning track at QCon London 2020 where I invited and shared the stage with incredible speakers Vincent Warmerdam, Susanne Groothuis, Peter Elger, and Hien Luu."}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p4", "contents": "QCon is a global software conference for software engineers, architects, and team leaders, with over 1,600 attendees in London. All speakers have a software background."}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p5", "contents": "Translations: Russian"}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p6", "contents": "Progress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents \u201cthe biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search\u201d."}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p7", "contents": "This post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved."}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p8", "contents": "Alongside this post, I\u2019ve prepared a notebook. You can see it here the notebook or run it on colab."}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p9", "contents": "I had a great time speaking at the MIT Analytics Lab about some of my favorite ideas in natural language processing and their practical applications."}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p10", "contents": "Discussions:\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\n"}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p11", "contents": "Translations: Russian"}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p12", "contents": "This year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn\u2019t a particularly novel architecture \u2013 it\u2019s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we\u2019ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we\u2019ll look at applications for the decoder-only transformer beyond language modeling."}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p13", "contents": "My goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they\u2019ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve."}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p14", "contents": "Discussions:\nHacker News (366 points, 21 comments), Reddit r/MachineLearning (256 points, 18 comments)\n\n\nTranslations: Chinese 1, Chinese 2, Japanese"}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p15", "contents": "The NumPy package is the workhorse of data analysis, machine learning, and scientific computing in the python ecosystem. It vastly simplifies manipulating and crunching vectors and matrices. Some of python\u2019s leading package rely on NumPy as a fundamental piece of their infrastructure (examples include scikit-learn, SciPy, pandas, and tensorflow). Beyond the ability to slice and dice numeric data, mastering numpy will give you an edge when dealing and debugging with advanced usecases in these libraries."}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p16", "contents": "In this post, we\u2019ll look at some of the main ways to use NumPy and how it can represent different types of data (tables, images, text\u2026etc) before we can serve them to machine learning models."}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p17", "contents": "I gave a talk at Qcon London this year. Watch it here:"}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p18", "contents": "Intuition & Use-Cases of Embeddings in NLP & beyond [YouTube]"}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p19", "contents": "https://www.infoq.com/presentations/nlp-word-embedding/ [infoQ]"}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p20", "contents": "In this video, I introduced word embeddings and the word2vec algorithm.  I then proceeded to discuss how the word2vec algorithm is used to create recommendation engines in companies like Airbnb and Alibaba. I close by glancing at real-world consequences of popular recommendation systems like those of YouTube and Facebook."}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p21", "contents": "My Illustrated Word2vec post used and built on the materials I created for this talk (but didn\u2019t include anything on the recommender application of word2vec). This was my first talk at a technical conference and I spent quite a bit of time preparing for it. In the six weeks prior to the conference I spent about 100 hours working on the presentation and ended up with 200 slides. It was an interesting balancing act of trying to make it introductory but not shallow, suitable for senior engineers and architects yet not necessarily ones who have machine learning experience."}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p22", "contents": " Discussions:\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\n\n\n\nTranslations: Chinese (Simplified), Korean, Portuguese, Russian\n"}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p23", "contents": "I find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you\u2019ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you\u2019ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2)."}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p24", "contents": "Word2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines."}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p25", "contents": "In this post, we\u2019ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let\u2019s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?"}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p26", "contents": "Discussions:\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\n\n\nTranslations: Chinese (Simplified), Japanese, Korean, Persian, Russian"}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p27", "contents": "The year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It\u2019s been referred to as NLP\u2019s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks)."}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p28", "contents": "Discussions:\nHacker News (195 points, 51 comments), Reddit r/Python (140 points, 18 comments)\n"}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p29", "contents": "If you\u2019re planning to learn data analysis, machine learning, or data science tools in python, you\u2019re most likely going to be using the wonderful pandas library. Pandas is an open source library for data manipulation and analysis in python."}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p30", "contents": "One of the easiest ways to think about that, is that you can load tables (and excel files) and then slice and dice them in multiple ways:"}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p32", "contents": "Discussions:\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\n\n\nTranslations: Chinese (Simplified), Japanese, Korean, Russian\n\nWatch: MIT\u2019s Deep Learning State of the Art lecture referencing this post"}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p33", "contents": "In the previous post, we looked at Attention \u2013 a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer \u2013 a model that uses attention to boost the speed with which these models can be trained. The Transformers outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud\u2019s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let\u2019s try to break the model apart and look at how it functions."}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p34", "contents": "The Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard\u2019s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter."}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p35", "contents": "Let\u2019s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another."}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p36", "contents": "Translations: Chinese (Simplified), Japanese, Korean, Russian\n\nWatch: MIT\u2019s Deep Learning State of the Art lecture referencing this post"}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p37", "contents": "May 25th update: New graphics (RNN animation, word embedding graph), color coding, elaborated on the final attention example."}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p38", "contents": "Note: The animations below are videos. Touch or hover on them (if you\u2019re using a mouse) to get play controls so you can pause if needed."}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p39", "contents": "Sequence-to-sequence models are deep learning models that have achieved a lot of success in tasks like machine translation, text summarization, and image captioning. Google Translate started using such a model in production in late 2016. These models are explained in the two pioneering papers (Sutskever et al., 2014, Cho et al., 2014)."}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p40", "contents": "I found, however, that understanding the model well enough to implement it requires unraveling a series of concepts that build on top of each other. I thought that a bunch of these ideas would be more accessible if expressed visually. That\u2019s what I aim to do in this post. You\u2019ll need some previous understanding of deep learning to get through this post. I hope it can be a useful companion to reading the papers mentioned above (and the attention papers linked later in the post)."}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p41", "contents": "A sequence-to-sequence model is a model that takes a sequence of items (words, letters, features of an images\u2026etc) and outputs another sequence of items. A trained model would work like this:"}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p42", "contents": "I love using python\u2019s Pandas package for data analysis. The 10 Minutes to pandas is a great place to start learning how to use it for data analysis."}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p43", "contents": "Things get a lot more interesting once you\u2019re comfortable with the fundamentals and start with Reshaping and Pivot Tables. That guide shows some of the more interesting functions of reshaping data. Below are some visualizations to go along with the Pandas reshaping guide."}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p44", "contents": "In the previous post, we looked at the basic concepts of neural networks. Let us now take another example as an excuse to guide us to explore some of the basic mathematical ideas involved in prediction with neural networks."}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p45", "contents": "Discussions:\nHacker News (63 points, 8 comments), Reddit r/programming (312 points, 37 comments)\n\nTranslations: Spanish\n"}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p46", "contents": "Update: Part 2 is now live: A Visual And Interactive Look at Basic Neural Network Math"}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p47", "contents": "I\u2019m not a machine learning expert. I\u2019m a software engineer by training and I\u2019ve had little interaction with AI. I had always wanted to delve deeper into machine learning, but never really found my \u201cin\u201d. That\u2019s why when Google open sourced TensorFlow in November 2015, I got super excited and knew it was time to jump in and start the learning journey. Not to sound dramatic, but to me, it actually felt kind of like Prometheus handing down fire to mankind from the Mount Olympus of machine learning. In the back of my head was the idea that the entire field of Big Data and technologies like Hadoop were vastly accelerated when Google researchers released their Map Reduce paper. This time it\u2019s not a paper \u2013 it\u2019s the actual software they use internally after years and years of evolution."}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p48", "contents": "So I started learning what I can about the basics of the topic, and saw the need for gentler resources for people with no experience in the field. This is my attempt at that."}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p49", "contents": "Discussion:\nReddit r/Android (80 points, 16 comments)\n"}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p51", "contents": "In November 2015, Google announced and open sourced TensorFlow, its latest and greatest machine learning library. This is a big deal for three reasons:"}
{"id": "http://jalammar.github.io/#sigmoid-visualization_p52", "contents": "This last reason is the operating reason for this post since we\u2019ll be focusing on Android. If you examine the tensorflow repo on GitHub, you\u2019ll find a little  tensorflow/examples/android directory. I\u2019ll try to shed some light on the Android TensorFlow example and some of the things going on under the hood."}
{"id": "http://jalammar.github.io/#sigmoid_p0", "contents": "Visualizing machine learning one concept at a time.@JayAlammar on Twitter."}
{"id": "http://jalammar.github.io/#sigmoid_p1", "contents": "Check out the first video in my new series introducing the general public to AI and machine learning."}
{"id": "http://jalammar.github.io/#sigmoid_p2", "contents": "My aim for this series is to help people integrate ML into their world-view away from all the hype and overpromises that plauge the topic."}
{"id": "http://jalammar.github.io/#sigmoid_p3", "contents": "I had an incredible time organizing and speaking at the AI/machine learning track at QCon London 2020 where I invited and shared the stage with incredible speakers Vincent Warmerdam, Susanne Groothuis, Peter Elger, and Hien Luu."}
{"id": "http://jalammar.github.io/#sigmoid_p4", "contents": "QCon is a global software conference for software engineers, architects, and team leaders, with over 1,600 attendees in London. All speakers have a software background."}
{"id": "http://jalammar.github.io/#sigmoid_p5", "contents": "Translations: Russian"}
{"id": "http://jalammar.github.io/#sigmoid_p6", "contents": "Progress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents \u201cthe biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search\u201d."}
{"id": "http://jalammar.github.io/#sigmoid_p7", "contents": "This post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved."}
{"id": "http://jalammar.github.io/#sigmoid_p8", "contents": "Alongside this post, I\u2019ve prepared a notebook. You can see it here the notebook or run it on colab."}
{"id": "http://jalammar.github.io/#sigmoid_p9", "contents": "I had a great time speaking at the MIT Analytics Lab about some of my favorite ideas in natural language processing and their practical applications."}
{"id": "http://jalammar.github.io/#sigmoid_p10", "contents": "Discussions:\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\n"}
{"id": "http://jalammar.github.io/#sigmoid_p11", "contents": "Translations: Russian"}
{"id": "http://jalammar.github.io/#sigmoid_p12", "contents": "This year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn\u2019t a particularly novel architecture \u2013 it\u2019s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we\u2019ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we\u2019ll look at applications for the decoder-only transformer beyond language modeling."}
{"id": "http://jalammar.github.io/#sigmoid_p13", "contents": "My goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they\u2019ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve."}
{"id": "http://jalammar.github.io/#sigmoid_p14", "contents": "Discussions:\nHacker News (366 points, 21 comments), Reddit r/MachineLearning (256 points, 18 comments)\n\n\nTranslations: Chinese 1, Chinese 2, Japanese"}
{"id": "http://jalammar.github.io/#sigmoid_p15", "contents": "The NumPy package is the workhorse of data analysis, machine learning, and scientific computing in the python ecosystem. It vastly simplifies manipulating and crunching vectors and matrices. Some of python\u2019s leading package rely on NumPy as a fundamental piece of their infrastructure (examples include scikit-learn, SciPy, pandas, and tensorflow). Beyond the ability to slice and dice numeric data, mastering numpy will give you an edge when dealing and debugging with advanced usecases in these libraries."}
{"id": "http://jalammar.github.io/#sigmoid_p16", "contents": "In this post, we\u2019ll look at some of the main ways to use NumPy and how it can represent different types of data (tables, images, text\u2026etc) before we can serve them to machine learning models."}
{"id": "http://jalammar.github.io/#sigmoid_p17", "contents": "I gave a talk at Qcon London this year. Watch it here:"}
{"id": "http://jalammar.github.io/#sigmoid_p18", "contents": "Intuition & Use-Cases of Embeddings in NLP & beyond [YouTube]"}
{"id": "http://jalammar.github.io/#sigmoid_p19", "contents": "https://www.infoq.com/presentations/nlp-word-embedding/ [infoQ]"}
{"id": "http://jalammar.github.io/#sigmoid_p20", "contents": "In this video, I introduced word embeddings and the word2vec algorithm.  I then proceeded to discuss how the word2vec algorithm is used to create recommendation engines in companies like Airbnb and Alibaba. I close by glancing at real-world consequences of popular recommendation systems like those of YouTube and Facebook."}
{"id": "http://jalammar.github.io/#sigmoid_p21", "contents": "My Illustrated Word2vec post used and built on the materials I created for this talk (but didn\u2019t include anything on the recommender application of word2vec). This was my first talk at a technical conference and I spent quite a bit of time preparing for it. In the six weeks prior to the conference I spent about 100 hours working on the presentation and ended up with 200 slides. It was an interesting balancing act of trying to make it introductory but not shallow, suitable for senior engineers and architects yet not necessarily ones who have machine learning experience."}
{"id": "http://jalammar.github.io/#sigmoid_p22", "contents": " Discussions:\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\n\n\n\nTranslations: Chinese (Simplified), Korean, Portuguese, Russian\n"}
{"id": "http://jalammar.github.io/#sigmoid_p23", "contents": "I find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you\u2019ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you\u2019ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2)."}
{"id": "http://jalammar.github.io/#sigmoid_p24", "contents": "Word2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines."}
{"id": "http://jalammar.github.io/#sigmoid_p25", "contents": "In this post, we\u2019ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let\u2019s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?"}
{"id": "http://jalammar.github.io/#sigmoid_p26", "contents": "Discussions:\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\n\n\nTranslations: Chinese (Simplified), Japanese, Korean, Persian, Russian"}
{"id": "http://jalammar.github.io/#sigmoid_p27", "contents": "The year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It\u2019s been referred to as NLP\u2019s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks)."}
{"id": "http://jalammar.github.io/#sigmoid_p28", "contents": "Discussions:\nHacker News (195 points, 51 comments), Reddit r/Python (140 points, 18 comments)\n"}
{"id": "http://jalammar.github.io/#sigmoid_p29", "contents": "If you\u2019re planning to learn data analysis, machine learning, or data science tools in python, you\u2019re most likely going to be using the wonderful pandas library. Pandas is an open source library for data manipulation and analysis in python."}
{"id": "http://jalammar.github.io/#sigmoid_p30", "contents": "One of the easiest ways to think about that, is that you can load tables (and excel files) and then slice and dice them in multiple ways:"}
{"id": "http://jalammar.github.io/#sigmoid_p32", "contents": "Discussions:\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\n\n\nTranslations: Chinese (Simplified), Japanese, Korean, Russian\n\nWatch: MIT\u2019s Deep Learning State of the Art lecture referencing this post"}
{"id": "http://jalammar.github.io/#sigmoid_p33", "contents": "In the previous post, we looked at Attention \u2013 a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer \u2013 a model that uses attention to boost the speed with which these models can be trained. The Transformers outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud\u2019s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let\u2019s try to break the model apart and look at how it functions."}
{"id": "http://jalammar.github.io/#sigmoid_p34", "contents": "The Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard\u2019s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter."}
{"id": "http://jalammar.github.io/#sigmoid_p35", "contents": "Let\u2019s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another."}
{"id": "http://jalammar.github.io/#sigmoid_p36", "contents": "Translations: Chinese (Simplified), Japanese, Korean, Russian\n\nWatch: MIT\u2019s Deep Learning State of the Art lecture referencing this post"}
{"id": "http://jalammar.github.io/#sigmoid_p37", "contents": "May 25th update: New graphics (RNN animation, word embedding graph), color coding, elaborated on the final attention example."}
{"id": "http://jalammar.github.io/#sigmoid_p38", "contents": "Note: The animations below are videos. Touch or hover on them (if you\u2019re using a mouse) to get play controls so you can pause if needed."}
{"id": "http://jalammar.github.io/#sigmoid_p39", "contents": "Sequence-to-sequence models are deep learning models that have achieved a lot of success in tasks like machine translation, text summarization, and image captioning. Google Translate started using such a model in production in late 2016. These models are explained in the two pioneering papers (Sutskever et al., 2014, Cho et al., 2014)."}
{"id": "http://jalammar.github.io/#sigmoid_p40", "contents": "I found, however, that understanding the model well enough to implement it requires unraveling a series of concepts that build on top of each other. I thought that a bunch of these ideas would be more accessible if expressed visually. That\u2019s what I aim to do in this post. You\u2019ll need some previous understanding of deep learning to get through this post. I hope it can be a useful companion to reading the papers mentioned above (and the attention papers linked later in the post)."}
{"id": "http://jalammar.github.io/#sigmoid_p41", "contents": "A sequence-to-sequence model is a model that takes a sequence of items (words, letters, features of an images\u2026etc) and outputs another sequence of items. A trained model would work like this:"}
{"id": "http://jalammar.github.io/#sigmoid_p42", "contents": "I love using python\u2019s Pandas package for data analysis. The 10 Minutes to pandas is a great place to start learning how to use it for data analysis."}
{"id": "http://jalammar.github.io/#sigmoid_p43", "contents": "Things get a lot more interesting once you\u2019re comfortable with the fundamentals and start with Reshaping and Pivot Tables. That guide shows some of the more interesting functions of reshaping data. Below are some visualizations to go along with the Pandas reshaping guide."}
{"id": "http://jalammar.github.io/#sigmoid_p44", "contents": "In the previous post, we looked at the basic concepts of neural networks. Let us now take another example as an excuse to guide us to explore some of the basic mathematical ideas involved in prediction with neural networks."}
{"id": "http://jalammar.github.io/#sigmoid_p45", "contents": "Discussions:\nHacker News (63 points, 8 comments), Reddit r/programming (312 points, 37 comments)\n\nTranslations: Spanish\n"}
{"id": "http://jalammar.github.io/#sigmoid_p46", "contents": "Update: Part 2 is now live: A Visual And Interactive Look at Basic Neural Network Math"}
{"id": "http://jalammar.github.io/#sigmoid_p47", "contents": "I\u2019m not a machine learning expert. I\u2019m a software engineer by training and I\u2019ve had little interaction with AI. I had always wanted to delve deeper into machine learning, but never really found my \u201cin\u201d. That\u2019s why when Google open sourced TensorFlow in November 2015, I got super excited and knew it was time to jump in and start the learning journey. Not to sound dramatic, but to me, it actually felt kind of like Prometheus handing down fire to mankind from the Mount Olympus of machine learning. In the back of my head was the idea that the entire field of Big Data and technologies like Hadoop were vastly accelerated when Google researchers released their Map Reduce paper. This time it\u2019s not a paper \u2013 it\u2019s the actual software they use internally after years and years of evolution."}
{"id": "http://jalammar.github.io/#sigmoid_p48", "contents": "So I started learning what I can about the basics of the topic, and saw the need for gentler resources for people with no experience in the field. This is my attempt at that."}
{"id": "http://jalammar.github.io/#sigmoid_p49", "contents": "Discussion:\nReddit r/Android (80 points, 16 comments)\n"}
{"id": "http://jalammar.github.io/#sigmoid_p51", "contents": "In November 2015, Google announced and open sourced TensorFlow, its latest and greatest machine learning library. This is a big deal for three reasons:"}
{"id": "http://jalammar.github.io/#sigmoid_p52", "contents": "This last reason is the operating reason for this post since we\u2019ll be focusing on Android. If you examine the tensorflow repo on GitHub, you\u2019ll find a little  tensorflow/examples/android directory. I\u2019ll try to shed some light on the Android TensorFlow example and some of the things going on under the hood."}
{"id": "http://jalammar.github.io/_p0", "contents": "Visualizing machine learning one concept at a time.@JayAlammar on Twitter."}
{"id": "http://jalammar.github.io/_p1", "contents": "Check out the first video in my new series introducing the general public to AI and machine learning."}
{"id": "http://jalammar.github.io/_p2", "contents": "My aim for this series is to help people integrate ML into their world-view away from all the hype and overpromises that plauge the topic."}
{"id": "http://jalammar.github.io/_p3", "contents": "I had an incredible time organizing and speaking at the AI/machine learning track at QCon London 2020 where I invited and shared the stage with incredible speakers Vincent Warmerdam, Susanne Groothuis, Peter Elger, and Hien Luu."}
{"id": "http://jalammar.github.io/_p4", "contents": "QCon is a global software conference for software engineers, architects, and team leaders, with over 1,600 attendees in London. All speakers have a software background."}
{"id": "http://jalammar.github.io/_p5", "contents": "Translations: Russian"}
{"id": "http://jalammar.github.io/_p6", "contents": "Progress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents \u201cthe biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search\u201d."}
{"id": "http://jalammar.github.io/_p7", "contents": "This post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved."}
{"id": "http://jalammar.github.io/_p8", "contents": "Alongside this post, I\u2019ve prepared a notebook. You can see it here the notebook or run it on colab."}
{"id": "http://jalammar.github.io/_p9", "contents": "I had a great time speaking at the MIT Analytics Lab about some of my favorite ideas in natural language processing and their practical applications."}
{"id": "http://jalammar.github.io/_p10", "contents": "Discussions:\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\n"}
{"id": "http://jalammar.github.io/_p11", "contents": "Translations: Russian"}
{"id": "http://jalammar.github.io/_p12", "contents": "This year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn\u2019t a particularly novel architecture \u2013 it\u2019s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we\u2019ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we\u2019ll look at applications for the decoder-only transformer beyond language modeling."}
{"id": "http://jalammar.github.io/_p13", "contents": "My goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they\u2019ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve."}
{"id": "http://jalammar.github.io/_p14", "contents": "Discussions:\nHacker News (366 points, 21 comments), Reddit r/MachineLearning (256 points, 18 comments)\n\n\nTranslations: Chinese 1, Chinese 2, Japanese"}
{"id": "http://jalammar.github.io/_p15", "contents": "The NumPy package is the workhorse of data analysis, machine learning, and scientific computing in the python ecosystem. It vastly simplifies manipulating and crunching vectors and matrices. Some of python\u2019s leading package rely on NumPy as a fundamental piece of their infrastructure (examples include scikit-learn, SciPy, pandas, and tensorflow). Beyond the ability to slice and dice numeric data, mastering numpy will give you an edge when dealing and debugging with advanced usecases in these libraries."}
{"id": "http://jalammar.github.io/_p16", "contents": "In this post, we\u2019ll look at some of the main ways to use NumPy and how it can represent different types of data (tables, images, text\u2026etc) before we can serve them to machine learning models."}
{"id": "http://jalammar.github.io/_p17", "contents": "I gave a talk at Qcon London this year. Watch it here:"}
{"id": "http://jalammar.github.io/_p18", "contents": "Intuition & Use-Cases of Embeddings in NLP & beyond [YouTube]"}
{"id": "http://jalammar.github.io/_p19", "contents": "https://www.infoq.com/presentations/nlp-word-embedding/ [infoQ]"}
{"id": "http://jalammar.github.io/_p20", "contents": "In this video, I introduced word embeddings and the word2vec algorithm.  I then proceeded to discuss how the word2vec algorithm is used to create recommendation engines in companies like Airbnb and Alibaba. I close by glancing at real-world consequences of popular recommendation systems like those of YouTube and Facebook."}
{"id": "http://jalammar.github.io/_p21", "contents": "My Illustrated Word2vec post used and built on the materials I created for this talk (but didn\u2019t include anything on the recommender application of word2vec). This was my first talk at a technical conference and I spent quite a bit of time preparing for it. In the six weeks prior to the conference I spent about 100 hours working on the presentation and ended up with 200 slides. It was an interesting balancing act of trying to make it introductory but not shallow, suitable for senior engineers and architects yet not necessarily ones who have machine learning experience."}
{"id": "http://jalammar.github.io/_p22", "contents": " Discussions:\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\n\n\n\nTranslations: Chinese (Simplified), Korean, Portuguese, Russian\n"}
{"id": "http://jalammar.github.io/_p23", "contents": "I find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you\u2019ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you\u2019ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2)."}
{"id": "http://jalammar.github.io/_p24", "contents": "Word2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines."}
{"id": "http://jalammar.github.io/_p25", "contents": "In this post, we\u2019ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let\u2019s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?"}
{"id": "http://jalammar.github.io/_p26", "contents": "Discussions:\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\n\n\nTranslations: Chinese (Simplified), Japanese, Korean, Persian, Russian"}
{"id": "http://jalammar.github.io/_p27", "contents": "The year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It\u2019s been referred to as NLP\u2019s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks)."}
{"id": "http://jalammar.github.io/_p28", "contents": "Discussions:\nHacker News (195 points, 51 comments), Reddit r/Python (140 points, 18 comments)\n"}
{"id": "http://jalammar.github.io/_p29", "contents": "If you\u2019re planning to learn data analysis, machine learning, or data science tools in python, you\u2019re most likely going to be using the wonderful pandas library. Pandas is an open source library for data manipulation and analysis in python."}
{"id": "http://jalammar.github.io/_p30", "contents": "One of the easiest ways to think about that, is that you can load tables (and excel files) and then slice and dice them in multiple ways:"}
{"id": "http://jalammar.github.io/_p32", "contents": "Discussions:\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\n\n\nTranslations: Chinese (Simplified), Japanese, Korean, Russian\n\nWatch: MIT\u2019s Deep Learning State of the Art lecture referencing this post"}
{"id": "http://jalammar.github.io/_p33", "contents": "In the previous post, we looked at Attention \u2013 a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer \u2013 a model that uses attention to boost the speed with which these models can be trained. The Transformers outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud\u2019s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let\u2019s try to break the model apart and look at how it functions."}
{"id": "http://jalammar.github.io/_p34", "contents": "The Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard\u2019s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter."}
{"id": "http://jalammar.github.io/_p35", "contents": "Let\u2019s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another."}
{"id": "http://jalammar.github.io/_p36", "contents": "Translations: Chinese (Simplified), Japanese, Korean, Russian\n\nWatch: MIT\u2019s Deep Learning State of the Art lecture referencing this post"}
{"id": "http://jalammar.github.io/_p37", "contents": "May 25th update: New graphics (RNN animation, word embedding graph), color coding, elaborated on the final attention example."}
{"id": "http://jalammar.github.io/_p38", "contents": "Note: The animations below are videos. Touch or hover on them (if you\u2019re using a mouse) to get play controls so you can pause if needed."}
{"id": "http://jalammar.github.io/_p39", "contents": "Sequence-to-sequence models are deep learning models that have achieved a lot of success in tasks like machine translation, text summarization, and image captioning. Google Translate started using such a model in production in late 2016. These models are explained in the two pioneering papers (Sutskever et al., 2014, Cho et al., 2014)."}
{"id": "http://jalammar.github.io/_p40", "contents": "I found, however, that understanding the model well enough to implement it requires unraveling a series of concepts that build on top of each other. I thought that a bunch of these ideas would be more accessible if expressed visually. That\u2019s what I aim to do in this post. You\u2019ll need some previous understanding of deep learning to get through this post. I hope it can be a useful companion to reading the papers mentioned above (and the attention papers linked later in the post)."}
{"id": "http://jalammar.github.io/_p41", "contents": "A sequence-to-sequence model is a model that takes a sequence of items (words, letters, features of an images\u2026etc) and outputs another sequence of items. A trained model would work like this:"}
{"id": "http://jalammar.github.io/_p42", "contents": "I love using python\u2019s Pandas package for data analysis. The 10 Minutes to pandas is a great place to start learning how to use it for data analysis."}
{"id": "http://jalammar.github.io/_p43", "contents": "Things get a lot more interesting once you\u2019re comfortable with the fundamentals and start with Reshaping and Pivot Tables. That guide shows some of the more interesting functions of reshaping data. Below are some visualizations to go along with the Pandas reshaping guide."}
{"id": "http://jalammar.github.io/_p44", "contents": "In the previous post, we looked at the basic concepts of neural networks. Let us now take another example as an excuse to guide us to explore some of the basic mathematical ideas involved in prediction with neural networks."}
{"id": "http://jalammar.github.io/_p45", "contents": "Discussions:\nHacker News (63 points, 8 comments), Reddit r/programming (312 points, 37 comments)\n\nTranslations: Spanish\n"}
{"id": "http://jalammar.github.io/_p46", "contents": "Update: Part 2 is now live: A Visual And Interactive Look at Basic Neural Network Math"}
{"id": "http://jalammar.github.io/_p47", "contents": "I\u2019m not a machine learning expert. I\u2019m a software engineer by training and I\u2019ve had little interaction with AI. I had always wanted to delve deeper into machine learning, but never really found my \u201cin\u201d. That\u2019s why when Google open sourced TensorFlow in November 2015, I got super excited and knew it was time to jump in and start the learning journey. Not to sound dramatic, but to me, it actually felt kind of like Prometheus handing down fire to mankind from the Mount Olympus of machine learning. In the back of my head was the idea that the entire field of Big Data and technologies like Hadoop were vastly accelerated when Google researchers released their Map Reduce paper. This time it\u2019s not a paper \u2013 it\u2019s the actual software they use internally after years and years of evolution."}
{"id": "http://jalammar.github.io/_p48", "contents": "So I started learning what I can about the basics of the topic, and saw the need for gentler resources for people with no experience in the field. This is my attempt at that."}
{"id": "http://jalammar.github.io/_p49", "contents": "Discussion:\nReddit r/Android (80 points, 16 comments)\n"}
{"id": "http://jalammar.github.io/_p51", "contents": "In November 2015, Google announced and open sourced TensorFlow, its latest and greatest machine learning library. This is a big deal for three reasons:"}
{"id": "http://jalammar.github.io/_p52", "contents": "This last reason is the operating reason for this post since we\u2019ll be focusing on Android. If you examine the tensorflow repo on GitHub, you\u2019ll find a little  tensorflow/examples/android directory. I\u2019ll try to shed some light on the Android TensorFlow example and some of the things going on under the hood."}
{"id": "http://jalammar.github.io//feedforward-neural-networks-visual-interactive/_p0", "contents": "Visualizing machine learning one concept at a time.@JayAlammar on Twitter."}
{"id": "http://jalammar.github.io//feedforward-neural-networks-visual-interactive/_p1", "contents": "In the previous post, we looked at the basic concepts of neural networks. Let us now take another example as an excuse to guide us to explore some of the basic mathematical ideas involved in prediction with neural networks."}
{"id": "http://jalammar.github.io//feedforward-neural-networks-visual-interactive/_p2", "contents": "If you had been aboard the Titanic, would you have survived the sinking event? Let\u2019s build a model to predict one\u2019s odds of survival."}
{"id": "http://jalammar.github.io//feedforward-neural-networks-visual-interactive/_p3", "contents": "This will be a neural network model building on what we discussed in the previous post, but will have a higher prediction accuracy because it utilizes hidden layers and activation functions."}
{"id": "http://jalammar.github.io//feedforward-neural-networks-visual-interactive/_p4", "contents": "The dataset we\u2019ll use this time will be the Titanic passenger list from Kaggle. It lists the names and other information of the passengers and shows whether each passenger survived the sinking event or not.\n"}
{"id": "http://jalammar.github.io//feedforward-neural-networks-visual-interactive/_p5", "contents": "The raw dataset looks like this:"}
{"id": "http://jalammar.github.io//feedforward-neural-networks-visual-interactive/_p6", "contents": "We won\u2019t bother with most of the columns for now. We\u2019ll just use the sex and age columns as our features, and survival as our label that we\u2019ll try to predict."}
{"id": "http://jalammar.github.io//feedforward-neural-networks-visual-interactive/_p7", "contents": "We\u2019ll attempt to build a network that predicts whether a passenger survived or not."}
{"id": "http://jalammar.github.io//feedforward-neural-networks-visual-interactive/_p8", "contents": "Neural networks need their inputs to be numeric. So we had to change the sex column \u2013 male is now 0, female is 1. You\u2019ll notice the dataset already uses something similar for the survival column \u2013 survived is 1, did not survive is 0."}
{"id": "http://jalammar.github.io//feedforward-neural-networks-visual-interactive/_p9", "contents": "The simplest neural network we can use to train to make this prediction looks like this:"}
{"id": "http://jalammar.github.io//feedforward-neural-networks-visual-interactive/_p10", "contents": "Let\u2019s recap the elements that make up this network and how they work:"}
{"id": "http://jalammar.github.io//feedforward-neural-networks-visual-interactive/_p12", "contents": "An input neuron is where we plug in an input value (e.g. the age of a person). It\u2019s where the calculation starts. The outgoing connection and the rest of the graph tell us what other calculations we need to do to calculate a prediction."}
{"id": "http://jalammar.github.io//feedforward-neural-networks-visual-interactive/_p14", "contents": "If a connection has a weight, then the value is multiplied by that weight as it passes through it."}
{"id": "http://jalammar.github.io//feedforward-neural-networks-visual-interactive/_p15", "contents": "If a neuron has inputs, it sums their value and sends that sum along its outgoing connection(s)."}
{"id": "http://jalammar.github.io//feedforward-neural-networks-visual-interactive/_p17", "contents": "To turn the network\u2019s calculation into a probability value between 0 and 1, we have to pass the value from the output layer through a \u201csigmoid\u201d formula. Sigmoid squashes the output value of a neuron to between 0 and 1 according to a specific curve."}
{"id": "http://jalammar.github.io//feedforward-neural-networks-visual-interactive/_p19", "contents": "Where e is the mathematical constant approximately equal to 2.71828"}
{"id": "http://jalammar.github.io//feedforward-neural-networks-visual-interactive/_p20", "contents": "Interact a little with sigmoid to see how it transforms various values"}
{"id": "http://jalammar.github.io//feedforward-neural-networks-visual-interactive/_p21", "contents": "f(0) =\n11+e\u2212x\\frac{1}{1 + e^{-x}}\u200b1+ e \u200b \u2212(x) \u200b\u200b\u200b\u200b1\u200b\u200b\n= "}
{"id": "http://jalammar.github.io//feedforward-neural-networks-visual-interactive/_p23", "contents": "To bring it all together, calculating a prediction with this shallow network looks like this:"}
{"id": "http://jalammar.github.io//feedforward-neural-networks-visual-interactive/_p24", "contents": "Now that we know the structure of our network, we can train it using  [gradient descent] running on the first 600 rows of the 891-row dataset. I will not be addressing the training process in this post because that\u2019s a separate concern at the moment. For now, I just want you to be comfortable with how a trained network calculates a prediction. Once you get this intuition down, we\u2019ll proceed to training in a future post."}
{"id": "http://jalammar.github.io//feedforward-neural-networks-visual-interactive/_p25", "contents": "The training process gives us the following values (with an accuracy of 73.20%):"}
{"id": "http://jalammar.github.io//feedforward-neural-networks-visual-interactive/_p26", "contents": "Intuitively, the weights indicate how much their associated property contribute to the prediction \u2013 odds of survival improve the younger a person is (since a larger age multiplied by the negative weight value gives a bigger negative number). They improve more if the person is female."}
{"id": "http://jalammar.github.io//feedforward-neural-networks-visual-interactive/_p27", "contents": "The trained network now looks like this:\n(hover or click on values in the table to see their individual predictions)"}
{"id": "http://jalammar.github.io//feedforward-neural-networks-visual-interactive/_p28", "contents": "An accuracy of 73.20% isn\u2019t very impressive. This is a case that can benefit from adding a hidden layer. Hidden layers give the model more capacity to represent more sophisticated prediction functions that may do a better job (Deep Learning ch.5 page 113)."}
{"id": "http://jalammar.github.io//feedforward-neural-networks-visual-interactive/_p30", "contents": "It\u2019s often useful to apply certain math functions to the weighted outputs. These are called \u201cactivation functions\u201d because historically they translated the output of the neuron into either 1 (On/active) or 0 (Off)."}
{"id": "http://jalammar.github.io//feedforward-neural-networks-visual-interactive/_p31", "contents": "Activation functions are vital for hidden layers. Without them, deep networks would be no better than a shallow linear network. Read the \u201cCommonly used activation functions\u201d section from Neural Networks Part 1: Setting up the Architecture for a look at various activation functions."}
{"id": "http://jalammar.github.io//feedforward-neural-networks-visual-interactive/_p33", "contents": "A leading choice for activation function is called ReLU. It returns 0 if its input is negative, returns the number itself otherwise. Very simple!"}
{"id": "http://jalammar.github.io//feedforward-neural-networks-visual-interactive/_p34", "contents": "f(x) = max(0, x)"}
{"id": "http://jalammar.github.io//feedforward-neural-networks-visual-interactive/_p35", "contents": "Interact a little with relu to see how it transforms various values"}
{"id": "http://jalammar.github.io//feedforward-neural-networks-visual-interactive/_p36", "contents": "f(0) =\nmax( 0,        x)\n= "}
{"id": "http://jalammar.github.io//feedforward-neural-networks-visual-interactive/_p37", "contents": "This post has been parked for more than a year. I had attempted to visualize a deeper network after this point, but that never materialized. I hope you enjoyed it. Let me know on @JayAlammar on Twitter if you have any feedback."}
{"id": "http://jalammar.github.io/android.hardware.camera2_p0", "contents": "File not found"}
{"id": "http://jalammar.github.io/android.hardware.camera2_p1", "contents": "\n        The site configured at this address does not\n        contain the requested file.\n      "}
{"id": "http://jalammar.github.io/android.hardware.camera2_p2", "contents": "\n        If this is your site, make sure that the filename case matches the URL.\n        For root URLs (like http://example.com/) you must provide an\n        index.html file.\n      "}
{"id": "http://jalammar.github.io/android.hardware.camera2_p3", "contents": "\nRead the full documentation\n        for more information about using GitHub Pages.\n      "}
{"id": "http://jalammar.github.io//Supercharging-android-apps-using-tensorflow/_p0", "contents": "Visualizing machine learning one concept at a time.@JayAlammar on Twitter."}
{"id": "http://jalammar.github.io//Supercharging-android-apps-using-tensorflow/_p1", "contents": "Discussion:\nReddit r/Android (80 points, 16 comments)\n"}
{"id": "http://jalammar.github.io//Supercharging-android-apps-using-tensorflow/_p3", "contents": "In November 2015, Google announced and open sourced TensorFlow, its latest and greatest machine learning library. This is a big deal for three reasons:"}
{"id": "http://jalammar.github.io//Supercharging-android-apps-using-tensorflow/_p4", "contents": "This last reason is the operating reason for this post since we\u2019ll be focusing on Android. If you examine the tensorflow repo on GitHub, you\u2019ll find a little  tensorflow/examples/android directory. I\u2019ll try to shed some light on the Android TensorFlow example and some of the things going on under the hood."}
{"id": "http://jalammar.github.io//Supercharging-android-apps-using-tensorflow/_p5", "contents": "The app glances out through your camera and tries to identify the objects it sees. Sometimes it does a good job, other times it can\u2019t quite pin down the object, and at times it leads to thought provoking guesses! Overall, it feels pretty magical."}
{"id": "http://jalammar.github.io//Supercharging-android-apps-using-tensorflow/_p7", "contents": "The app accomplishes this feat using a bundled machine learning model running in TensorFlow on the device (no network calls to a backend service). The model is trained against millions of images so that it can look at the photos the camera feeds it and classify the object into its best guess (from the 1000 object classifications it knows). Along with its best guess, it shows a confidence score to indicate how sure it is about its guess."}
{"id": "http://jalammar.github.io//Supercharging-android-apps-using-tensorflow/_p8", "contents": "The Android example page gives you an idea on how to build the app, and ultimately culminates in producing this APK (I built and uploaded the APK to save you some time since the building process requires installing the Android NDK and Bazel, Google\u2019s build tool)."}
{"id": "http://jalammar.github.io//Supercharging-android-apps-using-tensorflow/_p9", "contents": "NOTE: Android 5.0 or later required since the example uses the Camera2 package introduced in Android 5.0."}
{"id": "http://jalammar.github.io//Supercharging-android-apps-using-tensorflow/_p10", "contents": "NOTE: if your device runs Android 6.0 or later, you have to install the app with the following command (It gives the app the appropriate permissions it needs to run):"}
{"id": "http://jalammar.github.io//Supercharging-android-apps-using-tensorflow/_p12", "contents": "The core TensorFlow engine is built with C++, but programmers can write their TensorFlow software in either C++ or Python. The Android TensorFlow example uses the C++ interface in the following manner:"}
{"id": "http://jalammar.github.io//Supercharging-android-apps-using-tensorflow/_p13", "contents": "The good thing is that most of this logic is in normal Android Java SDK territory \u2013 so this should be familiar to most Android devs. So where is the C++?"}
{"id": "http://jalammar.github.io//Supercharging-android-apps-using-tensorflow/_p15", "contents": "If you look closely at TensorflowClassifier, you may notice the following methods:"}
{"id": "http://jalammar.github.io//Supercharging-android-apps-using-tensorflow/_p16", "contents": "The native keywords in these method signatures indicate that these methods are implemented in native C++ code. Look for them under the \u201candroid/jni\u201d directory and true enough, you\u2019ll find tensorflow_jni.cc"}
{"id": "http://jalammar.github.io//Supercharging-android-apps-using-tensorflow/_p17", "contents": "JNI (short for Java Native Interface) is a way in which the Java parts of an Android app can communicate with the native C++ parts. So when we call classifyImageBmp(bitmap) in our Java code, it will actually invoke the C++ function exported in tensorflow_jni.cc and return the value it returns."}
{"id": "http://jalammar.github.io//Supercharging-android-apps-using-tensorflow/_p18", "contents": "A Bitmap file cannot directly be sent to TensorFlow as input. It has be transformed into an input tensor that we\u2019d send in step #2 in the flow above. A tensor is an n-dimensional array of values, and is the motif TensorFlow uses to send data between all of its different parts/operations. This model expect a 3-dimensional array that supplies the Red/Green/Blue value of each pixel in the image. The dimensions are:"}
{"id": "http://jalammar.github.io//Supercharging-android-apps-using-tensorflow/_p19", "contents": "And the value of the cell would be the actual value of R or G or B channel for that pixel."}
{"id": "http://jalammar.github.io//Supercharging-android-apps-using-tensorflow/_p21", "contents": "(This is somewhat oversimplified. I glanced over two things for simplicity\u2019s sake. First is the conversion from the YUV format that the Android camera exports to the RGB format the model expects. Second is that the model actually takes a 4-dimensional tensor, but these three are the ones we care about)"}
{"id": "http://jalammar.github.io//Supercharging-android-apps-using-tensorflow/_p22", "contents": "As you read the example\u2019s README.md, you\u2019ll notice that it instructs you to download a zip file containing the TensorFlow model and add it to the assets directory. This zip file contains two files that are important for us:"}
{"id": "http://jalammar.github.io//Supercharging-android-apps-using-tensorflow/_p23", "contents": "tensorflow_inception_graph.pb- At 54 MBs unzipped, this file constitutes the majority of the APK size (58 MBs). This is our trained machine learning model and where the magic comes from. It\u2019s a pre-built TensorFlow Graph describing the exact operations needed to compute a classification from input image data. This Graph is serialized and encoded into binary with Google\u2019s Protocol Buffers so it can be deserialized across different platforms (think of it as a binary-encoded JSON file)."}
{"id": "http://jalammar.github.io//Supercharging-android-apps-using-tensorflow/_p24", "contents": "imagenet_comp_graph_label_strings.txt- this contains the 1000 classifications that the output of the model corresponds to (e.g. \u201cvending machine\u201d, \u201cwater bottle\u201d, \u201ccoffee mug\u201d). These classifications are defined by the ImageNet Large Scale Visual Recognition Challenge which the model was built to compete in."}
{"id": "http://jalammar.github.io//Supercharging-android-apps-using-tensorflow/_p25", "contents": "The model here is what\u2019s known as a deep convolutional neural network. It is built in the Inception architecture described in Going Deeper with Convolutions. Convolutional neural networks are some of the most popular models in deep learning. They have been very successful in image recognition (so much so, that most highly ranked teams in the competition used them)."}
{"id": "http://jalammar.github.io//Supercharging-android-apps-using-tensorflow/_p26", "contents": "The model is read from the file and fed into TensorFlow when the app starts up. This code  is actually really interesting to read and see how to communicate with tensorflow (if you run the app with your device connected to your computer, you can see these helpful log messages printed in logcat)."}
{"id": "http://jalammar.github.io//Supercharging-android-apps-using-tensorflow/_p27", "contents": "The Android app example is not built the traditional Gradle way. Because the app has to contain NDK elements as well as TensorFlow itself, a more elaborate build system was utilized. The example is configured to be built with Google\u2019s Bazel build system running from the TensorFlow root directory."}
{"id": "http://jalammar.github.io//Supercharging-android-apps-using-tensorflow/_p28", "contents": "The WORKSPACE file in the root directory specifies the main parameters of the project. The BUILD file in the Android directory instructs the build system to build the Java and C++ files of the app."}
{"id": "http://jalammar.github.io//Supercharging-android-apps-using-tensorflow/_p29", "contents": "Using a trained model in your app seems to be the lowest hanging fruit for mobile TensorFlow apps at the moment. While you can probably train a model on Android, mobile devices are not well suited for the intensive processing required by complex models with larger training sets."}
{"id": "http://jalammar.github.io//Supercharging-android-apps-using-tensorflow/_p30", "contents": "Want to learn more about machine learning? Consider checking out the Machine Learning course on Coursera. There\u2019s also a good discussion in /r/MachineLearning here: In your experience, which machine learning course on Coursera (or other MOOC web site) was the best?."}
{"id": "http://jalammar.github.io//Supercharging-android-apps-using-tensorflow/_p31", "contents": "Want to comment? /r/androiddev, Hacker News."}
