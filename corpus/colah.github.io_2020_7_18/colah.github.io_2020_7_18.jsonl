{"id": "https://colah.github.io/./contact.html_p0", "contents": "You can email me at christopherolah.co@gmail.com"}
{"id": "https://colah.github.io/./about.html_p0", "contents": "A wandering machine learning researcher, bouncing between groups. I want to understand things clearly, and explain them well."}
{"id": "https://colah.github.io/./about.html_p1", "contents": "Academic CV - Github - Twitter - Old Blog"}
{"id": "https://colah.github.io/../../about.html_p0", "contents": "Bad request"}
{"id": "https://colah.github.io/../../about.html_p1", "contents": "\n        Your browser sent a request that this server could not understand.\n        Sorry about that. Please check your request for errors and try again.\n        Contact us if the problem persists.\n      "}
{"id": "https://colah.github.io/../../contact.html_p0", "contents": "Bad request"}
{"id": "https://colah.github.io/../../contact.html_p1", "contents": "\n        Your browser sent a request that this server could not understand.\n        Sorry about that. Please check your request for errors and try again.\n        Contact us if the problem persists.\n      "}
{"id": "https://colah.github.io/../../posts/2014-10-Visualizing-MNIST/_p0", "contents": "Bad request"}
{"id": "https://colah.github.io/../../posts/2014-10-Visualizing-MNIST/_p1", "contents": "\n        Your browser sent a request that this server could not understand.\n        Sorry about that. Please check your request for errors and try again.\n        Contact us if the problem persists.\n      "}
{"id": "https://colah.github.io/../../posts/2014-07-Understanding-Convolutions/_p0", "contents": "Bad request"}
{"id": "https://colah.github.io/../../posts/2014-07-Understanding-Convolutions/_p1", "contents": "\n        Your browser sent a request that this server could not understand.\n        Sorry about that. Please check your request for errors and try again.\n        Contact us if the problem persists.\n      "}
{"id": "https://colah.github.io/../../posts/2014-07-Conv-Nets-Modular/_p0", "contents": "Bad request"}
{"id": "https://colah.github.io/../../posts/2014-07-Conv-Nets-Modular/_p1", "contents": "\n        Your browser sent a request that this server could not understand.\n        Sorry about that. Please check your request for errors and try again.\n        Contact us if the problem persists.\n      "}
{"id": "https://colah.github.io/../../posts/2014-07-NLP-RNNs-Representations/_p0", "contents": "Bad request"}
{"id": "https://colah.github.io/../../posts/2014-07-NLP-RNNs-Representations/_p1", "contents": "\n        Your browser sent a request that this server could not understand.\n        Sorry about that. Please check your request for errors and try again.\n        Contact us if the problem persists.\n      "}
{"id": "https://colah.github.io/../../posts/2014-03-NN-Manifolds-Topology/_p0", "contents": "Bad request"}
{"id": "https://colah.github.io/../../posts/2014-03-NN-Manifolds-Topology/_p1", "contents": "\n        Your browser sent a request that this server could not understand.\n        Sorry about that. Please check your request for errors and try again.\n        Contact us if the problem persists.\n      "}
{"id": "https://colah.github.io/../../rss.xml_p0", "contents": "Bad request"}
{"id": "https://colah.github.io/../../rss.xml_p1", "contents": "\n        Your browser sent a request that this server could not understand.\n        Sorry about that. Please check your request for errors and try again.\n        Contact us if the problem persists.\n      "}
{"id": "https://colah.github.io/../../posts/tags/neural_networks.html_p0", "contents": "Bad request"}
{"id": "https://colah.github.io/../../posts/tags/neural_networks.html_p1", "contents": "\n        Your browser sent a request that this server could not understand.\n        Sorry about that. Please check your request for errors and try again.\n        Contact us if the problem persists.\n      "}
{"id": "https://colah.github.io/../../posts/tags/deep_learning.html_p0", "contents": "Bad request"}
{"id": "https://colah.github.io/../../posts/tags/deep_learning.html_p1", "contents": "\n        Your browser sent a request that this server could not understand.\n        Sorry about that. Please check your request for errors and try again.\n        Contact us if the problem persists.\n      "}
{"id": "https://colah.github.io/../../posts/tags/representations.html_p0", "contents": "Bad request"}
{"id": "https://colah.github.io/../../posts/tags/representations.html_p1", "contents": "\n        Your browser sent a request that this server could not understand.\n        Sorry about that. Please check your request for errors and try again.\n        Contact us if the problem persists.\n      "}
{"id": "https://colah.github.io/../../posts/tags/NLP.html_p0", "contents": "Bad request"}
{"id": "https://colah.github.io/../../posts/tags/NLP.html_p1", "contents": "\n        Your browser sent a request that this server could not understand.\n        Sorry about that. Please check your request for errors and try again.\n        Contact us if the problem persists.\n      "}
{"id": "https://colah.github.io/../../posts/tags/recursive_neural_networks.html_p0", "contents": "Bad request"}
{"id": "https://colah.github.io/../../posts/tags/recursive_neural_networks.html_p1", "contents": "\n        Your browser sent a request that this server could not understand.\n        Sorry about that. Please check your request for errors and try again.\n        Contact us if the problem persists.\n      "}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p0", "contents": "Posted on July  7, 2014"}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p1", "contents": "neural networks, deep learning, representations, NLP, recursive neural networks"}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p2", "contents": "In the last few years, deep neural networks have dominated pattern recognition. They blew the previous state of the art out of the water for many computer vision tasks. Voice recognition is also moving that way."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p3", "contents": "But despite the results, we have to wonder\u2026 why do they work so well?"}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p4", "contents": "This post reviews some extremely remarkable results in applying deep neural networks to natural language processing (NLP). In doing so, I hope to make accessible one promising answer as to why deep neural networks work. I think it\u2019s a very elegant perspective."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p5", "contents": "A neural network with a hidden layer has universality: given enough hidden units, it can approximate any function. This is a frequently quoted \u2013 and even more frequently, misunderstood and applied \u2013 theorem."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p6", "contents": "It\u2019s true, essentially, because the hidden layer can be used as a lookup table."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p7", "contents": "For simplicity, let\u2019s consider a perceptron network. A perceptron is a very simple neuron that fires if it exceeds a certain threshold and doesn\u2019t fire if it doesn\u2019t reach that threshold. A perceptron network gets binary (0 and 1) inputs and gives binary outputs."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p8", "contents": "Note that there are only a finite number of possible inputs. For each possible input, we can construct a neuron in the hidden layer that fires for that input,1 and only on that specific input. Then we can use the connections between that neuron and the output neurons to control the output in that specific case. 2"}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p9", "contents": "And so, it\u2019s true that one hidden layer neural networks are universal. But there isn\u2019t anything particularly impressive or exciting about that. Saying that your model can do the same thing as a lookup table isn\u2019t a very strong argument for it. It just means it isn\u2019t impossible for your model to do the task."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p10", "contents": "Universality means that a network can fit to any training data you give it. It doesn\u2019t mean that it will interpolate to new data points in a reasonable way."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p11", "contents": "No, universality isn\u2019t an explanation for why neural networks work so well. The real reason seems to be something much more subtle\u2026 And, to understand it, we\u2019ll first need to understand some concrete results."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p12", "contents": "I\u2019d like to start by tracing a particularly interesting strand of deep learning research: word embeddings. In my personal opinion, word embeddings are one of the most exciting area of research in deep learning at the moment, although they were originally introduced by Bengio, et al. more than a decade ago.3 Beyond that, I think they are one of the best places to gain intuition about why deep learning is so effective."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p13", "contents": "A word embedding \\(W: \\mathrm{words} \\to \\mathbb{R}^n\\) is a paramaterized function mapping words in some language to high-dimensional vectors (perhaps 200 to 500 dimensions). For example, we might find:"}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p14", "contents": "\\[W(``\\text{cat}\\!\") = (0.2,~ \\text{-}0.4,~ 0.7,~ ...)\\]"}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p15", "contents": "\\[W(``\\text{mat}\\!\") = (0.0,~ 0.6,~ \\text{-}0.1,~ ...)\\]"}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p16", "contents": "(Typically, the function is a lookup table, parameterized by a matrix, \\(\\theta\\), with a row for each word: \\(W_\\theta(w_n) = \\theta_n\\).)"}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p17", "contents": "\\(W\\) is initialized to have random vectors for each word. It learns to have meaningful vectors in order to perform some task."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p18", "contents": "For example, one task we might train a network for is predicting whether a 5-gram (sequence of five words) is \u2018valid.\u2019 We can easily get lots of 5-grams from Wikipedia (eg. \u201ccat sat on the mat\u201d) and then \u2018break\u2019 half of them by switching a word with a random word (eg. \u201ccat sat song the mat\u201d), since that will almost certainly make our 5-gram nonsensical."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p19", "contents": "The model we train will run each word in the 5-gram through \\(W\\) to get a vector representing it and feed those into another \u2018module\u2019 called \\(R\\) which tries to predict if the 5-gram is \u2018valid\u2019 or \u2018broken.\u2019 Then, we\u2019d like:"}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p20", "contents": "\\[R(W(``\\text{cat}\\!\"),~ W(``\\text{sat}\\!\"),~ W(``\\text{on}\\!\"),~ W(``\\text{the}\\!\"),~ W(``\\text{mat}\\!\")) = 1\\]"}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p21", "contents": "\\[R(W(``\\text{cat}\\!\"),~ W(``\\text{sat}\\!\"),~ W(``\\text{song}\\!\"),~ W(``\\text{the}\\!\"),~ W(``\\text{mat}\\!\")) = 0\\]"}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p22", "contents": "In order to predict these values accurately, the network needs to learn good parameters for both \\(W\\) and \\(R\\)."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p23", "contents": "Now, this task isn\u2019t terribly interesting. Maybe it could be helpful in detecting grammatical errors in text or something. But what is extremely interesting is \\(W\\)."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p24", "contents": "(In fact, to us, the entire point of the task is to learn \\(W\\). We could have done several other tasks \u2013 another common one is predicting the next word in the sentence. But we don\u2019t really care. In the remainder of this section we will talk about many word embedding results and won\u2019t distinguish between different approaches.)"}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p25", "contents": "One thing we can do to get a feel for the word embedding space is to visualize them with t-SNE, a sophisticated technique for visualizing high-dimensional data."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p26", "contents": "This kind of \u2018map\u2019 of words makes a lot of intuitive sense to us. Similar words are close together. Another way to get at this is to look at which words are closest in the embedding to a given word. Again, the words tend to be quite similar."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p27", "contents": "It seems natural for a network to make words with similar meanings have similar vectors. If you switch a word for a synonym (eg. \u201ca few people sing well\u201d \\(\\to\\) \u201ca couple people sing well\u201d), the validity of the sentence doesn\u2019t change. While, from a naive perspective, the input sentence has changed a lot, if \\(W\\) maps synonyms (like \u201cfew\u201d and \u201ccouple\u201d) close together, from \\(R\\)\u2019s perspective little changes."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p28", "contents": "This is very powerful. The number of possible 5-grams is massive and we have a comparatively small number of data points to try to learn from. Similar words being close together allows us to generalize from one sentence to a class of similar sentences. This doesn\u2019t just mean switching a word for a synonym, but also switching a word for a word in a similar class (eg. \u201cthe wall is blue\u201d \\(\\to\\) \u201cthe wall is red\u201d). Further, we can change multiple words (eg. \u201cthe wall is blue\u201d \\(\\to\\) \u201cthe ceiling is red\u201d). The impact of this is exponential with respect to the number of words.4"}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p29", "contents": "So, clearly this is a very useful thing for \\(W\\) to do. But how does it learn to do this? It seems quite likely that there are lots of situations where it has seen a sentence like \u201cthe wall is blue\u201d and know that it is valid before it sees a sentence like \u201cthe wall is red\u201d. As such, shifting \u201cred\u201d a bit closer to \u201cblue\u201d makes the network perform better."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p30", "contents": "We still need to see examples of every word being used, but the analogies allow us to generalize to new combinations of words. You\u2019ve seen all the words that you understand before, but you haven\u2019t seen all the sentences that you understand before. So too with neural networks."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p31", "contents": "Word embeddings exhibit an even more remarkable property: analogies between words seem to be encoded in the difference vectors between words. For example, there seems to be a constant male-female difference vector:"}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p32", "contents": "\\[W(``\\text{woman}\\!\") - W(``\\text{man}\\!\") ~\\simeq~ W(``\\text{aunt}\\!\") - W(``\\text{uncle}\\!\")\\] \\[W(``\\text{woman}\\!\") - W(``\\text{man}\\!\") ~\\simeq~ W(``\\text{queen}\\!\") - W(``\\text{king}\\!\")\\]"}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p33", "contents": "This may not seem too surprising. After all, gender pronouns mean that switching a word can make a sentence grammatically incorrect. You write, \u201cshe is the aunt\u201d but \u201che is the uncle.\u201d Similarly, \u201che is the King\u201d but \u201cshe is the Queen.\u201d If one sees \u201cshe is the uncle,\u201d the most likely explanation is a grammatical error. If words are being randomly switched half the time, it seems pretty likely that happened here."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p34", "contents": "\u201cOf course!\u201d We say with hindsight, \u201cthe word embedding will learn to encode gender in a consistent way. In fact, there\u2019s probably a gender dimension. Same thing for singular vs plural. It\u2019s easy to find these trivial relationships!\u201d"}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p35", "contents": "It turns out, though, that much more sophisticated relationships are also encoded in this way. It seems almost miraculous!"}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p36", "contents": "It\u2019s important to appreciate that all of these properties of \\(W\\) are side effects. We didn\u2019t try to have similar words be close together. We didn\u2019t try to have analogies encoded with difference vectors. All we tried to do was perform a simple task, like predicting whether a sentence was valid. These properties more or less popped out of the optimization process."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p37", "contents": "This seems to be a great strength of neural networks: they learn better ways to represent data, automatically. Representing data well, in turn, seems to be essential to success at many machine learning problems. Word embeddings are just a particularly striking example of learning a representation."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p38", "contents": "The properties of word embeddings are certainly interesting, but can we do anything useful with them? Besides predicting silly things, like whether a 5-gram is \u2018valid\u2019?"}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p39", "contents": "We learned the word embedding in order to do well on a simple task, but based on the nice properties we\u2019ve observed in word embeddings, you may suspect that they could be generally useful in NLP tasks. In fact, word representations like these are extremely important:"}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p40", "contents": "The use of word representations\u2026 has become a key \u201csecret sauce\u201d for the success of many NLP systems in recent years, across tasks including named entity recognition, part-of-speech tagging, parsing, and semantic role labeling. (Luong et al. (2013))"}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p41", "contents": "This general tactic \u2013 learning a good representation on a task A and then using it on a task B \u2013 is one of the major tricks in the Deep Learning toolbox. It goes by different names depending on the details: pretraining, transfer learning, and multi-task learning. One of the great strengths of this approach is that it allows the representation to learn from more than one kind of data."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p42", "contents": "There\u2019s a counterpart to this trick. Instead of learning a way to represent one kind of data and using it to perform multiple kinds of tasks, we can learn a way to map multiple kinds of data into a single representation!"}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p43", "contents": "One nice example of this is a bilingual word-embedding, produced in Socher et al. (2013a). We can learn to embed words from two different languages in a single, shared space. In this case, we learn to embed English and Mandarin Chinese words in the same space."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p44", "contents": "We train two word embeddings, \\(W_{en}\\) and \\(W_{zh}\\) in a manner similar to how we did above. However, we know that certain English words and Chinese words have similar meanings. So, we optimize for an additional property: words that we know are close translations should be close together."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p45", "contents": "Of course, we observe that the words we knew had similar meanings end up close together. Since we optimized for that, it\u2019s not surprising. More interesting is that words we didn\u2019t know were translations end up close together."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p46", "contents": "In light of our previous experiences with word embeddings, this may not seem too surprising. Word embeddings pull similar words together, so if an English and Chinese word we know to mean similar things are near each other, their synonyms will also end up near each other. We also know that things like gender differences tend to end up being represented with a constant difference vector. It seems like forcing enough points to line up should force these difference vectors to be the same in both the English and Chinese embeddings. A result of this would be that if we know that two male versions of words translate to each other, we should also get the female words to translate to each other."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p47", "contents": "Intuitively, it feels a bit like the two languages have a similar \u2018shape\u2019 and that by forcing them to line up at different points, they overlap and other points get pulled into the right positions."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p48", "contents": "In bilingual word embeddings, we learn a shared representation for two very similar kinds of data. But we can also learn to embed very different kinds of data in the same space."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p49", "contents": "Recently, deep learning has begun exploring models that embed images and words in a single representation.5"}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p50", "contents": "The basic idea is that one classifies images by outputting a vector in a word embedding. Images of dogs are mapped near the \u201cdog\u201d word vector. Images of horses are mapped near the \u201chorse\u201d vector. Images of automobiles near the \u201cautomobile\u201d vector. And so on."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p51", "contents": "The interesting part is what happens when you test the model on new classes of images. For example, if the model wasn\u2019t trained to classify cats \u2013 that is, to map them near the \u201ccat\u201d vector \u2013 what happens when we try to classify images of cats?"}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p52", "contents": "It turns out that the network is able to handle these new classes of images quite reasonably. Images of cats aren\u2019t mapped to random points in the word embedding space. Instead, they tend to be mapped to the general vicinity of the \u201cdog\u201d vector, and, in fact, close to the \u201ccat\u201d vector. Similarly, the truck images end up relatively close to the \u201ctruck\u201d vector, which is near the related \u201cautomobile\u201d vector."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p53", "contents": "This was done by members of the Stanford group with only 8 known classes (and 2 unknown classes). The results are already quite impressive. But with so few known classes, there are very few points to interpolate the relationship between images and semantic space off of."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p54", "contents": "The Google group did a much larger version \u2013 instead of 8 categories, they used 1,000 \u2013 around the same time (Frome et al. (2013)) and has followed up with a new variation (Norouzi et al. (2014)). Both are based on a very powerful image classification model (from Krizehvsky et al. (2012)), but embed images into the word embedding space in different ways."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p55", "contents": "The results are impressive. While they may not get images of unknown classes to the precise vector representing that class, they are able to get to the right neighborhood. So, if you ask it to classify images of unknown classes and the classes are fairly different, it can distinguish between the different classes."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p56", "contents": "Even though I\u2019ve never seen a Aesculapian snake or an Armadillo before, if you show me a picture of one and a picture of the other, I can tell you which is which because I have a general idea of what sort of animal is associated with each word. These networks can accomplish the same thing."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p57", "contents": "(These results all exploit a sort of \u201cthese words are similar\u201d reasoning. But it seems like much stronger results should be possible based on relationships between words. In our word embedding space, there is a consistent difference vector between male and female version of words. Similarly, in image space, there are consistent features distinguishing between male and female. Beards, mustaches, and baldness are all strong, highly visible indicators of being male. Breasts and, less reliably, long hair, makeup and jewelery, are obvious indicators of being female.6 Even if you\u2019ve never seen a king before, if the queen, determined to be such by the presence of a crown, suddenly has a beard, it\u2019s pretty reasonable to give the male version.)"}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p58", "contents": "Shared embeddings are an extremely exciting area of research and drive at why the representation focused perspective of deep learning is so compelling."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p59", "contents": "We began our discussion of word embeddings with the following network:"}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p60", "contents": "The above diagram represents a modular network, \\(R(W(w_1),~ W(w_2),~ W(w_3),~ W(w_4),~ W(w_5))\\). It is built from two modules, \\(W\\) and \\(R\\). This approach, of building neural networks from smaller neural network \u201cmodules\u201d that can be composed together, is not very wide spread. It has, however, been very successful in NLP."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p61", "contents": "Models like the above are powerful, but they have an unfortunate limitation: they can only have a fixed number of inputs."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p62", "contents": "We can overcome this by adding an association module, \\(A\\), which will take two word or phrase representations and merge them."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p63", "contents": "By merging sequences of words, \\(A\\) takes us from representing words to representing phrases or even representing whole sentences! And because we can merge together different numbers of words, we don\u2019t have to have a fixed number of inputs."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p64", "contents": "It doesn\u2019t necessarily make sense to merge together words in a sentence linearly. If one considers the phrase \u201cthe cat sat on the mat\u201d, it can naturally be bracketed into segments: \u201c((the cat) (sat (on (the mat))))\u201d. We can apply \\(A\\) based on this bracketing:"}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p65", "contents": "These models are often called \u201crecursive neural networks\u201d because one often has the output of a module go into a module of the same type. They are also sometimes called \u201ctree-structured neural networks.\u201d"}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p66", "contents": "Recursive neural networks have had significant successes in a number of NLP tasks. For example, Socher et al. (2013c) uses a recursive neural network to predict sentence sentiment:"}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p67", "contents": "One major goal has been to create a reversible sentence representation, a representation that one can reconstruct an actual sentence from, with roughly the same meaning. For example, we can try to introduce a disassociation module, \\(D\\), that tries to undo \\(A\\):"}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p68", "contents": "If we could accomplish such a thing, it would be an extremely powerful tool. For example, we could try to make a bilingual sentence representation and use it for translation."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p69", "contents": "Unfortunately, this turns out to be very difficult. Very very difficult. And given the tremendous promise, there are lots of people working on it."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p70", "contents": "Recently, Cho et al. (2014) have made some progress on representing phrases, with a model that can encode English phrases and decode them in French. Look at the phrase representations it learns!"}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p71", "contents": "I\u2019ve heard some of the results reviewed above criticized by researchers in other fields, in particular, in NLP and linguistics. The concerns are not with the results themselves, but the conclusions drawn from them, and how they compare to other techniques."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p72", "contents": "I don\u2019t feel qualified to articulate these concerns. I\u2019d encourage someone who feels this way to describe the concerns in the comments."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p73", "contents": "The representation perspective of deep learning is a powerful view that seems to answer why deep neural networks are so effective. Beyond that, I think there\u2019s something extremely beautiful about it: why are neural networks effective? Because better ways of representing data can pop out of optimizing layered models."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p74", "contents": "Deep learning is a very young field, where theories aren\u2019t strongly established and views quickly change. That said, it is my impression that the representation-focused perspective of neural networks is presently very popular."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p75", "contents": "This post reviews a lot of research results I find very exciting, but my main motivation is to set the stage for a future post exploring connections between deep learning, type theory and functional programming. If you\u2019re interested, you can subscribe to my rss feed so that you\u2019ll see it when it is published."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p76", "contents": "(I would be delighted to hear your comments and thoughts: you can comment inline or at the end. For typos, technical errors, or clarifications you would like to see added, you are encouraged to make a pull request on github)"}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p77", "contents": "I\u2019m grateful to Eliana Lorch, Yoshua Bengio, Michael Nielsen, Laura Ball, Rob Gilson, and Jacob Steinhardt for their comments and support."}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p78", "contents": "Constructing a case for every possible input requires \\(2^n\\) hidden neurons, when you have \\(n\\) input neurons. In reality, the situation isn\u2019t usually that bad. You can have cases that encompass multiple inputs. And you can have overlapping cases that add together to achieve the right input on their intersection.\u21a9"}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p79", "contents": "(It isn\u2019t only perceptron networks that have universality. Networks of sigmoid neurons (and other activation functions) are also universal: give enough hidden neurons, they can approximate any continuous function arbitrarily well. Seeing this is significantly trickier because you can\u2019t just isolate inputs.)\u21a9"}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p80", "contents": "Word embeddings were originally developed in (Bengio et al, 2001; Bengio et al, 2003), a few years before the 2006 deep learning renewal, at a time when neural networks were out of fashion. The idea of distributed representations for symbols is even older, e.g. (Hinton 1986).\"\u21a9"}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p81", "contents": "The seminal paper, A Neural Probabilistic Language Model (Bengio, et al. 2003) has a great deal of insight about why word embeddings are powerful.\u21a9"}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p82", "contents": "Previous work has been done modeling the joint distributions of tags and images, but it took a very different perspective.\u21a9"}
{"id": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/_p83", "contents": "I\u2019m very conscious that physical indicators of gender can be misleading. I don\u2019t mean to imply, for example, that everyone who is bald is male or everyone who has breasts is female. Just that these often indicate such, and greatly adjust our prior.\u21a9"}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p0", "contents": "Posted on August 27, 2015"}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p1", "contents": "Humans don\u2019t start their thinking from scratch every second. As you read this essay, you understand each word based on your understanding of previous words. You don\u2019t throw everything away and start thinking from scratch again. Your thoughts have persistence."}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p2", "contents": "Traditional neural networks can\u2019t do this, and it seems like a major shortcoming. For example, imagine you want to classify what kind of event is happening at every point in a movie. It\u2019s unclear how a traditional neural network could use its reasoning about previous events in the film to inform later ones."}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p3", "contents": "Recurrent neural networks address this issue. They are networks with loops in them, allowing information to persist."}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p4", "contents": "In the above diagram, a chunk of neural network, \\(A\\), looks at some input \\(x_t\\) and outputs a value \\(h_t\\). A loop allows information to be passed from one step of the network to the next."}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p5", "contents": "These loops make recurrent neural networks seem kind of mysterious. However, if you think a bit more, it turns out that they aren\u2019t all that different than a normal neural network. A recurrent neural network can be thought of as multiple copies of the same network, each passing a message to a successor. Consider what happens if we unroll the loop:"}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p6", "contents": "This chain-like nature reveals that recurrent neural networks are intimately related to sequences and lists. They\u2019re the natural architecture of neural network to use for such data."}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p7", "contents": "And they certainly are used! In the last few years, there have been incredible success applying RNNs to a variety of problems: speech recognition, language modeling, translation, image captioning\u2026 The list goes on. I\u2019ll leave discussion of the amazing feats one can achieve with RNNs to Andrej Karpathy\u2019s excellent blog post, The Unreasonable Effectiveness of Recurrent Neural Networks. But they really are pretty amazing."}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p8", "contents": "Essential to these successes is the use of \u201cLSTMs,\u201d a very special kind of recurrent neural network which works, for many tasks, much much better than the standard version. Almost all exciting results based on recurrent neural networks are achieved with them. It\u2019s these LSTMs that this essay will explore."}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p9", "contents": "One of the appeals of RNNs is the idea that they might be able to connect previous information to the present task, such as using previous video frames might inform the understanding of the present frame. If RNNs could do this, they\u2019d be extremely useful. But can they? It depends."}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p10", "contents": "Sometimes, we only need to look at recent information to perform the present task. For example, consider a language model trying to predict the next word based on the previous ones. If we are trying to predict the last word in \u201cthe clouds are in the sky,\u201d we don\u2019t need any further context \u2013 it\u2019s pretty obvious the next word is going to be sky. In such cases, where the gap between the relevant information and the place that it\u2019s needed is small, RNNs can learn to use the past information."}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p11", "contents": "But there are also cases where we need more context. Consider trying to predict the last word in the text \u201cI grew up in France\u2026 I speak fluent French.\u201d Recent information suggests that the next word is probably the name of a language, but if we want to narrow down which language, we need the context of France, from further back. It\u2019s entirely possible for the gap between the relevant information and the point where it is needed to become very large."}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p12", "contents": "Unfortunately, as that gap grows, RNNs become unable to learn to connect the information."}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p13", "contents": "In theory, RNNs are absolutely capable of handling such \u201clong-term dependencies.\u201d A human could carefully pick parameters for them to solve toy problems of this form. Sadly, in practice, RNNs don\u2019t seem to be able to learn them. The problem was explored in depth by Hochreiter (1991) [German] and Bengio, et al. (1994), who found some pretty fundamental reasons why it might be difficult."}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p14", "contents": "Thankfully, LSTMs don\u2019t have this problem!"}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p15", "contents": "Long Short Term Memory networks \u2013 usually just called \u201cLSTMs\u201d \u2013 are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter & Schmidhuber (1997), and were refined and popularized by many people in following work.1 They work tremendously well on a large variety of problems, and are now widely used."}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p16", "contents": "LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!"}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p17", "contents": "All recurrent neural networks have the form of a chain of repeating modules of neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer."}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p18", "contents": "LSTMs also have this chain like structure, but the repeating module has a different structure. Instead of having a single neural network layer, there are four, interacting in a very special way."}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p19", "contents": "Don\u2019t worry about the details of what\u2019s going on. We\u2019ll walk through the LSTM diagram step by step later. For now, let\u2019s just try to get comfortable with the notation we\u2019ll be using."}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p20", "contents": "In the above diagram, each line carries an entire vector, from the output of one node to the inputs of others. The pink circles represent pointwise operations, like vector addition, while the yellow boxes are learned neural network layers. Lines merging denote concatenation, while a line forking denote its content being copied and the copies going to different locations. "}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p21", "contents": "The key to LSTMs is the cell state, the horizontal line running through the top of the diagram."}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p22", "contents": "The cell state is kind of like a conveyor belt. It runs straight down the entire chain, with only some minor linear interactions. It\u2019s very easy for information to just flow along it unchanged."}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p23", "contents": "The LSTM does have the ability to remove or add information to the cell state, carefully regulated by structures called gates."}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p24", "contents": "Gates are a way to optionally let information through. They are composed out of a sigmoid neural net layer and a pointwise multiplication operation."}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p25", "contents": "The sigmoid layer outputs numbers between zero and one, describing how much of each component should be let through. A value of zero means \u201clet nothing through,\u201d while a value of one means \u201clet everything through!\u201d"}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p26", "contents": "An LSTM has three of these gates, to protect and control the cell state."}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p27", "contents": "The first step in our LSTM is to decide what information we\u2019re going to throw away from the cell state. This decision is made by a sigmoid layer called the \u201cforget gate layer.\u201d It looks at \\(h_{t-1}\\) and \\(x_t\\), and outputs a number between \\(0\\) and \\(1\\) for each number in the cell state \\(C_{t-1}\\). A \\(1\\) represents \u201ccompletely keep this\u201d while a \\(0\\) represents \u201ccompletely get rid of this.\u201d"}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p28", "contents": "Let\u2019s go back to our example of a language model trying to predict the next word based on all the previous ones. In such a problem, the cell state might include the gender of the present subject, so that the correct pronouns can be used. When we see a new subject, we want to forget the gender of the old subject."}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p29", "contents": "The next step is to decide what new information we\u2019re going to store in the cell state. This has two parts. First, a sigmoid layer called the \u201cinput gate layer\u201d decides which values we\u2019ll update. Next, a tanh layer creates a vector of new candidate values, \\(\\tilde{C}_t\\), that could be added to the state. In the next step, we\u2019ll combine these two to create an update to the state."}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p30", "contents": "In the example of our language model, we\u2019d want to add the gender of the new subject to the cell state, to replace the old one we\u2019re forgetting."}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p31", "contents": "It\u2019s now time to update the old cell state, \\(C_{t-1}\\), into the new cell state \\(C_t\\). The previous steps already decided what to do, we just need to actually do it."}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p32", "contents": "We multiply the old state by \\(f_t\\), forgetting the things we decided to forget earlier. Then we add \\(i_t*\\tilde{C}_t\\). This is the new candidate values, scaled by how much we decided to update each state value."}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p33", "contents": "In the case of the language model, this is where we\u2019d actually drop the information about the old subject\u2019s gender and add the new information, as we decided in the previous steps."}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p34", "contents": "Finally, we need to decide what we\u2019re going to output. This output will be based on our cell state, but will be a filtered version. First, we run a sigmoid layer which decides what parts of the cell state we\u2019re going to output. Then, we put the cell state through \\(\\tanh\\) (to push the values to be between \\(-1\\) and \\(1\\)) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to."}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p35", "contents": "For the language model example, since it just saw a subject, it might want to output information relevant to a verb, in case that\u2019s what is coming next. For example, it might output whether the subject is singular or plural, so that we know what form a verb should be conjugated into if that\u2019s what follows next."}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p36", "contents": "What I\u2019ve described so far is a pretty normal LSTM. But not all LSTMs are the same as the above. In fact, it seems like almost every paper involving LSTMs uses a slightly different version. The differences are minor, but it\u2019s worth mentioning some of them."}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p37", "contents": "One popular LSTM variant, introduced by Gers & Schmidhuber (2000), is adding \u201cpeephole connections.\u201d This means that we let the gate layers look at the cell state."}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p38", "contents": "The above diagram adds peepholes to all the gates, but many papers will give some peepholes and not others."}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p39", "contents": "Another variation is to use coupled forget and input gates. Instead of separately deciding what to forget and what we should add new information to, we make those decisions together. We only forget when we\u2019re going to input something in its place. We only input new values to the state when we forget something older."}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p40", "contents": "A slightly more dramatic variation on the LSTM is the Gated Recurrent Unit, or GRU, introduced by Cho, et al. (2014). It combines the forget and input gates into a single \u201cupdate gate.\u201d It also merges the cell state and hidden state, and makes some other changes. The resulting model is simpler than standard LSTM models, and has been growing increasingly popular."}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p41", "contents": "These are only a few of the most notable LSTM variants. There are lots of others, like Depth Gated RNNs by Yao, et al. (2015). There\u2019s also some completely different approach to tackling long-term dependencies, like Clockwork RNNs by Koutnik, et al. (2014)."}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p42", "contents": "Which of these variants is best? Do the differences matter? Greff, et al. (2015) do a nice comparison of popular variants, finding that they\u2019re all about the same. Jozefowicz, et al. (2015) tested more than ten thousand RNN architectures, finding some that worked better than LSTMs on certain tasks."}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p43", "contents": "Earlier, I mentioned the remarkable results people are achieving with RNNs. Essentially all of these are achieved using LSTMs. They really work a lot better for most tasks!"}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p44", "contents": "Written down as a set of equations, LSTMs look pretty intimidating. Hopefully, walking through them step by step in this essay has made them a bit more approachable."}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p45", "contents": "LSTMs were a big step in what we can accomplish with RNNs. It\u2019s natural to wonder: is there another big step? A common opinion among researchers is: \u201cYes! There is a next step and it\u2019s attention!\u201d The idea is to let every step of an RNN pick information to look at from some larger collection of information. For example, if you are using an RNN to create a caption describing an image, it might pick a part of the image to look at for every word it outputs. In fact, Xu, et al. (2015) do exactly this \u2013 it might be a fun starting point if you want to explore attention! There\u2019s been a number of really exciting results using attention, and it seems like a lot more are around the corner\u2026"}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p46", "contents": "Attention isn\u2019t the only exciting thread in RNN research. For example, Grid LSTMs by Kalchbrenner, et al. (2015) seem extremely promising. Work using RNNs in generative models \u2013 such as Gregor, et al. (2015), Chung, et al. (2015), or Bayer & Osendorfer (2015)  \u2013 also seems very interesting. The last few years have been an exciting time for recurrent neural networks, and the coming ones promise to only be more so!"}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p47", "contents": "I\u2019m grateful to a number of people for helping me better understand LSTMs, commenting on the visualizations, and providing feedback on this post."}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p48", "contents": "I\u2019m very grateful to my colleagues at Google for their helpful feedback, especially Oriol Vinyals, Greg Corrado, Jon Shlens, Luke Vilnis, and Ilya Sutskever. I\u2019m also thankful to many other friends and colleagues for taking the time to help me, including Dario Amodei, and Jacob Steinhardt. I\u2019m especially thankful to Kyunghyun Cho for extremely thoughtful correspondence about my diagrams."}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p49", "contents": "Before this post, I practiced explaining LSTMs during two seminar series I taught on neural networks. Thanks to everyone who participated in those for their patience with me, and for their feedback."}
{"id": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/_p50", "contents": "In addition to the original authors, a lot of people contributed to the modern LSTM. A non-comprehensive list is: Felix Gers, Fred Cummins, Santiago Fernandez, Justin Bayer, Daan Wierstra, Julian Togelius, Faustino Gomez, Matteo Gagliolo, and Alex Graves.\u21a9"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p0", "contents": "Posted on January 16, 2015"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p1", "contents": "data visualization, machine learning, word embeddings, neural networks, deep learning, user interface, wikipedia"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p2", "contents": "In a previous post, we explored techniques for visualizing high-dimensional data. Trying to visualize high dimensional data is, by itself, very interesting, but my real goal is something else. I think these techniques form a set of basic building blocks to try and understand machine learning, and specifically to understand the internal operations of deep neural networks."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p3", "contents": "Deep neural networks are an approach to machine learning that has revolutionized computer vision and speech recognition in the last few years, blowing the previous state of the art results out of the water. They\u2019ve also brought promising results to many other areas, including language understanding and machine translation. Despite this, it remains challenging to understand what, exactly, these networks are doing."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p4", "contents": "I think that dimensionality reduction, thoughtfully applied, can give us a lot of traction on understanding neural networks."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p5", "contents": "Understanding neural networks is just scratching the surface, however, because understanding the network is fundamentally tied to understanding the data it operates on. The combination of neural networks and dimensionality reduction turns out to be a very interesting tool for visualizing high-dimensional data \u2013 a much more powerful tool than dimensionality reduction on its own."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p6", "contents": "As we dig into this, we\u2019ll observe what I believe to be an important connection between neural networks, visualization, and user interface."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p7", "contents": "Not all neural networks are hard to understand. In fact, low-dimensional neural networks \u2013 networks which have only two or three neurons in each layer \u2013 are quite easy to understand."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p8", "contents": "Consider the following dataset, consisting of two curves on the plane. Given a point on one of the curves, our network should predict which curve it came from."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p9", "contents": "A network with just an input layer and an output layer tries to divide the two classes with a straight line."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p10", "contents": "In the case of this dataset, it is not possible to classify it perfectly by dividing it with a straight line. And so, a network with only an input layer and an output layer can not classify it perfectly."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p11", "contents": "But, in practice, neural networks have additional layers in the middle, called \u201chidden\u201d layers. These layers warp and reshape the data to make it easier to classify."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p12", "contents": "We call the versions of the data corresponding to different layers representations.1 The input layer\u2019s representation is the raw data. The middle \u201chidden\u201d layer\u2019s representation is a warped, easier to classify, version of the raw data."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p13", "contents": "Low-dimensional neural networks are really easy to reason about because we can just look at their representations, and at how one representation transforms into another. If we have a question about what it is doing, we can just look. (There\u2019s quite a bit we can learn from low-dimensional neural networks, as explored in my post Neural Networks, Manifolds, and Topology.)"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p14", "contents": "Unfortunately, neural networks are usually not low-dimensional. The strength of neural networks is classifying high-dimensional data, like computer vision data, which often has tens or hundreds of thousands of dimensions. The hidden representations we learn are also of very high dimensionality."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p15", "contents": "For example, suppose we are trying to classify MNIST. The input representation, MNIST, is a collection of 784-dimensional vectors! And, even for a very simple network, we\u2019ll have a high-dimensional hidden representation. To be concrete, let\u2019s use one hidden layer with a hundred sigmoid neurons."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p16", "contents": "While we can\u2019t visualize the high-dimensional representations directly, we can visualize them using dimensionality reduction. Below, we look at nearest neighbor graphs of MNIST in its raw form and in a hidden representation from a trained MNIST network."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p17", "contents": "At the input layer, the classes are quite tangled. But, by the next layer, because the model has been trained to distinguish the digit classes, the hidden layer has learned to transform the data into a new representation in which the digit classes are much more separated."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p18", "contents": "This approach, visualizing high-dimensional representations using dimensionality reduction, is an extremely broadly applicable technique for inspecting models in deep learning."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p19", "contents": "In addition to helping us understand what a neural network is doing, inspecting representations allows us to understand the data itself. Even with sophisticated dimensionality reduction techniques, lots of real world data is incomprehensible \u2013 its structure is too complicated and chaotic. But higher level representations tend to be simpler and calmer, and much easier for humans to understand."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p20", "contents": "(To be clear, using dimensionality reduction on representations isn\u2019t novel. In fact, they\u2019ve become fairly common. One really beautiful example is Andrej Karpathy\u2019s visualizations of a high-level ImageNet representation. My contribution here isn\u2019t the basic idea, but taking it really seriously and seeing where it goes.)"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p21", "contents": "Word embeddings are a remarkable kind of representation. They form when we try to solve language tasks with neural networks."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p22", "contents": "For these tasks, the input to the network is typically a word, or multiple words. Each word can be thought of as a unit vector in a ridiculously high-dimensional space, with each dimension corresponding to a word in the vocabulary. The network warps and compresses this space, mapping words into a couple hundred dimensions. This is called a word embedding."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p23", "contents": "In a word embedding, every word is a couple hundred dimensional vector. These vectors have some really nice properties. The property we will visualize here is that words with similar meanings are close together."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p24", "contents": "(These embeddings have lots of other interesting properties, besides proximity. For example, directions in the embedding space seems to have semantic meaning. Further, difference vectors between words seem to encode analogies. For example, the difference between woman and man is approximately the same as the difference between queen and king: \\(v(``\\text{woman}\\!\") - v(``\\text{man}\\!\") ~\\simeq\\) \\(v(``\\text{queen}\\!\") - v(``\\text{king}\\!\")\\). For more on word embeddings, see my post Deep Learning, NLP, and Representations.)"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p25", "contents": "To visualize the word embedding in two dimensions, we need to choose a dimensionality reduction technique to use. t-SNE optimizes for keeping points close to their neighbors, so it is the natural tool if we want to visualize which words are close together in our word embedding."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p26", "contents": "Examining the t-SNE plot, we see that neighboring words tend to be related. But there\u2019s so many words! To get a higher-level view, lets highlight a few kinds of words.2 We can see areas corresponding to cities, food, body parts, feelings, relatives and different \u201ctravel\u201d verbs."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p28", "contents": "That\u2019s just scratching the surface. In the following interactive visualization, you can choose lots of different categories to color the words by. You can also inspect points individually by hovering over them, revealing the corresponding word."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p30", "contents": "Looking at the above visualization, we can see lots of clusters, from broad clusters like regions (region.n.03) and people (person.n.01), to smaller ones like body parts (body_part.n.01), units of distance (linear_unit.n.01) and food (food.n.01). The network successfully learned to put similar words close together."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p31", "contents": "Paragraph vectors, introduced by Le & Mikolov (2014), are vectors that represent chunks of text. Paragraph vectors come in a few variations but the simplest one, which we are using here, is basically some really nice features on top of a bag of words representation."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p32", "contents": "With word embeddings, we learn vectors in order to solve a language task involving the word. With paragraph vectors, we learn vectors in order to predict which words are in a paragraph."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p33", "contents": "Concretely, the neural network learns a low-dimensional approximation of word statistics for different paragraphs. In the hidden representation of this neural network, we get vectors representing each paragraph. These vectors have nice properties, in particular that similar paragraphs are close together."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p34", "contents": "Now, Google has some pretty awesome people. Andrew Dai, Quoc Le, and Greg Corrado decided to create paragraph vectors for some very interesting data sets. One of those was Wikipedia, creating a vector for every English Wikipedia article. I was lucky enough to be there at the time, and make some neat visualizations. (See Dai, et al. (2014))"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p35", "contents": "Since there are a very large number of Wikipedia articles, we visualize a random subset. Again, we use t-SNE, because we want to understand what is close together."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p36", "contents": "The result is that we get a visualization of the entirety of Wikipedia. A map of Wikipedia. A large fraction of Wikipedia\u2019s articles fall into a few broad topics: sports, music (songs and albums), films, species, and science. I wouldn\u2019t have guessed that! Why, for example, is sports so massive? Well, it seems like many individual athletes, teams, stadiums, seasons, tournaments and games end up with their own articles \u2013 that adds up to a lot of articles! Similar reasons lead to the large music, films and species clusters."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p38", "contents": "This map of Wikipedia presents important structure on multiple scales. While, there is a large cluster for sports, there are sub-clusters for individual sports like tennis. Films have a separate cluster for non-Western films, like bollywood. Even very fine grained topics, like human proteins, are separated out!"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p39", "contents": "Again, this is only scratching the surface. In the following interactive visualization, you can explore for your self. You can color points by their Wikipedia categories, or inspect individual points by hovering to see the article title. Clicking on a point will open the article."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p41", "contents": "(Note: Wikipedia categories can be quite unintuitive and much broader than you expect. For example, every human is included in the category applied ethics because humans are in people which is in personhood which is in issues in ethics which is in applied ethics.)"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p42", "contents": "The previous two examples have been, while fun, kind of strange. They were both produced by networks doing simple contrived tasks that we don\u2019t actually care about, with the goal of creating nice representations. The representations they produce are really cool and useful\u2026 But they don\u2019t do too much to validate our approach to understanding neural networks."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p43", "contents": "Let\u2019s look at a cutting edge network doing a real task: translating English to French."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p44", "contents": "Sutskever et al. (2014) translate English sentences into French sentences using two recurrent neural networks. The first consumes the English sentence, word by word, to produce a representation of it, and the second takes the representation of the English sentence and sequentially outputs translated words. The two are jointly trained, and use a multilayered Long Short Term Memory architecture.3"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p46", "contents": "We can look at the representation right after the English \u201cend of sentence\u201d (EOS) symbol to get a representation of the English sentence. This representation is actually quite a remarkable thing. Somehow, from an English sentence, we\u2019ve formed a vector that encodes the information we need to create a French version of that sentence."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p47", "contents": "Let\u2019s give this representation a closer look with t-SNE."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p49", "contents": "This visualization revealed something that was fairly surprising to us: the representation is dominated by the first word."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p50", "contents": "If you look carefully, there\u2019s a bit more structure than just that. In some places, we can see subclusters corresponding to the second word (for example, in the quotes cluster, we see subclusters for \u201cI\u201d and \u201cWe\u201d). In other places we can see sentences with similar first words mix together (eg. \u201cThis\u201d and \u201cThat\u201d). But by and large, the sentence representation is controlled by the first word."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p51", "contents": "There are a few reasons this might be the case. The first is that, at the point we grab this representation, the network is giving the first translated word, and so the representation may strongly emphasize the information it needs at that instant. It\u2019s also possible that the first word is much harder than the other words to translate because, for the other words, it is allowed to know what the previous word in the translation was and can kind of Markov chain along."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p52", "contents": "Still, while there are reasons for this to be the case, it was pretty surprising. I think there must be lots of cases like this, where a quick visualization would reveal surprising insights into the models we work with. But, because visualization is inconvenient, we don\u2019t end up seeing them."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p53", "contents": "There are a lot of established best practices for visualizing low dimensional data. Many of these are even taught in school. \u201cLabel your axes.\u201d \u201cPut units on the axes.\u201d And so on. These are excellent practices for visualizing and communicating low-dimensional data."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p54", "contents": "Unfortunately, they aren\u2019t as helpful when we visualize high-dimensional data. Label the axes of a t-SNE plot? The axes don\u2019t really have any meaning, nor are the units very meaningful. The only really meaningful thing, in a t-SNE plot, is which points are close together."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p55", "contents": "There are also some unusual challenges when doing t-SNE plots. Consider the following t-SNE visualization of word embeddings. Look at the cluster of male names on the left hand side\u2026"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p57", "contents": "\u2026 but you can\u2019t look at the cluster of male names on the left hand side. (It\u2019s frustrating not to be able to hover, isn\u2019t it?) While the points are in the exact same positions as in our earlier visualization, without the ability to look at which words correspond to points, this plot is essentially useless. At best, we can look at it and say that the data probably isn\u2019t random."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p58", "contents": "The problem is that in dimensionality reduced plots of high-dimensional data, position doesn\u2019t explain the data points. This is true even if you understand precisely what the plot you are looking at is."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p59", "contents": "Well, we can fix that. Let\u2019s add back in the tooltip. Now, by hovering over points you can see what word the correspond to. Why don\u2019t you look at the body part cluster?"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p61", "contents": "You are forgiven if you didn\u2019t have the patience to look at several hundred data points in order to find the body part cluster. And, unless you remembered where it was from before, that\u2019s the effort one would expect it to take you."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p62", "contents": "The ability to inspect points is not sufficient. When dealing with thousands of points, one needs a way to quickly get a high-level view of the data, and then drill in on the parts that are interesting."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p63", "contents": "This brings us to my personal theory of visualizing high dimensional data (based on my whole three months of working on visualizing it):"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p64", "contents": "Interactive visualizations are a really easy way to get both of these properties. But they aren\u2019t the only way. There\u2019s a really beautiful visualization of MNIST in the original t-SNE paper, Maaten & Hinton (2008), on the page labeled 2596:"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p66", "contents": "By directly embedding every MNIST digit\u2019s image in the visualization, Maaten and Hinton made it very easy to inspect individual points. Further, from the \u2018texture\u2019 of clusters, one can also quickly recognize their nature."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p67", "contents": "Unfortunately, that approach only works because MNIST images are small and simple. In their exciting paper on phrase representations, Cho et al. (2014) include some very small subsections of a t-SNE visualization of phrases:"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p69", "contents": "Unfortunately, embedding the phrases directly in the visualization just doesn\u2019t work. They\u2019re too large and clunky. Actually, I just don\u2019t see any good way to visualize this data without using interactive media."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p70", "contents": "Now that we\u2019ve looked at a bunch of exciting representations, let\u2019s return to our simple MNIST networks and examine the representations they form. We\u2019ll use PCA for dimensionality reduction now, since it will allow us to observe some interesting geometric properties of these representations, and because it is less stochastic than the other dimensionality reduction algorithms we\u2019ve discussed."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p71", "contents": "The following network has a 5 unit sigmoid layer. Such a network would never be used in practice, but is a bit fun to look at."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p72", "contents": "Then network\u2019s hidden representation looks like a projection of a high-dimensional cube. Why? Well, sigmoid units tend to give values close to 0 or 1, and less frequently anything in the middle. If you do that in a bunch of dimensions, you end up with concentration at the corners of a high-dimensional cube and, to a lesser extent, along its edges. PCA then projects this down into two dimensions."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p73", "contents": "This cube-like structure is a kind of geometric fingerprint of sigmoid layers. Do other activation functions have a similar geometric fingerprint? Let\u2019s look at a ReLU layer."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p74", "contents": "Because ReLU\u2019s have a high probability of being zero, lots of points concentrate on the origin, and along axes. Projected into two dimensions, it looks like a bunch of \u201cspouts\u201d shooting out from the origin."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p75", "contents": "These geometric properties are much more visible when there are only a few neurons."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p76", "contents": "Every time we train a neural net, we get new representations. This is true even if we train the same network multiple times. The result is that it is very easy to end up with lots of representations of a dataset."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p77", "contents": "We rarely look at any of these representations, but if we want to, it\u2019s pretty easy to make visualizations of all of them. Here\u2019s a bunch to look at."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p79", "contents": "Now, while we can visualize a lot of representations like this, it isn\u2019t terribly helpful. What do we learn from it? Not much. We have lots of particular representations, but it\u2019s hard to compare them or get a big picture view."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p80", "contents": "Let\u2019s focus on comparing representations for a moment. The tricky thing about this is that fundamentally similar neural networks can be very different in ways we don\u2019t care about. Two neurons might be switched. The representation could be rotated or flipped."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p82", "contents": "We want to, somehow, forget about these unimportant differences and focus only on the important differences. We want a canonical form for representations, that encodes only meaningful differences."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p83", "contents": "Distance seems fundamental, here. All of these unimportant differences are isometries \u2013 that is, transformations like rotation or switching two dimensions do not change the distances between points. On the other hand, distance between points is really important: things being close together is a representations way of saying that they are similar, and things being far apart is a representation saying they are different."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p84", "contents": "Thankfully, there\u2019s an easy way to forget about isometries. For a representation \\(X\\), there\u2019s an associated metric function, \\(d_X\\), which gives us the distance between pairs of points within that representation. For another representation \\(Y\\), \\(d_X = d_Y\\) if and only if \\(X\\) is isometric to \\(Y\\). The metric functions encode precisely the information we want!"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p85", "contents": "We can\u2019t really work with \\(d_X\\) because it is actually a function on a very high-dimensional continuous space.4 We need to discretize it for it to be useful."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p86", "contents": "\\[D_X = \\left[\\begin{array}{cccc} \n  d_X(x_0, x_0) & d_X(x_1, x_0) & d_X(x_2, x_0) & ... \\\\\n  d_X(x_0, x_1) & d_X(x_1, x_1) & d_X(x_2, x_1) & ... \\\\\n  d_X(x_0, x_2) & d_X(x_1, x_2) & d_X(x_2, x_2) & ... \\\\\n  ... & ... & ... & ... \\\\ \n\\end{array} \\right]\\]"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p87", "contents": "One thing we can do with \\(D_X\\) is flatten it to get a vector encoding the properties of the representation \\(X\\). We can do this for a lot of representations, and we get a collection of high-dimensional vectors."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p88", "contents": "The natural thing to do, of course, is to apply dimensionality reduction, such as t-SNE, to our representations. Geoff Hinton dubbed this use of t-SNE \u201cmeta-SNE\u201d. But one can also use other kinds of dimensionality reduction.5 6"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p89", "contents": "In the following visualization, there are three boxes. The largest one, on the left, visualizes the space of representations, with every point corresponding to a representation. The points are positioned by dimensionality reduction of the flattened distance matrices, as above. One way to think about this that distance between representations in the visualization represents how much they disagree on which points are similar and which points are different."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p90", "contents": "Next, the middle box is a regular visualization of a representation of MNIST, like the many we\u2019ve seen previously. It displays which ever representation you hover over in left box. Finally, the right most box displays particular MNIST digits, depending on which point you hover over in the middle box."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p92", "contents": "This visualization shifts us from looking at trees to seeing the forest. It moves us from looking at representations, to looking at the space of representations. It\u2019s a step up the ladder of abstraction."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p93", "contents": "Imagine training a neural network and watching its representations wander through this space. You can see how your representations compare to other \u201clandmark\u201d representations from past experiments. If your model\u2019s first layer representation is in the same place a really successful model\u2019s was during training, that\u2019s a good sign! If it\u2019s veering off towards a cluster you know had too high learning rates, you know you should lower it. This can give us qualitative feedback during neural network training."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p94", "contents": "It also allows us to ask whether two models which achieve comparable results are doing similar things internally or not."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p95", "contents": "All of the examples above visualize not only the neural network, but the data it operates on. This is because the network is inextricably tied to the data it operates on.7"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p96", "contents": "The visualizations are a bit like looking through a telescope. Just like a telescope transforms the sky into something we can see, the neural network transforms the data into a more accessible form. One learns about the telescope by observing how it magnifies the night sky, but the really remarkable thing is what one learns about the stars. Similarly, visualizing representations teaches us about neural networks, but it teaches us just as much, perhaps more, about the data itself."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p97", "contents": "(If the telescope is doing a good job, it fades from the consciousness of the person looking through it. But if there\u2019s a scratch on one of the telescope\u2019s lenses, the scratch is highly visible. If one has an example of a better telescope, the flaws in the worse one will suddenly stand out. Similarly, most of what we learn about neural networks from representations is in unexpected behavior, or by comparing representations.)"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p98", "contents": "Understanding data and understanding models that work on that data are intimately linked. In fact, I think that understanding your model has to imply understanding the data it works on. 8"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p99", "contents": "While the idea that we should try to visualize neural networks has existed in our community for a while, this converse idea \u2013 that we can use neural networks for visualization \u2013 seems equally important is almost entirely unexplored."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p100", "contents": "Let\u2019s explore it."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p101", "contents": "In his talk \u2018Media for Thinking the Unthinkable\u2019, Bret Victor raises a really beautiful quote from Richard Hamming:"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p102", "contents": "Just as there are odors that dogs can smell and we cannot, as well as sounds that dogs can hear and we cannot, so too there are wavelengths of light we cannot see and flavors we cannot taste."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p103", "contents": "Why then, given our brains wired the way they are, does the remark \u201cPerhaps there are thoughts we cannot think,\u201d surprise you?"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p104", "contents": "Evolution, so far, may possibly have blocked us from being able to think in some directions; there could be unthinkable thoughts."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p105", "contents": "\u00a0 \u00a0 - Richard Hamming, The Unreasonable Effectiveness of Mathematics"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p106", "contents": "Victor continues with his own thoughts:"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p107", "contents": "These sounds that we can\u2019t hear, this light that we can\u2019t see, how do we even know about these things in the first place? Well, we built tools. We built tools that adapt these things that are outside of our senses, to our human bodies, our human senses."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p108", "contents": "We can\u2019t hear ultrasonic sound, but you hook a microphone up to an oscilloscope and there it is. You\u2019re seeing that sound with your plain old monkey eyes. We can\u2019t see cells and we can\u2019t see galaxies, but we build microscopes and telescopes and these tools adapt the world to our human bodies, to our human senses."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p109", "contents": "When Hamming says there could be unthinkable thoughts, we have to take that as \u201cYes, but we build tools that adapt these unthinkable thoughts to the way that our minds work and allow us to think these thoughts that were previously unthinkable.\u201d"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p110", "contents": "\u00a0 \u00a0 - Bret Victor, Media for Thinking the Unthinkable"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p111", "contents": "This quote really resonates with me. As a machine learning researcher, my job is basically to struggle with data that is incomprehensible \u2013 literally impossible for the human mind to comprehend \u2013 and try to build tools to think about it and work with it.9"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p112", "contents": "However, from the representation perspective, there\u2019s a further natural step to go with this idea\u2026"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p113", "contents": "Lets consider human vision for a moment. Our ability to see is amazing. The amazing part isn\u2019t our eyes detecting photons, though. That\u2019s the easy, simple part. The amazing thing is the ability of our brain to transform the mess of swirling high-dimensional data into something we can understand. To present it to us so well that it seems simple! We can do this because our brains have highly specialized pathways for processing visual data."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p114", "contents": "Just as neural networks transform data from the original raw representations into nice representations, the brain transforms our senses from complicated high-dimensional data into nice representations, from the incomprehensible to the comprehensible. My eye detects photons, but before I even become consciously aware of what my eye sees, the data goes through incredibly sophisticated transformations, turning it into something I can reason about.10 The brain does such a good job that vision seems easy! It\u2019s only when you try to understand visual data without using your visual system that you realize how incredibly complicated and difficult to understand it is."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p115", "contents": "Unfortunately, for every sense we have, there are countless others we don\u2019t. Countless modes of experience lost to us. This is a tragedy. Imagine the senses we could have! There are vast collections of text out there: libraries, wikipedia, the Internet as a whole \u2013 imagine having a sense that allowed you to see a whole corpus at once, which parts are similar and which are different! Every collision at the Large Hadron Collider is monitored by a battery of different sensors \u2013 imagine having a sense that allowed us to \u2018see\u2019 collisions as clearly as we can see images! The barrier between us and these potential senses isn\u2019t getting the data, it\u2019s getting the data to our brain in a nice representation."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p116", "contents": "The easiest way to get new kinds of data into the brain is to simply project it into existing senses. In some very particular cases, this works really well. For example, microscopes and telescopes are extremely good at making a new kind of data accessible by projecting it into our normal visual sense. They work because macroscopic visual data and microscopic visual data are just visual data on different scales, with very similar structure to normal visual data, and are well handled by the same visual processing systems. Much more often, projecting data into an existing sense (for example, with PCA) throws away all but the crudest facets of the data. It\u2019s like taking an image and throwing away everything except the average color. It\u2019s something\u2026 but not much."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p117", "contents": "We can also try to get this data to us symbolically. Of course, rattling off 10,000-dimensional vectors to people is hopeless. But traditional statistics gives us some simple models we can fit, and then discuss using language of means, variance, covariance and so on. Unfortunately, fitting gaussians is like describing clouds as ovals. Talking about the covariance of two variables is like talking about the slope, in a particular direction, of a high-dimensional surface. Even very sophisticated models from statistics seem unable to cope with the complicated, swirling, high-dimensional data we see in problems like vision."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p118", "contents": "Deep learning gives us models that can work with this data. More that that, it gives us new representations of the data. The representations it produces aren\u2019t optimized to be nice representations for the human brain \u2013 I have no idea how one would optimize for that, or even what it would mean \u2013 but they are much nicer than the original data. I think that learning representations, with deep learning or other powerful models, is essential to helping humans understand new forms of data."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p119", "contents": "The best example I can give is the visualization of Wikipedia from earlier. Wikipedia is a repository of human knowledge. By combining deep learning and dimensionality reduction, we can make a map of it, as we saw earlier:"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p121", "contents": "This style of visualization feels important to me. Using deep learning, we\u2019ve made a visualization, an interface, for humans to interact with Wikipedia as a whole. I\u2019m not claiming that it\u2019s a great interface. I\u2019m not even sure I think it is terribly useful. But it\u2019s a starting point and a proof of concept."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p122", "contents": "Why not just use dimensionality reduction by itself? If we had just used dimensionality reduction, we would be visualizing geometric or topological features of the Wikipedia data. Using deep learning to transform the data allows us to visualize the underlying structure, the important variations \u2013 in some cases, the very meaning of the data11 \u2013 instead."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p123", "contents": "I think that high-quality representations have a lot of potential for users interacting with complicated data, going far beyond what is explored here. The most natural direction is machine learning: once you are in a high-quality representation, many normally difficult tasks can be accomplished with very simple techniques and comparatively little data.12 With a curated collection of representations, one could make some really exciting machine learning accessible,13 although it would carry with it challenges for end users14 and the producers of representations.15"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p124", "contents": "Reasoning about data through representations can be useful even for kinds of data the human mind understands really well, because it can make explicit and quantifiable things that are normally tacit and subjective."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p125", "contents": "You probably understand English very well, but much of this knowledge is subjective. The meaning of words is socially constructed, arising from what people mean by them and how they use them. It\u2019s canonicalized in dictionaries, but only to a limited extent. But the subtleties in usage and meaning are very interesting because of how they reflect culture and society. Unfortunately, these things are kind of fuzzy, and one typically needs to rely on anecdotes and personal impressions."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p126", "contents": "One remarkable property of high-quality word embeddings is that they seem to reify these fuzzy properties into concrete mathematical structures! As mentioned earlier, directions in word embeddings correspond to particular kinds of differences in meaning. For example, there is some direction corresponding to gender. (For more details, see my post Deep Learning, NLP, and Representations.)"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p127", "contents": "By taking the difference of two word vectors, we can get directions for gender. For example, we can get a masculine direction (eg. \u201cman\u201d - \u201cwoman\u201d) or a feminine direction (eg. \u201cwoman\u201d - \u201cman\u201d). We can also get age directions. For example, we can get an adult direction (eg. \u201cwoman\u201d - \u201cgirl\u201d) or a child direction (eg. \u201cboy\u201d - \u201cman\u201d)."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p128", "contents": "Once we have these directions, there\u2019s a very natural question to ask: which words are furthest in these directions? What are the most masculine or feminine words? The most adult, the most childish? Well, let\u2019s look at the Wikipedia GloVe vectors, from Pennington, et al. at Stanford:"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p129", "contents": "Of course, these results depend on a lot of details. 16"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p130", "contents": "I\u2019d like to emphasize that which words are feminine or masculine, young or adult, isn\u2019t intrinsic. It\u2019s a reflection of our culture, through our use of language in a cultural artifact. What this might say about our culture is beyond the scope of this essay. My hope is that this trick, and machine learning more broadly, might be a useful tool in sociology, and especially subjects like gender, race, and disability studies."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p131", "contents": "Right now machine learning research is mostly about getting computers to be able to understand data that humans do: images, sounds, text, and so on. But the focus is going to shift to getting computers to understand things that humans don\u2019t. We can either figure out how to use this as a bridge to allow humans to understand these things, or we can surrender entire modalities \u2013 as rich, perhaps more rich, than vision or sound \u2013 to be the sole domain of computers. I think user interface could be the difference between powerful machine learning tools \u2013 artificial intelligence \u2013 being a black box or a cognitive tool that extends the human mind."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p132", "contents": "There\u2019s actually two kinds of black boxes we need to avoid. Two slightly different, but closely connected problems. The first problem is that deep learning itself is presently a kind of black box. The second is that tools using deep learning to solve particular problems might be black boxes."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p133", "contents": "We need to figure out how to open the deep learning black box. One powerful approach is visualizing representations. In this essay, we used interactive media to visualize and explore some powerful models from Google\u2019s deep learning research group. We then observed that particular neural network architectures leave geometric signatures in their representations. Finally, we created the meta-SNE algorithm, in order to step up the ladder of abstraction, and think about the space of neural networks, instead of particular ones."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p134", "contents": "The problem of particular tools being black boxes is, in some ways, harder, because there\u2019s so much diversity. But a common problem is that humans can\u2019t think about the sort of high-dimensional structures machine learning problems typically involve. We observed that visualizing representations can also be a tool to help humans understand and reason about these structures. We saw that representations can be helpful even for data we understand really well. "}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p135", "contents": "These are problems we\u2019re only beginning to attack. I think there\u2019s a lot more for us to uncover here. It\u2019s an odd kind of work, at the intersection of machine learning, mathematics, and user interface. But I think it\u2019s important."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p136", "contents": "If you enjoyed this post, consider subscribing to my rss feed."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p137", "contents": "(I would be delighted to hear your comments and thoughts: you can comment inline or at the end. For typos, technical errors, or clarifications you would like to see added, you are encouraged to make a pull request on github.)"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p138", "contents": "I\u2019m grateful for the hospitality of Google\u2019s deep learning research group, which had me as an intern while I did most of the work this post is based on. I\u2019m especially grateful to my internship host, Jeff Dean."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p139", "contents": "I was greatly helped by the comments, advice, and encouragement of many Googlers, both in the deep learning group and outside of it. These include: Greg Corrado, Jon Shlens, Matthieu Devin, Andrew Dai, Quoc Le, Anelia Angelova, Oriol Vinyals, Ilya Sutskever, Ian Goodfellow, Jutta Degener, and Anna Goldie."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p140", "contents": "I was strongly influenced by the thoughts, comments and notes of Michael Nielsen, especially his notes on Bret Victor\u2019s work. Michael\u2019s thoughts persuaded me that I should think seriously about interactive visualizations for understanding deep learning. The section \u201cUnthinkable Thoughts, Incomprehensible Data\u201d was particularily influenced by him."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p141", "contents": "I was also helped by the support and comments of a number of other non-Googler friends, including Yoshua Bengio, Laura Ball, Rob Gilson, Henry de Valence, Yomna Nasser, and James Koppel."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p142", "contents": "This blog post was made possible by a number of wonderful Javascript libraries, including D3.js, MathJax, and jQuery. A big thank you to everyone who contributed to these libraries."}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p143", "contents": "The representation perspective is an abstraction over inputs. Instead of trying to understand what the neural network does to a single input, we try to understand what it does to the space of inputs, to the data manifold. It\u2019s a step up the ladder of abstraction. Later, we will take a second step, allowing us to look at the space of neural networks, instead of a single one.\u21a9"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p144", "contents": "We categorize words using WordNet synsets. Each synset is labeled something like \u201cregion.n.03\u201d (region, noun, meaning 3) or \u201ctravel.v.01\u201d (travel, verb, meaning 1).\u21a9"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p145", "contents": "It should be noted that, later, Sutskever et al. switched to reversing the order of the input sentence, finding this improved their results.   \u21a9"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p146", "contents": "The natural way to think about distance between functions is to consider them as infinite dimensional vectors \\((f(0), ~ f(1),~ f(2)...)\\). In the case of a function on the real numbers or on \\(\\mathbb{R}^n\\), it\u2019s a \\(2^{\\aleph_0}\\) dimensional vector! While we can actually represent the function finitely (because we know it\u2019s based on a neural network, which has a finite number of paramaters) it\u2019s really hard to actually calculate distances.\u21a9"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p147", "contents": "I\u2019ve heard that some similar techniques may be used in neuroscience, where one often needs to compare different representations of the same data. Further, in a previous post, John MacCuish commented that one could use the Mantel Test on the distance matrices to compare representations \u2013 this gets at a very similar idea!\u21a9"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p148", "contents": "There are some variations you can do on the basic meta-SNE algorithm. For example, meta-SNE analyzes how much representations agree on which data points are similar, by comparing distance between the points in different representations. But we can also compare how much representations agree on analogies, on the manner in which things are different, by comparing the distance between differences of vectors in different representations. In principle, this information is encoded in the distances between data points, but one can make it much more explicit. It may also be the case that we care more about which networks are very similar, in which case we could apply some non-linearity pointwise to the distance matrix, to exaggerate the difference between close and not-close data points.\u21a9"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p149", "contents": "The neural network is a function with the domain of the data manifold it was trained on.\u21a9"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p150", "contents": "People sometimes complain: \u201cNeural networks are so hard to understand! Why can\u2019t we use understandable models, like SVMs?\u201d Well, you understand SVMs, and you don\u2019t understand visual pattern recognition. If SVMs could solve visual pattern recognition, you would understand it. Therefore, SVMs are not capable of this, nor is any other model you can really understand. (I don\u2019t mean this to be a \u2018proof\u2019 obviously, but I am pretty serious about this view.)\u21a9"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p151", "contents": "You could imagine defining a field this way, as attempting to build tools for thinking about and working with the complicated high-dimensional probability distributions we see in the real world. The field you get isn\u2019t quite machine learning, but it has a lot of overlap. It actually feels more compelling to me. Perhaps this is \u201cdata science\u201d?\u21a9"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p152", "contents": "As someone without a neuroscience background, I feel a bit nervous making remarks like this. That said, I think what I\u2019m mostly saying is an interpretation, an abstraction, over some fairly basic facts about how human vision works. I also know that at least some neuroscientists subscribe to this interpretation and seriously look at things through this sort of lens. For example, see DiCarlo and Cox\u2019s paper \u2018Untangling invariant object recognition\u2019.\u21a9"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p153", "contents": "This is a very bold claim. My defense is this: word embedding models seem to encode semantic meaning in directions, creating a \u201csemantic vector space.\u201d Paragraph vectors (at least the kind that we\u2019re using) do the same. Somehow, these models seem to discover human meaning while learning the structure of the space. The results of the DeViSE paper suggest that this may be somewhat general in good high-level representations.\u21a9"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p154", "contents": "This is some combination of transfer learning, pretraining, and multi-task learning. How well it works varies, but there\u2019s certainly a lot of successes. Obviously, the ideal is to have a lot of data to train a representation specifically for your task. But failing that, we can also try to make very transferable representations, possibly by training them for a bunch of different tasks.\u21a9"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p155", "contents": "Curating large collections of structured data has lead to some really interesting tools (for example, Wolfram Alpha). My intuition is that curating a collection of high-quality representations for different kinds of data could also be really interesting. I think MetaMind is the closest thing I know of to this, right now.\u21a9"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p156", "contents": "It seems like a lot of the problems that exist with proprietary file formats could end up happening here. An end user could very easily end up tied to a particular representation. Do we need open or standardized representations?\u21a9"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p157", "contents": "The problem with just releasing representations, from the perspective of the model producer, is what Geoff Hinton calls \u201cDark Knowledge\u201d. Representations subtly encode a lot of the knowledge of your model. By releasing representations, organizations are implicitly releasing a significant amount of information about their model.\u21a9"}
{"id": "https://colah.github.io/posts/2015-01-Visualizing-Representations/_p158", "contents": "The answer depends a lot on the corpus you train on. If you train your word embedding on a news corpus, that will be different than if you train it on Wikipedia. And I assume if you trained on a corpus of 19th century literature, that would be very different again. It also depends on your model, and how well you trained it. The precise interpretation is obviously sensitive to the model. But, generally, it will be something like this: There is a certain difference in how language is used around the word \u201cman\u201d and the word \u201cwoman\u201d; which words cause language around them to change most in that manner? (Replace \u201cman\u201d and \u201cwoman\u201d for whatever you want.)\u21a9"}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p0", "contents": "Posted on December  8, 2014"}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p1", "contents": "group theory, probability, convolution, math"}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p2", "contents": "Consider a square. Is it symmetric? How is it symmetric? How much symmetry does it have? What kind of symmetry does it have?"}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p3", "contents": "What do those questions even mean?"}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p4", "contents": "If you ask someone, they might tell you that a square has rotational symmetry. If you rotate a square by 90\u00b0, it\u2019s the same shape. Without knowing which corner was which, it would seem the exact same as it was before. You could lift it up, rotate it, and set it back down so that it covers the exact same space."}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p5", "contents": "Let\u2019s call this rotation transformation \\(r\\). To be precise, \\(r\\) rotates a square clockwise by 90\u00b0. For example, \\(r\\sq{e} = \\sq{r}\\). (The \u201cF\u201d on the square is there to let us determine orientation and see transformations.)"}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p6", "contents": "You might also be told that a square has horizontal symmetry or vertical symmetry. You can flip a square horizontally or vertically and still have a square. Let\u2019s focus on horizontal symmetry for now. We\u2019ll call horizontal flips \\(s\\). \\(s\\) performs a reflection across a vertical line through the middle of the square. For example, \\(s\\sq{e} = \\sq{s}\\)."}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p7", "contents": "We now have two transformations, \\(r\\) and \\(s\\), which transform squares into another square of the same shape. It turns out that these two transformations form a kind of \u201cbasis\u201d for all the others. By using them in some pattern, you can build the other transformations, like vertical flipping."}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p8", "contents": "Starting with our original square \\(\\sq{e}\\) in the bottom left corner, the following graph shows the transformed versions generated by combining \\(r\\) and \\(s\\) in different ways. \\(r\\) and \\(s\\) are represented by arrows of different colors. \\(r\\) arrows are colored blue and \\(s\\) arrows are colored red."}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p9", "contents": "We can use the graph to investigate what happens if we perform a sequence of transformations. For example, what happens if we rotate, flip and then rotate again? Well, we start at our original square, \\(\\sq{e}\\), and trace: \\(\\sq{e} \\xrightarrow{r} \\sq{r} \\xrightarrow{s} \\sq{r3s} \\xrightarrow{r} \\sq{s}\\). In the end, we\u2019re left with just horizontally flipped version of the original, \\(s\\sq{e} = \\sq{s}\\). If we want to express this surprising fact, we can use multiplication like notation: \\(rsr \\sq{e} = s \\sq{e}\\)."}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p10", "contents": "If we want to think about our graph a bit more abstractly, we can represent all the squares as the original square transformed by \\(r\\) and \\(s\\). For example, \\(\\sq{r2s} = r^2s\\sq{e}\\)."}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p11", "contents": "Here, \\(e\\) is the identity transformation, which doesn\u2019t transform the object at all. For example \\(e\\sq{e} = \\sq{e}\\). (Why have \\(e\\), if it doesn\u2019t do anything? It\u2019s a lot like having the number zero.)"}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p12", "contents": "We can go a bit further. The original square, \\(\\sq{e}\\), seems a bit unnecessary in \\(rsr \\sq{e} = s \\sq{e}\\). Why not just say \\(rsr = s\\)? We can just drop the factored out \\(\\sq{e}\\), both in equations and our graph."}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p13", "contents": "Now, here\u2019s the essential realization: \\(r\\) and \\(s\\) could have been other things and we would have had the exact same graph. \\(r\\) could have been rotating 90\u00b0 counterclockwise. \\(s\\) could have been vertical flips. Or we could have been transforming an entirely different kind of object. All that matters is the relationship between \\(r\\) and \\(s\\), how they interact. What we saw with the squares was just one particular way this graph, this abstract pattern, could appear in the real world."}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p14", "contents": "Mathematicians call these abstract patterns groups. There is an entire field of math dedicated to them. Connections between a group and an object like the square are called group actions."}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p15", "contents": "Not all graphs are groups. Only a very special kind of graph is. (We won\u2019t give a formal definition here, but we will get a good feel for it.)"}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p16", "contents": "Firstly, the graph is directed (the edges are arrows) and has colored edges. At every vertex, exactly one arrow of a given color comes out and one goes in."}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p17", "contents": "But the key property of these graphs is more subtle. We created our graph by starting with an original square, \\(\\sq{e}\\). But what if we said the original square was \\(\\sq{s} = s\\sq{e}\\)?"}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p18", "contents": "Which position we say is the \u201cinitial\u201d position is arbitrary. No matter which position you think of as the initial one, the graph is the same. The graph is perfectly symmetrical, in some sense.1 Imagine that the edges are paths of different color you can walk on, and you\u2019re standing on one of the nodes: from your perspective the graph is the same no matter which node you\u2019re standing on. No matter which node you\u2019re on, taking a red path, a blue path, and then a red path and then a blue path again will bring you back to where you started."}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p19", "contents": "In Euclidean space, we reason about points by their relative position to an origin. Similarly, in our group, we pick some origin (eg. \\(\\sq{e}\\)) and talk about points by their relative positions. We call these relative positions (such as \\(r\\), \\(s\\), or \\(r^3s\\)), the elements of the group."}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p20", "contents": "Just like we can add difference vectors of points, we can \u201cadd\u201d elements of a group together. It isn\u2019t actually addition, of course, but it is a natural way to combine elements of the group. Sometimes we talk about it by analogy with addition and write combining two elements \\(a\\) and \\(b\\) as \\(a+b\\), while other times we make analogies to multiplication and write \\(a\\cdot b\\)."}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p21", "contents": "\u201cAdding\u201d or \u201cmultiplying\u201d two group elements is actually quite similar to vector addition. We decide that one point on the graph is our identity element (the original position), and find the two elements we want to multiply, \\(a\\) and \\(b\\). We pick paths from the identity to \\(a\\) and \\(b\\). Then we stick the \\(a\\) path on to the end of \\(b\\), to bring us to \\(a+b\\) or \\(a\\cdot b\\) (depending on the chosen notation)."}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p22", "contents": "(This section is optional.)"}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p23", "contents": "The above is almost unrecognizable as group theory, from a traditional perspective. Usually, we think of groups as a kind of abstraction."}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p24", "contents": "There are lots of kinds of mathematical objects and, as you look at more of them, one beings to see patterns. For example, in arithmetic, we see \\(a\\!\\cdot\\!(b+c) ~=~ a\\!\\cdot\\! b ~+~ a\\!\\cdot\\! c\\) and in set theory we see \\(A\\cap (B \\cup C) = A\\cap B ~\\cup~ A\\cap C\\). There are many other examples of this pattern, and many other patterns."}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p25", "contents": "One also notices that many important results are true for a broad class of objects, and they\u2019re all true for the same reason. They\u2019re true because all the objects observe a particular pattern. Knowing that a mathematical object obeys that pattern is sufficient to prove the result holds."}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p26", "contents": "So, we formalize those patterns into what we call mathematical structures.2 There\u2019s a lot of them, and you can find a very long list of algebraic ones on wikipedia. We can study a mathematical structure and prove results that hold for any instance of that structure. (Programmers and computer scientists can see this as making mathematics polymorphic.3)"}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p27", "contents": "We can now give the classical definition of a group. Don\u2019t worry too much if you have trouble following."}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p28", "contents": "Definition: A group \\(G = (S, ~\\cdot~)\\) is a set \\(S\\) equipped with a binary operation \\((~\\cdot~)\\), a function mapping pairs of group elements to group elements, with the following properties:"}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p29", "contents": "Why those rules? Why not more or less? Well, we could define a group to have more or less requirements. If it was weaker, had less requirements, more kinds of objects would be groups and the results we prove about groups would be more broadly applicable. If it was stronger, had more requirements, we would be talking about a more specific kind of object and could prove more about them. In mathematics one often balances generality and specificity like this."}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p30", "contents": "Mathematicians study both weaker and stronger versions of groups. But, somehow, groups are special. They aren\u2019t too hot, they aren\u2019t too cold: they\u2019re just right."}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p31", "contents": "This might seem kind of arbitrary. Why should these particular rules be a particularly good collection? One thing that I find very helpful and motivating is realizing that they\u2019re equivalent to the requirements we made when we were thinking of groups as graphs. Identity corresponds to there being a starting point, inverses to being able to go backwards on arrows, and associativity is equivalent to the perfect symmetry of the graph.4"}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p32", "contents": "Consider three cards, \\(\\cards{123}\\). There are some transformations that are natural to apply to them. We\u2019ll call the operation of switching the first two cards \\((12)\\). Similarly, we\u2019ll call the operation of switching the second cards \\((23)\\). So,"}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p33", "contents": "\\[(12)\\cards{123} = \\cards{213} ~~~~~~~~~~~~~~~~~~~~~~~~ (23)\\cards{123} = \\cards{132}\\]"}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p34", "contents": "Together, these two operations generate a group, the Symmetric Group on 3 symbols, \\(S_3\\)."}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p35", "contents": "Each group element is a particular way to rearrange the cards, a permutation."}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p36", "contents": "One interesting thing to think about is shuffling. When we shuffle cards, we try to put them in a random ordering, a random permutation. This means we create a probability distribution over the group."}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p37", "contents": "Ideally, our shuffle would give us a uniform distribution \u2013 every permutation would be equally likely. But we can easily imagine an imperfect shuffle, where some permutations are more likely than others."}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p38", "contents": "Of course, if the first shuffle doesn\u2019t randomize them, we can shuffle again!"}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p39", "contents": "Generally, repeated shuffles will cause probability mass to diffuse, bringing us closer to the uniform distribution.5"}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p40", "contents": "This should feel similar to the falling ball example in the Understanding Convolutions post. Fundamentally, they are the same thing: convolution."}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p41", "contents": "The earlier visualizations of probability distributions on the permutations were kind of messy. The natural way to visualize it is on the Cayley diagram!"}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p42", "contents": "Let\u2019s consider a very simple probability distribution. 40% of the time we apply the operation \\((12)\\), permuting our cards to \\(\\cards{213}\\). 60% of the time we apply \\((23)\\), permuting our cards to \\(\\cards{132}\\). That\u2019s a terrible shuffle, but it is easy to think about."}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p43", "contents": "To be a bit more explicit, let\u2019s picture us as starting with all the probability density on the unpermuted cards \\(\\cards{123}\\) (i.e.\u00a0the identity), and then we apply our very silly shuffle."}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p44", "contents": "When we shuffle, we sample this distribution, getting some permutation \\(a\\) with probability \\(f(a)\\)."}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p45", "contents": "What happens when we shuffle a second time?"}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p46", "contents": "Well, the first time we shuffled, we got a permutation \\(a\\) with probability \\(f(a)\\). The second time we shuffle, we will get another permutation \\(b\\) with probability \\(g(b)\\). These two actions happen with probability \\(f(a)g(b)\\) and result is a permutation \\(c = b\\cdot a\\)."}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p47", "contents": "To get the actual probability of \\(c\\), though, it is not sufficient to just look at one pair of permutations that bring us to \\(c\\). Instead, we need to sum over all possible pairs of permutations. This is the convolution of \\(g\\) and \\(f\\) (like in function composition, the right side goes first)."}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p48", "contents": "\\[(g\\ast f)(c) = \\sum_{b \\cdot a = c} g(b)f(a)\\]"}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p49", "contents": "Substituting \\(b = ca^{-1}\\), we get:"}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p50", "contents": "\\[(g\\ast f)(c) = \\sum_{a} g(ca^{-1})f(a)\\]"}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p51", "contents": "This can be nicely thought of as a sum over the intermediate permutations, \\(a\\), looking at the probability of that intermediate permutation, and the probability of the permutation necessary to bring us to \\(c\\) from there."}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p52", "contents": "Alternatively, we can substitute \\(a = b^{-1}c\\) to get:"}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p53", "contents": "\\[(g\\ast f)(c) = \\sum_{b} g(b)f(b^{-1}c)\\]"}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p54", "contents": "The traditional definition of group convolution. (If you let the group operation be addition, this is the normal definition of convolution.)"}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p55", "contents": "(This section is optional and assumes a stronger background than the rest of the article. Less mathematically inclined readers might wish to skip this section.)"}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p56", "contents": "The traditional definition of convolution requires that you be able to take inverses, and multiply every element by every other element. This means you need to be working on a group, or perhaps a quasigroup."}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p57", "contents": "But if you switch to the definition \\((g\\ast f)(c) = \\sum_{b \\cdot a = c} g(b)f(a)\\), which seems much more natural, convolution makes sense on just about any algebraic structure with a binary operator. Certainly, you can talk about convolutions on monoids, groupoids, and categories. As far as I can tell, no one\u2019s really considered these.6"}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p58", "contents": "One cute thing about this is that convolution often inherits the algebraic properties of the domains of the functions being convolved. For example, if you convolve functions on associative domains, the convolution operation is associative:"}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p59", "contents": "\\[((A\\ast B) \\ast C)(x) = \\sum_{a \\cdot b \\cdot c = x} A(a)B(b)C(c) = (A\\ast (B \\ast C))(x)\\]"}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p60", "contents": "Similarly, if the domain is commutative, so is convolution. And if it has identity, so does convolution. Sadly, convolution doesn\u2019t get inverses if the domain has inverses, so the parallel breaks down at Abelian monoids."}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p61", "contents": "With the math working out so nicely, you might wonder if there\u2019s any reason one might actually use these. Well, convolution on monoids seems natural in cases where you \u201ccan\u2019t go backwards\u201d. And convolution on categories allows for a kind of state. In fact, I think you could very naturally describe probabilistic automaton in terms of category convolutions."}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p62", "contents": "This essay takes an unusual perspective on group theory. Cayley diagrams have been around for a long time, but, as far as I know, taking them seriously as an approach to group theory, as a kind of foundation, is a recent idea, engineered by Nathan Carter in his book Visual Group Theory. Interested readers are encouraged to look at his book."}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p63", "contents": "Group convolutions provide elegant language for talking about lots of situations involving probability. But, since this is a series of blog posts on convolutional neural networks, you may suspect that I have other interests in them. Well, you guessed correctly. Group convolutions naturally extend convolutional neural networks, with everything fitting together extremely nicely. Since convolutional neural networks are one of the most powerful tools in machine learning right now, that\u2019s pretty interesting. In our next post, we will explore these networks."}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p64", "contents": "This post is part of a series on convolutional neural networks and their generalizations. The first two posts will be review for those familiar with deep learning, while later ones should be of interest to everyone. To get updates, subscribe to my RSS feed!"}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p65", "contents": "Please comment below or on the side. Pull requests can be made on github."}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p66", "contents": "I\u2019m grateful to Yomna Nasser, Harry de Valence, Sam Eisenstat, and Sebastian Zany for taking the time to read and comment on draft version of this post \u2013 their feedback improved it a lot!"}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p67", "contents": "I\u2019m also grateful to Guillaume Alain, Eliana Lorch, Dario Amodei, Aaron Courville, Yoshua Bengio, and Michael Nielsen for discussion of group convolution and its potential applications to neural networks."}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p68", "contents": "Note that the graph embedding isn\u2019t necessarily symmetrical.\u21a9"}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p69", "contents": "Usually people talk about algebraic structures, abstract mathematical structures from algebra. There are similar abstract mathematical structures in other areas, particularly in analysis. For example: metric spaces, topological spaces and measure spaces. However, these are rarely lumped together in the way that algebraic structures are.\u21a9"}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p70", "contents": "This is actually a very deep analogy. In programming, we often try to write polymorphic functions that can act on many kinds of objects. In mathematics, we\u2019re trying to make polymorphic proofs that can operate on different kinds of mathematical object. The Curry\u2013Howard correspondence formalizes this connection between programs and proofs."}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p71", "contents": "(Some programming languages, like Haskell, even have implementations of common algebraic structures as classes!)"}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p72", "contents": "It\u2019s also worth noting that, just as most approaches to polymorphism in programming give us subclasses and superclasses, algebraic structures also kind of have \u201csub-structures\u201d and \u201csuper-structures\u201d.\u21a9"}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p73", "contents": "The associativity part is a bit tricky to see, especially because we never rigorously defined the \u201cperfect symmetry\u201d of our \u201cgroup graphs.\u201d"}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p74", "contents": "One definition is that, given a loop originating at \\(e\\) on the graph, \\(((bc)d)... = e\\), that same sequence is also a loop if it starts at a point \\(a\\), that is \\((((ab)c)d)... = a\\). It\u2019s pretty straightforward to see that this follows from associativity, but what about the other direction?"}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p75", "contents": "Well, we want to prove for all \\(a,b,c\\), that, \\(a(bc) = (ab)c\\). Let \\(d = (bc)^{-1}\\), the reverse of the path to \\(bc\\). Then \\((bc)d = e\\) is a loop. By the graph symmetry, \\(((ab)c)d = a\\). We now right-mulitply by \\(d^{-1} = (bc)\\) to get \\((ab)c = a(bc)\\), which is associativity.\u21a9"}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p76", "contents": "How many times do you have to shuffle a deck of cards to make it truly random? This question was explored by the mathematician Persi Diaconis.\u21a9"}
{"id": "https://colah.github.io/posts/2014-12-Groups-Convolution/_p77", "contents": "I can\u2019t really find instances of people talking about these convolutions as independent things, but the operation seems to be implicitly constructed in objects built to study these structures. Just as multiplication in group rings is group convolution, multiplication in monoid rings is monoid convolution, multiplication in groupoid algebras is groupoid convolution, and multiplication in categorical algebras is category convolution.\u21a9"}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p0", "contents": "Posted on October  9, 2014"}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p1", "contents": "MNIST, data visualization, machine learning, word embeddings, neural networks, deep learning"}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p2", "contents": "At some fundamental level, no one understands machine learning."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p3", "contents": "It isn\u2019t a matter of things being too complicated. Almost everything we do is fundamentally very simple. Unfortunately, an innate human handicap interferes with us understanding these simple things."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p4", "contents": "Humans evolved to reason fluidly about two and three dimensions. With some effort, we may think in four dimensions. Machine learning often demands we work with thousands of dimensions \u2013 or tens of thousands, or millions! Even very simple things become hard to understand when you do them in very high numbers of dimensions."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p5", "contents": "Reasoning directly about these high dimensional spaces is just short of hopeless."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p6", "contents": "As is often the case when humans can\u2019t directly do something, we\u2019ve built tools to help us. There is an entire, well-developed field, called dimensionality reduction, which explores techniques for translating high-dimensional data into lower dimensional data. Much work has also been done on the closely related subject of visualizing high dimensional data."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p7", "contents": "These techniques are the basic building blocks we will need if we wish to visualize machine learning, and deep learning specifically. My hope is that, through visualization and observing more directly what is actually happening, we can understand neural networks in a much deeper and more direct way."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p8", "contents": "And so, the first thing on our agenda is to familiarize ourselves with dimensionality reduction. To do that, we\u2019re going to need a dataset to test these techniques on."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p9", "contents": "MNIST is a simple computer vision dataset. It consists of 28x28 pixel images of handwritten digits, such as:"}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p11", "contents": "Every MNIST data point, every image, can be thought of as an array of numbers describing how dark each pixel is. For example, we might think of \\(\\mnist[1]{1}\\) as something like:"}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p13", "contents": "Since each image has 28 by 28 pixels, we get a 28x28 array. We can flatten each array into a \\(28*28 = 784\\) dimensional vector. Each component of the vector is a value between zero and one describing the intensity of the pixel. Thus, we generally think of MNIST as being a collection of 784-dimensional vectors."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p14", "contents": "Not all vectors in this 784-dimensional space are MNIST digits. Typical points in this space are very different! To get a sense of what a typical point looks like, we can randomly pick a few points and examine them. In a random point \u2013 a random 28x28 image \u2013 each pixel is randomly black, white or some shade of gray. The result is that random points look like noise."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p16", "contents": "Images like MNIST digits are very rare. While the MNIST data points are embedded in 784-dimensional space, they live in a very small subspace. With some slightly harder arguments, we can see that they occupy a lower dimensional subspace."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p17", "contents": "People have lots of theories about what sort of lower dimensional structure MNIST, and similar data, have. One popular theory among machine learning researchers is the manifold hypothesis: MNIST is a low dimensional manifold, sweeping and curving through its high-dimensional embedding space. Another hypothesis, more associated with topological data analysis, is that data like MNIST consists of blobs with tentacle-like protrusions sticking out into the surrounding space."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p18", "contents": "But no one really knows, so lets explore!"}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p19", "contents": "We can think of the MNIST data points as points suspended in a 784-dimensional cube. Each dimension of the cube corresponds to a particular pixel. The data points range from zero to one according to the pixels intensity. On one side of the dimension, there are images where that pixel is white. On the other side of the dimension, there are images where it is black. In between, there are images where it is gray."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p21", "contents": "If we think of it this way, a natural question occurs. What does the cube look like if we look at a particular two-dimensional face? Like staring into a snow-globe, we see the data points projected into two dimensions, with one dimension corresponding to the intensity of a particular pixel, and the other corresponding to the intensity of a second pixel. Examining this allows us to explore MNIST in a very raw way."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p22", "contents": " In this visualization, each dot is an MNIST data point. The dots are colored based on which class of digit the data point belongs to. When your mouse hovers over a dot, the image for that data point is displayed on each axis. Each axis corresponds to the intensity of a particular pixel, as labeled and visualized as a blue dot in the small image beside it. By clicking on the image, you can change which pixel is displayed on that axis."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p23", "contents": "Exploring this visualization, we can see some glimpses of the structure of MNIST. Looking at the pixels \\(p_{18,16}\\) and \\(p_{7,12}\\), we are able to separate a lot of zeros to the bottom right and a lot of nines to the top left. Looking at pixels \\(p_{5,6}\\) and \\(p_{7,9}\\) we can see a lot of twos at the top right and threes at the bottom right."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p24", "contents": "Despite minor successes like these, one can\u2019t really can\u2019t understand MNIST this way. The small insights one gains feel very fragile and feel a lot like luck. The truth is, simply, that very little of MNIST\u2019s structure is visible from these perspectives. You can\u2019t understand images by looking at just two pixels at a time."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p25", "contents": "But there\u2019s lots of other perspectives we could look at MNIST from! In these perspectives, instead of looking a face straight on, one looks at it from an angle."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p26", "contents": "The challenge is that we need to choose what perspective we want to use. What angle do we want to look at it from horizontally? What angle do we want to look at it from vertically? Thankfully, there\u2019s a technique called Principal Components Analysis (PCA) that will find the best possible angle for us. By this, we mean that PCA will find the angle that spreads out the points the most (captures the most variance possible)."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p27", "contents": "But, what does it even mean to look at a 784-dimensional cube from an angle? Well, we need to decide which direction every axis of the cube should be tilted: to one side, to the other, or somewhere in between?"}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p28", "contents": "To be concrete, the following are pictures of the two angles PCA chooses. Red represents tilting a pixel\u2019s dimension to one side, blue to the other."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p30", "contents": "If an MNIST digit primarily highlights red, it ends up on one side. If it highlights blue, it ends up on a different side. The first angle \u2013 the \u201cfirst principal component\u201d \u2013 will be our horizontal angle, pushing ones (which highlight lots of red and little blue) to the left and zeros (which highlight lots of blue and little red) to the right."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p32", "contents": "Now that we know what the best horizontal and vertical angle are, we can try to look at the cube from that perspective."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p34", "contents": "This visualization is much like the one above, but now the axes are fixed to displaying the first and second \u2018principal components,\u2019 basically angles of looking at the data. In the image on each axis, blue and red are used to denote what the \u2018tilt\u2019 is for that pixel. Pixel intensity in blue regions pushes a data point to one side, pixel intensity in red regions pushes us to the other."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p36", "contents": "While much better than before, it\u2019s still not terribly good. Unfortunately, even looking at the data from the best angle, MNIST data doesn\u2019t line up nicely for us to look at. It\u2019s a non-trivial high-dimensional structure, and these sorts of linear projections just aren\u2019t going to cut it."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p37", "contents": "Thankfully, we have some powerful tools for dealing with datasets which are\u2026 uncooperative."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p38", "contents": "What would we consider a success? What would it mean to have the \u2018perfect\u2019 visualization of MNIST? What should our goal be?"}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p39", "contents": "One really nice property would be if the distances between points in our visualization were the same as the distances between points in the original space. If that was true, we\u2019d be capturing the global geometry of the data."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p40", "contents": "Let\u2019s be a bit more precise. For any two MNIST data points, \\(x_i\\) and \\(x_j\\), there are two notions of distance between them. One is the distance between them in the original space1 and one is the distance between them in our visualization. We will use \\(d^*_{i,j}\\) to denote the distance between \\(x_i\\) and \\(x_j\\) in the original space and \\(d_{i,j}\\) to denote the distance between \\(x_i\\) and \\(x_j\\) in our visualization. Now we can define a cost:"}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p41", "contents": "\\[C = \\sum_{i\\neq j} ~(d^{*}_{i,j} - d_{i,j})^2\\]"}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p42", "contents": "This value describes how bad a visualization is. It basically says: \u201cIt\u2019s bad for distances to not be the same. In fact, it\u2019s quadratically bad.\u201d If it\u2019s high, it means that distances are dissimilar to the original space. If it\u2019s small, it means they are similar. If it is zero, we have a \u2018perfect\u2019 embedding."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p43", "contents": "That sounds like an optimization problem! And deep learning researchers know what to do with those! We pick a random starting point and apply gradient descent. 2"}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p44", "contents": "This technique is called multidimensional scaling (or MDS). If you like, there\u2019s a more physical description of what\u2019s going on. First, we randomly position each point on a plane. Next we connect each pair of points with a spring with the length of the original distance, \\(d^{*}_{i,j}\\). Then we let the points move freely and allow physics to take its course!"}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p45", "contents": "We don\u2019t reach a cost of zero, of course. Generally, high-dimensional structures can\u2019t be embedded in two dimensions in a way that preserves distances perfectly. We\u2019re demanding the impossible! But, even though we don\u2019t get a perfect answer, we do improve a lot on the original random embedding, and come to a decent visualization. We can see the different classes begin to separate, especially the ones."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p46", "contents": "Still, it seems like we should be able to do much better. Perhaps we should consider different cost functions? There\u2019s a huge space of possibilities. To start, there\u2019s a lot of variations on MDS. A common theme is cost functions emphasizing local structure as more important to maintain than global structure. A very simple example of this is Sammon\u2019s Mapping, defined by the cost function:"}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p47", "contents": "\\[C = \\sum_{i\\neq j} \\frac{(d^{*}_{i,j} - d_{i,j})^2}{d^{*}_{i,j}}\\]"}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p48", "contents": "In Sammon\u2019s mapping, we try harder to preserve the distances between nearby points than between those which are far apart. If two points are twice as close in the original space as two others, it is twice as important to maintain the distance between them."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p49", "contents": "For MNIST, the result isn\u2019t that different. The reason has to do with a rather unintuitive property regarding distances in high-dimensional data like MNIST. Let\u2019s consider the distances between some MNIST digits. For example, the distance between the similar ones, \\(\\mnist{6}\\) and \\(\\mnist{8}\\), is \\[d(\\mnist{6}, \\mnist{8}) = 4.53\\] On the other hand, the difference between the very different data points, \\(\\mnist{4}\\) and \\(\\mnist{12}\\), is \\[d(\\mnist{4}, \\mnist{12}) = 12.0\\] less than three times \\(d(\\mnist{6}, \\mnist{8})\\)!"}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p50", "contents": "Because there\u2019s so many ways similar points can be slightly different, the average distance between similar points is quite high. Conversely, as you get further away from a point, the amount of volume within that distance increases to an extremely high power, and so you are likely to run into different kinds of points. The result is that, in pixel space, the difference in distances between \u2018similar\u2019 and \u2018different\u2019 points can be much less than we\u2019d like, even in good cases."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p51", "contents": "Perhaps, if local behavior is what we want our embedding to preserve, we should optimize for that more explicitly."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p52", "contents": "Consider a nearest neighbor graph of MNIST. For example, consider a graph \\((V,E)\\) where the nodes are MNIST data points, and each point is connected to the three points that are closest to it in the original space.3 This graph is a simple way to encode local structure and forget about everything else."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p53", "contents": "Given such a graph, we can use standard graph layout algorithms to visualize MNIST. Here, we will use force-directed graph drawing: we pretend that all points are repelling charged particles, and that the edges are springs. This gives us a cost function:"}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p54", "contents": "\\[C~ = ~\\sum_{i\\neq j}\\frac{1}{d_{i,j}} ~+~ \\frac{1}{2}\\sum_{(i,j) \\in E} (d_{i,j} - d^{*}_{i,j})^2\\]"}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p55", "contents": "Which we minimize."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p56", "contents": "The graph discovers a lot of structure in MNIST. In particular, it seems to find the different MNIST classes. While they overlap, during the graph layout optimization we can see the clusters sliding over each other. They are unable to avoid overlapping when embedded on the plane due to connections between classes, but the cost function is at least trying to separate them."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p57", "contents": "One nice property of the graph visualization is that it explicitly shows us which points are connected to which other points. In earlier visualizations, if we see a point in a strange place, we are uncertain as to whether it\u2019s just stuck there, or if it should actually be there. The graph structure avoids this. For example, if you look at the red cluster of zeros, you will see a single blue point, the six \\(\\mnist{494}\\), among them. You can see from its neighbors that it is supposed to be there, and from looking at it you can see that it is, in fact, a very poorly written six that looks more like a zero."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p58", "contents": "The final technique I wish to introduce is the t-Distributed Stochastic Neighbor Embedding (t-SNE). This technique is extremely popular in the deep learning community. Unfortunately, t-SNE\u2019s cost function involves some non-trivial mathematical machinery and requires some significant effort to understand."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p59", "contents": "But, roughly, what t-SNE tries to optimize for is preserving the topology of the data. For every point, it constructs a notion of which other points are its \u2018neighbors,\u2019 trying to make all points have the same number of neighbors. Then it tries to embed them so that those points all have the same number of neighbors."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p60", "contents": "In some ways, t-SNE is a lot like the graph based visualization. But instead of just having points be neighbors (if there\u2019s an edge) or not neighbors (if there isn\u2019t an edge), t-SNE has a continuous spectrum of having points be neighbors to different extents."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p61", "contents": "t-SNE is often very successful at revealing clusters and subclusters in data."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p62", "contents": "t-SNE does an impressive job finding clusters and subclusters in the data, but is prone to getting stuck in local minima. For example, in the following image we can see two clusters of zeros (red) that fail to come together because a cluster of sixes (blue) get stuck between them."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p64", "contents": "A number of tricks can help us avoid these bad local minima. Firstly, using more data helps a lot. Because these visualizations are embeded in a blog post, they only use 1,000 points. Using the full 50,000 MNIST points works a lot better. In addition, it is recommended that one use simulated annealing and carefully select a number of hyperparamters."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p65", "contents": "Well done t-SNE plots reveal many interesting features of MNIST."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p66", "contents": "An even nicer plot can be found on the page labeled 2590, in the original t-SNE paper, Maaten & Hinton (2008)."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p67", "contents": "It\u2019s not just the classes that t-SNE finds. Let\u2019s look more closely at the ones."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p68", "contents": "The ones cluster is stretched horizontally. As we look at digits from left to right, we see a consistent pattern."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p69", "contents": "\\[\\mnist[1]{7} \\to \\mnist[1]{4} \\to \\mnist[1]{8} \\to \\mnist[1]{6} \\to \\mnist[1]{2} \\to \\mnist[1]{1}\\]"}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p70", "contents": "They move from forward leaning ones, like \\(\\mnist[1]{4}\\), into straighter like \\(\\mnist[1]{6}\\), and finally to slightly backwards leaning ones, like \\(\\mnist[1]{1}\\). It seems that in MNIST, the primary factor of variation in the ones is tilting. This is likely because MNIST normalizes digits in a number of ways, centering and scaling them. After that, the easiest way to be \u201cfar apart\u201d is to rotate and not overlap very much."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p71", "contents": "Similar structure can be observed in other classes, if you look at the t-SNE plot again."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p72", "contents": "Watching these visualizations, there\u2019s sometimes this sense that they\u2019re begging for another dimension. For example, watching the graph visualization optimize, one can see clusters slide over top of each other."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p73", "contents": "Really, we\u2019re trying to compress this extremely high-dimensional structure into two dimensions. It seems natural to think that there would be very big wins from adding an additional dimension. If nothing else, at least in three dimensions a line connecting two clusters doesn\u2019t divide the plane, precluding other connections between clusters."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p74", "contents": "In the following visualization, we construct a nearest neighbor graph of MNIST, as before, and optimize the same cost function. The only difference is that there are now three dimensions to lay it out in."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p75", "contents": "The three dimensional version, unsurprisingly, works much better. The clusters are quite separated and, while entangled, no longer overlap."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p76", "contents": "In this visualization, we can begin to see why it is easy to achieve around 95% accuracy classifying MNIST digits, but quickly becomes harder after that. You can make a lot of ground classifying digits by chopping off the colored protrusions above, the clusters of each class sticking out. (This is more or less what a linear Support Vector Machine does.4) But there\u2019s some much harder entangled sections, especially in the middle, that are difficult to classify."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p77", "contents": "Of course, we could do any of the above techniques in 3D! Even something as simple as MDS is able to display quite a bit in 3D."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p78", "contents": "In three dimensions, MDS does a much better job separating the classes than it did with two dimensions."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p79", "contents": "And, of course, we can do t-SNE in three dimensions."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p80", "contents": "Because t-SNE puts so much space between clusters, it benefits a lot less from the transition to three dimensions. It\u2019s still quite nice, though, and becomes much more so with more points."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p81", "contents": "If you want to visualize high dimensional data, there are, indeed, significant gains to doing it in three dimensions over two."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p82", "contents": "Dimensionality reduction is a well developed area, and we\u2019re only scratching the surface here. There are hundreds of techniques and variants that are unmentioned here. I encourage you to explore!"}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p83", "contents": "It\u2019s easy to slip into a mind set of thinking one of these techniques is better than the others, but I think they\u2019re all complementary. There\u2019s no way to map high-dimensional data into low dimensions and preserve all the structure. So, an approach must make trade-offs, sacrificing one property to preserve another. PCA tries to preserve linear structure, MDS tries to preserve global geometry, and t-SNE tries to preserve topology (neighborhood structure)."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p84", "contents": "These techniques give us a way to gain traction on understanding high-dimensional data. While directly trying to understand high-dimensional data with the human mind is all but hopeless, with these tools we can begin to make progress."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p85", "contents": "In the next post, we will explore applying these techniques to some different kinds of data \u2013 in particular, to visualizing representations of text. Then, equipped with these techniques, we will shift our focus to understanding neural networks themselves, visualizing how they transform high-dimensional data and building techniques to visualize the space of neural networks. If you\u2019re interested, you can subscribe to my rss feed so that you\u2019ll see these posts when they are published."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p86", "contents": "(I would be delighted to hear your comments and thoughts: you can comment inline or at the end. For typos, technical errors, or clarifications you would like to see added, you are encouraged to make a pull request on github)"}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p87", "contents": "I\u2019m grateful for the hospitality of Google\u2019s deep learning research group, which had me as an intern while I wrote this post and did the work it is based on. I\u2019m especially grateful to my internship host, Jeff Dean."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p88", "contents": "I was greatly helped by the comments, advice, and encouragement of many Googlers, both in the deep learning group and outside of it. These include: Greg Corrado, Jon Shlens, Matthieu Devin, Andrew Dai, Quoc Le, Anelia Angelova, Oriol Vinyals, Ilya Sutskever, Ian Goodfellow, Jutta Degener, and Anna Goldie."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p89", "contents": "I was strongly influenced by the thoughts, comments and notes of Michael Nielsen, especially his notes on Bret Victor\u2019s work. Michael\u2019s thoughts persuaded me that I should think seriously about interactive visualizations for understanding deep learning."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p90", "contents": "I was also helped by the support of a number of non-Googler friends, including Yoshua Bengio, Dario Amodei, Eliana Lorch, Taren Stinebrickner-Kauffman, and Laura Ball."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p91", "contents": "This blog post was made possible by a number of wonderful Javascript libraries, including D3.js, MathJax, jQuery, and three.js. A big thank you to everyone who contributed to these libraries."}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p92", "contents": "We have a number of options for defining distance between these high-dimensional vectors. For this post, we will use L2 distance, \\(d(x_i,x_j) = \\sqrt{\\sum_n (x_{i,n}-x_{j,n})^2}\\) \u00a0\u21a9"}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p93", "contents": "We initialize the points\u2019 positions by sampling a Gaussian around the origin. Our optimization process isn\u2019t standard gradient descent. Instead, we use a variant of momentum gradient descent. Before adding the gradient to the momentum, we normalize the gradient. This reduces the need for hyper-parameter tuning. \u00a0\u21a9"}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p94", "contents": "Note that points can end up connected to more, if they are the nearest neighbor of many points. \u00a0\u21a9"}
{"id": "https://colah.github.io/posts/2014-10-Visualizing-MNIST/_p95", "contents": "This isn\u2019t quite true. A linear SVM operates on the original space. This is a non-linear transformation of the original space. That said, this strongly suggests something similar in the original space, and so we\u2019d expect something similar to be true. \u00a0\u21a9"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p0", "contents": "Posted on July 13, 2014"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p1", "contents": "neural networks, convolutional neural networks, convolution, math, probability"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p2", "contents": "In a previous post, we built up an understanding of convolutional neural networks, without referring to any significant mathematics. To go further, however, we need to understand convolutions."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p3", "contents": "If we just wanted to understand convolutional neural networks, it might suffice to roughly understand convolutions. But the aim of this series is to bring us to the frontier of convolutional neural networks and explore new options. To do that, we\u2019re going to need to understand convolutions very deeply."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p4", "contents": "Thankfully, with a few examples, convolution becomes quite a straightforward idea."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p5", "contents": "Imagine we drop a ball from some height onto the ground, where it only has one dimension of motion. How likely is it that a ball will go a distance \\(c\\) if you drop it and then drop it again from above the point at which it landed?"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p6", "contents": "Let\u2019s break this down. After the first drop, it will land \\(a\\) units away from the starting point with probability \\(f(a)\\), where \\(f\\) is the probability distribution."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p7", "contents": "Now after this first drop, we pick the ball up and drop it from another height above the point where it first landed. The probability of the ball rolling \\(b\\) units away from the new starting point is \\(g(b)\\), where \\(g\\) may be a different probability distribution if it\u2019s dropped from a different height."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p8", "contents": "If we fix the result of the first drop so we know the ball went distance \\(a\\), for the ball to go a total distance \\(c\\), the distance traveled in the second drop is also fixed at \\(b\\), where \\(a+b=c\\). So the probability of this happening is simply \\(f(a) \\cdot g(b)\\).1"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p9", "contents": "Let\u2019s think about this with a specific discrete example. We want the total distance \\(c\\) to be 3. If the first time it rolls, \\(a=2\\), the second time it must roll \\(b=1\\) in order to reach our total distance \\(a+b=3\\). The probability of this is \\(f(2) \\cdot g(1)\\)."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p10", "contents": "However, this isn\u2019t the only way we could get to a total distance of 3. The ball could roll 1 units the first time, and 2 the second. Or 0 units the first time and all 3 the second. It could go any \\(a\\) and \\(b\\), as long as they add to 3."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p11", "contents": "The probabilities are \\(f(1) \\cdot g(2)\\) and \\(f(0) \\cdot g(3)\\), respectively."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p12", "contents": "In order to find the total likelihood of the ball reaching a total distance of \\(c\\), we can\u2019t consider only one possible way of reaching \\(c\\). Instead, we consider all the possible ways of partitioning \\(c\\) into two drops \\(a\\) and \\(b\\) and sum over the probability of each way."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p13", "contents": "\\[...~~ f(0)\\!\\cdot\\! g(3) ~+~ f(1)\\!\\cdot\\! g(2) ~+~ f(2)\\!\\cdot\\! g(1)~~...\\]"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p14", "contents": "We already know that the probability for each case of \\(a+b=c\\) is simply \\(f(a) \\cdot g(b)\\). So, summing over every solution to \\(a+b=c\\), we can denote the total likelihood as:"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p15", "contents": "\\[\\sum_{a+b=c} f(a) \\cdot g(b)\\]"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p16", "contents": "Turns out, we\u2019re doing a convolution! In particular, the convolution of \\(f\\) and \\(g\\), evluated at \\(c\\) is defined:"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p17", "contents": "\\[(f\\ast g)(c) = \\sum_{a+b=c} f(a) \\cdot g(b)~~~~\\]"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p18", "contents": "If we substitute \\(b = c-a\\), we get:"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p19", "contents": "\\[(f\\ast g)(c) = \\sum_a f(a) \\cdot g(c-a)\\]"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p20", "contents": "This is the standard definition2 of convolution."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p21", "contents": "To make this a bit more concrete, we can think about this in terms of positions the ball might land. After the first drop, it will land at an intermediate position \\(a\\) with probability \\(f(a)\\). If it lands at \\(a\\), it has probability \\(g(c-a)\\) of landing at a position \\(c\\)."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p22", "contents": "To get the convolution, we consider all intermediate positions."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p23", "contents": "There\u2019s a very nice trick that helps one think about convolutions more easily."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p24", "contents": "First, an observation. Suppose the probability that a ball lands a certain distance \\(x\\) from where it started is \\(f(x)\\). Then, afterwards, the probability that it started a distance \\(x\\) from where it landed is \\(f(-x)\\)."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p25", "contents": "If we know the ball lands at a position \\(c\\) after the second drop, what is the probability that the previous position was \\(a\\)?"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p26", "contents": "So the probability that the previous position was \\(a\\) is \\(g(-(a-c)) = g(c-a)\\)."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p27", "contents": "Now, consider the probability each intermediate position contributes to the ball finally landing at \\(c\\). We know the probability of the first drop putting the ball into the intermediate position a is \\(f(a)\\). We also know that the probability of it having been in \\(a\\), if it lands at \\(c\\) is \\(g(c-a)\\)."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p28", "contents": "Summing over the \\(a\\)s, we get the convolution."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p29", "contents": "The advantage of this approach is that it allows us to visualize the evaluation of a convolution at a value \\(c\\) in a single picture. By shifting the bottom half around, we can evaluate the convolution at other values of \\(c\\). This allows us to understand the convolution as a whole."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p30", "contents": "For example, we can see that it peaks when the distributions align."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p31", "contents": "And shrinks as the intersection between the distributions gets smaller."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p32", "contents": "By using this trick in an animation, it really becomes possible to visually understand convolutions."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p33", "contents": "Below, we\u2019re able to visualize the convolution of two box functions:"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p34", "contents": "Armed with this perspective, a lot of things become more intuitive."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p35", "contents": "Let\u2019s consider a non-probabilistic example. Convolutions are sometimes used in audio manipulation. For example, one might use a function with two spikes in it, but zero everywhere else, to create an echo. As our double-spiked function slides, one spike hits a point in time first, adding that signal to the output sound, and later, another spike follows, adding a second, delayed copy."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p36", "contents": "Convolutions are an extremely general idea. We can also use them in a higher number of dimensions."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p37", "contents": "Let\u2019s consider our example of a falling ball again. Now, as it falls, it\u2019s position shifts not only in one dimension, but in two."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p38", "contents": "Convolution is the same as before:"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p39", "contents": "\\[(f\\ast g)(c) = \\sum_{a+b=c} f(a) \\cdot g(b)\\]"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p40", "contents": "Except, now \\(a\\), \\(b\\) and \\(c\\) are vectors. To be more explicit,"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p41", "contents": "\\[(f\\ast g)(c_1, c_2) = \\sum_{\\begin{array}{c}a_1+b_1=c_1\\\\a_2+b_2=c_2\\end{array}} f(a_1,a_2) \\cdot g(b_1,b_2)\\]"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p42", "contents": "Or in the standard definition:"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p43", "contents": "\\[(f\\ast g)(c_1, c_2) = \\sum_{a_1, a_2} f(a_1, a_2) \\cdot g(c_1-a_1,~ c_2-a_2)\\]"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p44", "contents": "Just like one-dimensional convolutions, we can think of a two-dimensional convolution as sliding one function on top of another, multiplying and adding."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p45", "contents": "One common application of this is image processing. We can think of images as two-dimensional functions. Many important image transformations are convolutions where you convolve the image function with a very small, local function called a \u201ckernel.\u201d"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p46", "contents": "The kernel slides to every position of the image and computes a new pixel as a weighted sum of the pixels it floats over."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p47", "contents": "For example, by averaging a 3x3 box of pixels, we can blur an image. To do this, our kernel takes the value \\(1/9\\) on each pixel in the box,"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p48", "contents": "We can also detect edges by taking the values \\(-1\\) and \\(1\\) on two adjacent pixels, and zero everywhere else. That is, we subtract two adjacent pixels. When side by side pixels are similar, this is gives us approximately zero. On edges, however, adjacent pixels are very different in the direction perpendicular to the edge."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p49", "contents": "The gimp documentation has many other examples."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p50", "contents": "So, how does convolution relate to convolutional neural networks?"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p51", "contents": "Consider a 1-dimensional convolutional layer with inputs \\(\\{x_n\\}\\) and outputs \\(\\{y_n\\}\\), like we discussed in the previous post:"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p52", "contents": "As we observed, we can describe the outputs in terms of the inputs:"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p53", "contents": "\\[y_n = A(x_{n}, x_{n+1}, ...)\\]"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p54", "contents": "Generally, \\(A\\) would be multiple neurons. But suppose it is a single neuron for a moment."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p55", "contents": "Recall that a typical neuron in a neural network is described by:"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p56", "contents": "\\[\\sigma(w_0x_0 + w_1x_1 + w_2x_2 ~...~ + b)\\]"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p57", "contents": "Where \\(x_0\\), \\(x_1\\)\u2026 are the inputs. The weights, \\(w_0\\), \\(w_1\\), \u2026 describe how the neuron connects to its inputs. A negative weight means that an input inhibits the neuron from firing, while a positive weight encourages it to. The weights are the heart of the neuron, controlling its behavior.3 Saying that multiple neurons are identical is the same thing as saying that the weights are the same."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p58", "contents": "It\u2019s this wiring of neurons, describing all the weights and which ones are identical, that convolution will handle for us."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p59", "contents": "Typically, we describe all the neurons in a layer at once, rather than individually. The trick is to have a weight matrix, \\(W\\):"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p60", "contents": "\\[y = \\sigma(Wx + b)\\]"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p61", "contents": "For example, we get:"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p62", "contents": "\\[y_0 = \\sigma(W_{0,0}x_0 + W_{0,1}x_1 + W_{0,2}x_2 ...)\\]"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p63", "contents": "\\[y_1 = \\sigma(W_{1,0}x_0 + W_{1,1}x_1 + W_{1,2}x_2 ...)\\]"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p64", "contents": "Each row of the matrix describes the weights connecting a neuron to its inputs."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p65", "contents": "Returning to the convolutional layer, though, because there are multiple copies of the same neuron, many weights appear in multiple positions."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p66", "contents": "Which corresponds to the equations:"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p67", "contents": "\\[y_0 = \\sigma(W_0x_0 + W_1x_1 -b)\\]"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p68", "contents": "\\[y_1 = \\sigma(W_0x_1 + W_1x_2 -b)\\]"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p69", "contents": "So while, normally, a weight matrix connects every input to every neuron with different weights:"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p70", "contents": "\\[W = \\left[\\begin{array}{ccccc} \nW_{0,0} & W_{0,1} & W_{0,2} & W_{0,3} & ...\\\\\nW_{1,0} & W_{1,1} & W_{1,2} & W_{1,3} & ...\\\\\nW_{2,0} & W_{2,1} & W_{2,2} & W_{2,3} & ...\\\\\nW_{3,0} & W_{3,1} & W_{3,2} & W_{3,3} & ...\\\\\n...     &   ...   &   ...   &  ...    & ...\\\\\n\\end{array}\\right]\\]"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p71", "contents": "The matrix for a convolutional layer like the one above looks quite different. The same weights appear in a bunch of positions. And because neurons don\u2019t connect to many possible inputs, there\u2019s lots of zeros."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p72", "contents": "\\[W = \\left[\\begin{array}{ccccc} \nw_0 & w_1 &  0  &  0  & ...\\\\\n 0  & w_0 & w_1 &  0  & ...\\\\\n 0  &  0  & w_0 & w_1 & ...\\\\\n 0  &  0  &  0  & w_0 & ...\\\\\n... & ... & ... & ... & ...\\\\\n\\end{array}\\right]\\]"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p73", "contents": "Multiplying by the above matrix is the same thing as convolving with \\([...0, w_1, w_0, 0...]\\). The function sliding to different positions corresponds to having neurons at those positions."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p74", "contents": "What about two-dimensional convolutional layers?"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p75", "contents": "The wiring of a two dimensional convolutional layer corresponds to a two-dimensional convolution."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p76", "contents": "Consider our example of using a convolution to detect edges in an image, above, by sliding a kernel around and applying it to every patch. Just like this, a convolutional layer will apply a neuron to every patch of the image."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p77", "contents": "We introduced a lot of mathematical machinery in this blog post, but it may not be obvious what we gained. Convolution is obviously a useful tool in probability theory and computer graphics, but what do we gain from phrasing convolutional neural networks in terms of convolutions?"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p78", "contents": "The first advantage is that we have some very powerful language for describing the wiring of networks. The examples we\u2019ve dealt with so far haven\u2019t been complicated enough for this benefit to become clear, but convolutions will allow us to get rid of huge amounts of unpleasant book-keeping for us."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p79", "contents": "Secondly, convolutions come with significant implementational advantages. Many libraries provide highly efficient convolution routines. Further, while convolution naively appears to be an \\(O(n^2)\\) operation, using some rather deep mathematical insights, it is possible to create a \\(O(n\\log(n))\\) implementation. We will discuss this in much greater detail in a future post."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p80", "contents": "In fact, the use of highly-efficient parallel convolution implementations on GPUs has been essential to recent progress in computer vision."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p81", "contents": "This post is part of a series on convolutional neural networks and their generalizations. The first two posts will be review for those familiar with deep learning, while later ones should be of interest to everyone. To get updates, subscribe to my RSS feed!"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p82", "contents": "Please comment below or on the side. Pull requests can be made on github."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p83", "contents": "I\u2019m extremely grateful to Eliana Lorch, for extensive discussion of convolutions and help writing this post."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p84", "contents": "I\u2019m also grateful to Michael Nielsen and Dario Amodei for their comments and support."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p85", "contents": "We want the probability of the ball rolling \\(a\\) units the first time and also rolling \\(b\\) units the second time. The distributions \\(P(A) = f(a)\\) and \\(P(b) = g(b)\\) are independent, with both distributions centered at 0. So \\(P(a,b) = P(a) * P(b) = f(a) \\cdot g(b)\\).\u21a9"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p86", "contents": "The non-standard definition, which I haven\u2019t previously seen, seems to have a lot of benefits. In future posts, we will find this definition very helpful because it lends itself to generalization to new algebraic structures. But it also has the advantage that it makes a lot of algebraic properties of convolutions really obvious."}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p87", "contents": "For example, convolution is a commutative operation. That is, \\(f\\ast g = g\\ast f\\). Why?"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p88", "contents": "\\[\\sum_{a+b=c} f(a) \\cdot g(b) ~~=~  \\sum_{b+a=c} g(b) \\cdot f(a)\\]"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p89", "contents": "Convolution is also associative. That is, \\((f\\ast g)\\ast h = f\\ast (g\\ast h)\\). Why?"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p90", "contents": "\\[\\sum_{(a+b)+c=d} (f(a) \\cdot g(b)) \\cdot h(c) ~~=~ \\sum_{a+(b+c)=d} f(a) \\cdot (g(b) \\cdot h(c))\\]\u21a9"}
{"id": "https://colah.github.io/posts/2014-07-Understanding-Convolutions/_p91", "contents": "There\u2019s also the bias, which is the \u201cthreshold\u201d for whether the neuron fires, but it\u2019s much simpler and I don\u2019t want to clutter this section talking about it.\u21a9"}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p0", "contents": "Posted on July  8, 2014"}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p1", "contents": "neural networks, deep learning, convolutional neural networks, modular neural networks"}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p2", "contents": "In the last few years, deep neural networks have lead to breakthrough results on a variety of pattern recognition problems, such as computer vision and voice recognition. One of the essential components leading to these results has been a special kind of neural network called a convolutional neural network."}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p3", "contents": "At its most basic, convolutional neural networks can be thought of as a kind of neural network that uses many identical copies of the same neuron.1 This allows the network to have lots of neurons and express computationally large models while keeping the number of actual parameters \u2013 the values describing how neurons behave \u2013 that need to be learned fairly small."}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p4", "contents": "This trick of having multiple copies of the same neuron is roughly analogous to the abstraction of functions in mathematics and computer science. When programming, we write a function once and use it in many places \u2013 not writing the same code a hundred times in different places makes it faster to program, and results in fewer bugs. Similarly, a convolutional neural network can learn a neuron once and use it in many places, making it easier to learn the model and reducing error."}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p5", "contents": "Suppose you want a neural network to look at audio samples and predict whether a human is speaking or not. Maybe you want to do more analysis if someone is speaking."}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p6", "contents": "You get audio samples at different points in time. The samples are evenly spaced."}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p7", "contents": "The simplest way to try and classify them with a neural network is to just connect them all to a fully-connected layer. There are a bunch of different neurons, and every input connects to every neuron."}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p8", "contents": "A more sophisticated approach notices a kind of symmetry in the properties it\u2019s useful to look for in the data. We care a lot about local properties of the data: What frequency of sounds are there around a given time? Are they increasing or decreasing? And so on."}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p9", "contents": "We care about the same properties at all points in time. It\u2019s useful to know the frequencies at the beginning, it\u2019s useful to know the frequencies in the middle, and it\u2019s also useful to know the frequencies at the end. Again, note that these are local properties, in that we only need to look at a small window of the audio sample in order to determine them."}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p10", "contents": "So, we can create a group of neurons, \\(A\\), that look at small time segments of our data.2 \\(A\\) looks at all such segments, computing certain features. Then, the output of this convolutional layer is fed into a fully-connected layer, \\(F\\)."}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p11", "contents": "In the above example, \\(A\\) only looked at segments consisting of two points. This isn\u2019t realistic. Usually, a convolution layer\u2019s window would be much larger."}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p12", "contents": "In the following example, \\(A\\) looks at 3 points. That isn\u2019t realistic either \u2013 sadly, it\u2019s tricky to visualize \\(A\\) connecting to lots of points."}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p13", "contents": "One very nice property of convolutional layers is that they\u2019re composable. You can feed the output of one convolutional layer into another. With each layer, the network can detect higher-level, more abstract features."}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p14", "contents": "In the following example, we have a new group of neurons, \\(B\\). \\(B\\) is used to create another convolutional layer stacked on top of the previous one."}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p15", "contents": "Convolutional layers are often interweaved with pooling layers. In particular, there is a kind of layer called a max-pooling layer that is extremely popular."}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p16", "contents": "Often, from a high level perspective, we don\u2019t care about the precise point in time a feature is present. If a shift in frequency occurs slightly earlier or later, does it matter?"}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p17", "contents": "A max-pooling layer takes the maximum of features over small blocks of a previous layer. The output tells us if a feature was present in a region of the previous layer, but not precisely where."}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p18", "contents": "Max-pooling layers kind of \u201czoom out\u201d. They allow later convolutional layers to work on larger sections of the data, because a small patch after the pooling layer corresponds to a much larger patch before it. They also make us invariant to some very small transformations of the data."}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p19", "contents": "In our previous examples, we\u2019ve used 1-dimensional convolutional layers. However, convolutional layers can work on higher-dimensional data as well. In fact, the most famous successes of convolutional neural networks are applying 2D convolutional neural networks to recognizing images."}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p20", "contents": "In a 2-dimensional convolutional layer, instead of looking at segments, \\(A\\) will now look at patches."}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p21", "contents": "For each patch, \\(A\\) will compute features. For example, it might learn to detect the presence of an edge. Or it might learn to detect a texture. Or perhaps a contrast between two colors."}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p22", "contents": "In the previous example, we fed the output of our convolutional layer into a fully-connected layer. But we can also compose two convolutional layers, as we did in the one dimensional case."}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p23", "contents": "We can also do max pooling in two dimensions. Here, we take the maximum of features over a small patch."}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p24", "contents": "What this really boils down to is that, when considering an entire image, we don\u2019t care about the exact position of an edge, down to a pixel. It\u2019s enough to know where it is to within a few pixels."}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p25", "contents": "Three-dimensional convolutional networks are also sometimes used, for data like videos or volumetric data (eg. 3D medical scans). However, they are not very widely used, and much harder to visualize."}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p26", "contents": "Now, we previously said that \\(A\\) was a group of neurons. We should be a bit more precise about this: what is \\(A\\) exactly?"}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p27", "contents": "In traditional convolutional layers, \\(A\\) is a bunch of neurons in parallel, that all get the same inputs and compute different features."}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p28", "contents": "For example, in a 2-dimensional convolutional layer, one neuron might detect horizontal edges, another might detect vertical edges, and another might detect green-red color contrasts."}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p29", "contents": "That said, in the recent paper \u2018Network in Network\u2019 (Lin et al. (2013)), a new \u201cMlpconv\u201d layer is proposed. In this model, \\(A\\) would have multiple layers of neurons, with the final layer outputting higher level features for the region. In the paper, the model achieves some very impressive results, setting new state of the art on a number of benchmark datasets."}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p30", "contents": "That said, for the purposes of this post, we will focus on standard convolutional layers. There\u2019s already enough for us to consider there!"}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p31", "contents": "Earlier, we alluded to recent breakthroughs in computer vision using convolutional neural networks. Before we go on, I\u2019d like to briefly discuss some of these results as motivation."}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p32", "contents": "In 2012, Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton blew existing image classification results out of the water (Krizehvsky et al. (2012))."}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p33", "contents": "Their progress was the result of combining together a bunch of different pieces. They used GPUs to train a very large, deep, neural network. They used a new kind of neuron (ReLUs) and a new technique to reduce a problem called \u2018overfitting\u2019 (DropOut). They used a very large dataset with lots of image categories (ImageNet). And, of course, it was a convolutional neural network."}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p34", "contents": "Their architecture, illustrated below, was very deep. It has 5 convolutional layers,3 with pooling interspersed, and three fully-connected layers. The early layers are split over the two GPUs."}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p35", "contents": "They trained their network to classify images into a thousand different categories."}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p36", "contents": "Randomly guessing, one would guess the correct answer 0.1% of the time. Krizhevsky, et al.\u2019s model is able to give the right answer 63% of the time. Further, one of the top 5 answers it gives is right 85% of the time!"}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p37", "contents": "Even some of its errors seem pretty reasonable to me!"}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p38", "contents": "We can also examine what the first layer of the network learns to do."}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p39", "contents": "Recall that the convolutional layers were split between the two GPUs. Information doesn\u2019t go back and forth each layer, so the split sides are disconnected in a real way. It turns out that, every time the model is run, the two sides specialize."}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p40", "contents": "Neurons in one side focus on black and white, learning to detect edges of different orientations and sizes. Neurons on the other side specialize on color and texture, detecting color contrasts and patterns.4 Remember that the neurons are randomly initialized. No human went and set them to be edge detectors, or to split in this way. It arose simply from training the network to classify images."}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p41", "contents": "These remarkable results (and other exciting results around that time) were only the beginning. They were quickly followed by a lot of other work testing modified approaches and gradually improving the results, or applying them to other areas. And, in addition to the neural networks community, many in the computer vision community have adopted deep convolutional neural networks."}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p42", "contents": "Convolutional neural networks are an essential tool in computer vision and modern pattern recognition."}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p43", "contents": "Consider a 1-dimensional convolutional layer with inputs \\(\\{x_n\\}\\) and outputs \\(\\{y_n\\}\\):"}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p44", "contents": "It\u2019s relatively easy to describe the outputs in terms of the inputs:"}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p45", "contents": "\\[y_n = A(x_{n}, x_{n+1}, ...)\\]"}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p46", "contents": "For example, in the above:"}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p47", "contents": "\\[y_0 = A(x_0, x_1)\\] \\[y_1 = A(x_1, x_2)\\]"}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p48", "contents": "Similarly, if we consider a 2-dimensional convolutional layer, with inputs \\(\\{x_{n,m}\\}\\) and outputs \\(\\{y_{n,m}\\}\\):"}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p49", "contents": "We can, again, write down the outputs in terms of the inputs:"}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p50", "contents": "\\[y_{n,m} = A\\left(\\begin{array}{ccc} x_{n,~m}, & x_{n+1,~m},& ...,~\\\\ x_{n,~m+1}, & x_{n+1,~m+1}, & ..., ~\\\\ &...\\\\\\end{array}\\right)\\]"}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p51", "contents": "For example:"}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p52", "contents": "\\[y_{0,0} = A\\left(\\begin{array}{cc} x_{0,~0}, & x_{1,~0},~\\\\ x_{0,~1}, & x_{1,~1}~\\\\\\end{array}\\right)\\] \\[y_{1,0} = A\\left(\\begin{array}{cc} x_{1,~0}, & x_{2,~0},~\\\\ x_{1,~1}, & x_{2,~1}~\\\\\\end{array}\\right)\\]"}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p53", "contents": "If one combines this with the equation for \\(A(x)\\),"}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p54", "contents": "\\[A(x) = \\sigma(Wx + b)\\]"}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p55", "contents": "one has everything they need to implement a convolutional neural network, at least in theory."}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p56", "contents": "In practice, this is often not best way to think about convolutional neural networks. There is an alternative formulation, in terms of a mathematical operation called convolution, that is often more helpful."}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p57", "contents": "The convolution operation is a powerful tool. In mathematics, it comes up in diverse contexts, ranging from the study of partial differential equations to probability theory. In part because of its role in PDEs, convolution is very important in the physical sciences. It also has an important role in many applied areas, like computer graphics and signal processing."}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p58", "contents": "For us, convolution will provide a number of benefits. Firstly, it will allow us to create much more efficient implementations of convolutional layers than the naive perspective might suggest. Secondly, it will remove a lot of messiness from our formulation, handling all the bookkeeping presently showing up in the indexing of \\(x\\)s \u2013 the present formulation may not seem messy yet, but that\u2019s only because we haven\u2019t got into the tricky cases yet. Finally, convolution will give us a significantly different perspective for reasoning about convolutional layers."}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p59", "contents": "I admire the elegance of your method of computation; it must be nice to ride through these fields upon the horse of true mathematics while the like of us have to make our way laboriously on foot. \u2003\u2014 Albert Einstein"}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p60", "contents": "Read the next post!"}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p61", "contents": "This post is part of a series on convolutional neural networks and their generalizations. The first two posts will be review for those familiar with deep learning, while later ones should be of interest to everyone. To get updates, subscribe to my RSS feed!"}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p62", "contents": "Please comment below or on the side. Pull requests can be made on github."}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p63", "contents": "I\u2019m grateful to Eliana Lorch, Aaron Courville, and Sebastian Zany for their comments and support."}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p64", "contents": "It should be noted that not all neural networks that use multiple copies of the same neuron are convolutional neural networks. Convolutional neural networks are just one type of neural network that uses the more general trick, weight-tying. Other kinds of neural network that do this are recurrent neural networks and recursive neural networks.\u21a9"}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p65", "contents": "Groups of neurons, like \\(A\\), that appear in multiple places are sometimes called modules, and networks that use them are sometimes called modular neural networks.\u21a9"}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p66", "contents": "They also test using 7 in the paper.\u21a9"}
{"id": "https://colah.github.io/posts/2014-07-Conv-Nets-Modular/_p67", "contents": "This seems to have interesting analogies to rods and cones in the retina.\u21a9"}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p0", "contents": "Posted on July  6, 2014"}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p1", "contents": "math, fanfiction, graphs, visualization"}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p2", "contents": "On a website called fanfiction.net, users write millions of stories about their favorite stories. They have diverse opinions about them. They love some stories, and hate others. The opinions are noisy, and it\u2019s hard to see the big picture."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p3", "contents": "With tools from mathematics and some helpful software, however, we can visualize the underlying structure."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p4", "contents": "In the following post, we will visualize the Harry Potter, Naruto and Twilight fandoms on fanfiction.net. We will also use Google\u2019s PageRank algorithm to rank stories, and perform collaborative filtering to make story recommendations to top fanfiction.net users."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p5", "contents": "If you\u2019re not interested in the details, you can skip to the following:"}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p6", "contents": "Interactive Graphs: Harry Potter, Naruto, Twilight"}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p7", "contents": "Story Rankings: Harry Potter, Naruto, Twilight"}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p8", "contents": "Story Recommendations: Harry Potter, Naruto, Twilight"}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p9", "contents": "And of course, you might skim below to see the pretty pictures!"}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p10", "contents": "Fanfiction is a wide-spread phenomenon where fans of different works write derivative stories. This ranges from young children writing their first stories about their favorite fictional characters, to professional-quality stories written by aspiring novelists. Many such stories are posted to websites where they are read by a large audience and commented on. The largest such website is fanficiton.net."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p11", "contents": "The sheer amount of fanfiction out there is rather staggering. The total number of stories on fanfiction.net exceeds six million. Harry Potter stories account for around 14% of these, followed by Naruto (around 7%) and Twilight (around 4%) (FFN Research). The majority of these stories have very little in the way of readership, but popular stories can have a large number of readers."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p12", "contents": "Some research was done into the demographics of fanfiction.net users and other topics by FFN Research. They found that 78% of fanfiction.net authors who joined in 2010 identified as female. Further, around 80% of users who report their age are between 13 and 17."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p13", "contents": "A lot of other interesting research and analysis has been done on the blogs Destination: Toast! and TOASTYSTATS."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p14", "contents": "In addition to allowing users to post stories they write, fanfiction.net allows authors to \u201cfavorite\u201d stories they like. Looking at which stories tend to be favorited by the same users gives us a way to understand connections between stories."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p15", "contents": "In order to analyze this, we must collect a large amount of metadata from fanfiction.net (\u201cscraping\u201d). We note that we don\u2019t actually collect any significant content, just a lot of data about relationships between pieces of content. Fanfiction.net\u2019s terms of service, as the author understands them, allow this with some restrictions:"}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p16", "contents": "4(E) You agree not to use or launch any automated system, including without limitation, \u201crobots,\u201d \u201cspiders,\u201d or \u201coffline readers,\u201d that accesses the Website in a manner that sends more request messages to the FanFiction.Net servers in a given period of time than a human can reasonably produce in the same period by using a conventional on-line web browser. Notwithstanding the foregoing, FanFiction.Net grants the operators of public search engines permission to use spiders to copy materials from the site for the sole purpose of and solely to the extent necessary for creating publicly available searchable indices of the materials, but not caches or archives of such materials\u2026"}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p17", "contents": "In order to ensure compliance with these terms, the author intentionally built significant rate limiting into the scraper and took care to minimize the load put on fanfiction.net. While the issue of academic analysis was not mentioned, it was not excluded and fanfiction.net\u2019s operators have not previously objected to similar academic work. Further, this work could be the preliminary research needed for someone to build a good fanficiton search engine."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p18", "contents": "Another section of the terms of service prohibits collecting personally identifiable information, which they define to include usernames. As such, I have deliberately discarded all such information and don\u2019t use it. (Though, I note that several search engines do \u2013 try searching for an authors name on any major search engine.) I do refer to some usernames in this post, but that was done entirely by hand."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p19", "contents": "In collecting data, since we are only looking at a subset of users, it is important to be wary of sampling bias. For example, if we sampled authors starting from the favorites of a particular author, or from those who had contributed stories to a community, we might get a very skewed perspective of the stories on fanfiction.net. The author considered a number of approaches, but concluded the fairest approach would be to use the authors of the most reviewed stories on fanfiction.net. This is a bias, but it should bias us towards the most interesting and important parts of the graph."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p20", "contents": "A graph, in the context of mathematics, is a collection of objects called vertices joined by connections called edges. For example, cities can be thought of as the vertices a graph connected by different highways and roads (the edges)."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p21", "contents": "A weighted graph is a graph where some edges are \u201cstronger\u201d than others. For example, some cities are connected by giant 6-lane highways, while others are connected by gravel roads. Larger weights represent stronger connections and smaller weights represent weaker ones. A weight of zero is the same thing as having no connection at all."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p22", "contents": "We will be interpreting fanfiction as a weighted graph, where edges represent a \u201cconnection\u201d between stories. We will be using as our weights for edges the probability that someone will like both stories, given that they like one. That is, \\(W_{a, b} = \\frac{|F_a \\cap F_b|}{|F_a \\cup F_b|}\\) where \\(F_s\\) is the users who favorited the story \\(s\\)."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p23", "contents": "There are lots of other possibilities, some resulting in directed graphs:"}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p24", "contents": "Our experience was that it didn\u2019t matter too much for the results, for large graphs."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p25", "contents": "(It\u2019s worth noting that many of these could easily generalize to higher-dimensional edges for a weighted hyper-graph.)"}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p26", "contents": "In our selected weight definition, \\(W_{a, b} = \\frac{|F_a \\cap F_b|}{|F_a \\cup F_b|}\\), we give equal weight to the preferences of all users. But there\u2019s a lot of variance between users: some favorite everything under the sun, while others very selectively favorite stories they really like. If we give the users who favorite thousands of stories the same weight as users who favorite ten, the users who favorite thousands dominate everything (and aren\u2019t a very good signal)."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p27", "contents": "Instead, we give each user \\(u\\) a weight of \\(\\frac{1}{20+n(u)}\\) where \\(n(u)\\) denotes the number of stories \\(u\\) has favorited. This results in a measure on the space of users, \\(\\mu(S) = \\sum_{u \\in S} \\frac{1}{20+n(u)}\\), and the equation for our weights becomes \\(W_{a, b} = \\frac{\\mu(F_a \\cap F_b)}{\\mu(F_a \\cup F_b)}\\)."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p28", "contents": "Applying these techniques to a couple of the top Harry Potter stories, we get the following graph (using graphviz):"}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p29", "contents": "With a small amount of investigation, it\u2019s easy to understand a lot of the graph\u2019s structure. For example, on the lower right hand side, there\u2019s a triangular clique."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p30", "contents": "A quick Google search reveals that this triangular clique consists of the \u201cDark Prince Trilogy\u201d by Kurinoone. The stories are more strongly linked to their immediate predecessor/successor than the pair separated by a story are to eachother."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p31", "contents": "If we use different tools, we can visualize much larger graphs."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p32", "contents": "We consider the top 2,000 most reviewed Harry Potter stories and their authors. Based on the author\u2019s favorite lists, we construct a weighted graph, with the stories as nodes (edge weights are calculated as above)."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p33", "contents": "We then prune the graph\u2019s edges, keeping the top 8,000 most strongly weighted edges. We also prune the nodes, keeping only those with at least one edge. This leaves us with a graph of 1,623 nodes and 8,000 edges."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p34", "contents": "We then load this graph into the graph visualization tool gephi. We layout the graph using the OpenOrd and ForceAtlas2 layout algorithms. (OpenOrd was particularly good at extracting clusters. Beyond that, this was largely a matter of aesthetic taste.)"}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p35", "contents": "We can see lots of interesting structure in this graph: there are lots of clusters, some more connected than others."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p36", "contents": "A first hypothesis might be that some of these clusters are caused by language. As it turns out, this is the case:"}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p37", "contents": "Another cause of clusters may be the \u201cship\u201d (romantic pairing of the story). Many readers have a strong loyalty to a particular ship \u2013 for example, they might feel very strongly that Harry and Hermione should be together."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p38", "contents": "(Note: Ships are inferred from tags story summaries. HP = Harry Potter, HG = Hermione Granger, GW = Ginny Weasley, DM = Draco Malfoy, SS = Severus Snape and LV = Lord Voldemort.)"}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p39", "contents": "One interesting point is that by far the most diffused ship is HP/GW. It seems likely that this is because it is the ship we see in cannon Harry Potter, and so many stories not focused on romance default to it and unaligned readers are more tolerant of it."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p40", "contents": "One striking pattern in fanfiction is that a massive fraction of stories are male/male pairings. Such stories are frequently referred to as \u201cslash.\u201d"}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p41", "contents": "Many stories include a slash tag in the summary. Some other stories tag themselves as \u201cno-slash.\u201d"}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p42", "contents": "One interesting pattern is that stories tagged \u201cno-slash\u201d concentrate around parts of the border of slash stories. One possible reason may be that authors writing stories that might, from a glance at the summary or characters list, look like slash (for example, a story about Snape mentoring Harry, or Draco and Harry as friends) feel the need to explicitly signal that that is not the topic of their story."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p43", "contents": "The predisposition of the French cluster towards slash stories is interesting, but the cluster is so small I am hesitant to read anything into it."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p44", "contents": "You can also explore an interactive graph of Harry Potter fanfiction."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p45", "contents": "Of course, we can apply the exact same tricks to other fandoms."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p47", "contents": "For example, Naruto is the second biggest fandom. Here\u2019s a graph of it:"}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p48", "contents": "We can look at languages again:"}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p49", "contents": "And also for ships:"}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p51", "contents": "And again, we can graph the top twilight stories:"}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p52", "contents": "We can color it by language:"}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p53", "contents": "And by ship:"}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p54", "contents": "One thing that seems pretty surprising, without inside knowledge of the fandom, is the lack of stories where the pairing involves Jacob. On further inspection, we find that there are stories like that on fanfiction.net, but they aren\u2019t amongst the most highly reviewed. Perhaps this pairing prefers other websites? I\u2019d love comments from anyone with insight into this."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p55", "contents": "You can also explore an interactive graph of Naruto fanfiction and of Twilight fanfiction."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p56", "contents": "What are the best fanfics on fanfiction.net? How can we identify them?"}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p57", "contents": "A naive approach would be to select the most favorited or reviewed stories. But people\u2019s quality of taste varies. A more sophisticated approach is Google\u2019s PageRank algorithm which is used to determine which web pages are of high quality."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p58", "contents": "In a normal vote gives equal weight to every voter. But some voters are better qualified to decide than others. In PageRank, we recalculate the votes again and again, giving each \u201cperson\u2019s\u201d vote a weight based on how many votes they received in the previous step."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p59", "contents": "In the case of the Internet, we interpret a website linking to another website as that website voting for the one it links to. Similarly, we can apply it to fanfiction by interpreting story A as \u201cvoting\u201d for a story B with a weight of the probability that a user who likes A also likes B."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p60", "contents": "Harry Potter top stories by PageRank:"}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p61", "contents": "Naruto top stories by PageRank:"}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p62", "contents": "Twilight top stories by PageRank:"}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p63", "contents": "One neat thing we can do is give nodes on our graphs a size based on their PageRank. (We can also color nodes based on the first three components of the singular value decomposition of the adjacency matrix.)"}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p64", "contents": "There\u2019s something that\u2019s just begging to be done, at this point: story recommendations. Given our knowledge of what stories many users like, can we recommend other stories that they\u2019re probable to like?"}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p65", "contents": "This problem is called collaborative filtering, and is a well-established area. Unfortunately, it isn\u2019t something I\u2019m terribly knowledgeable about, so I took a relatively naive approach: sum over the preferences of all users, weighted by how similar their preferences are to the user you are trying to predict."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p66", "contents": "Specifically, we give each story, \\(s\\), a rank \\(R_u(s)\\), for a user \\(u\\). If the rank is high, we think \\(u\\) is likely to like \\(s\\)."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p67", "contents": "\\[R_u(s) = \\sum_{v\\in F_s \\setminus \\{u\\}} \\left(\\frac{|S(u)\\cap S(v)|}{20+|S(v)|}\\right)^2\\]"}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p68", "contents": "where \\(F_s\\) is the set of users who favorited \\(s\\) and \\(S(u)\\) is the stories favorited by the user \\(u\\)."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p69", "contents": "For example, we can make recommendations for S\u2019TarKan, the author of the most favorited Harry Potter story on fanfiction.net:"}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p70", "contents": "A * denotes that this is already one of the users favorite stories or one of their own stories. We can exclude their favorite stories, and their own stories:"}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p71", "contents": "These are all very popular stories. It\u2019s not very useful to S\u2019TarKan if we recommend them extremely popular stories that they\u2019ve almost certainly seen before. As such, it is interesting to penalize the popularity of stories."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p72", "contents": "Consider \\(\\frac{R_u(s)}{|F_s|^k}\\). When \\(k = 0\\), it\u2019s our original rank. When \\(k = 1\\), it full normalizes stories against popularity. And in between, it penalizes popularity to varying degrees. If we set k = 0.7, we get these recommendations:"}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p73", "contents": "You can think of these as stories that are unexpectedly popular amongst similar users. Similar users like them a lot more than random users like them. (Though, perhaps 0.7 is a bit too extreme.)"}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p74", "contents": "Curious about what this algorithm would recommend for you? If you\u2019re a popular fanfiction author, you may be in my recommendations for top users for Harry Potter, Naruto or Twilight."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p75", "contents": "Since my scripts can\u2019t look at your author name while complying with fanfiction.net\u2019s terms of service, you will need to know your author ID. To get it, go to your fanfiction.net profile page and look at the URL. It will be of the form: http://fanfiction.net/u/author_ID/.... Then search for your author ID in the file!"}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p76", "contents": "I\u2019m certain one could do much better if they wanted to put a bit more effort into it. :)"}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p77", "contents": "In light of all this, I\u2019d like to reflect on a few things."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p78", "contents": "Big Data: A year ago, I was very dismissive of \u201cbig data\u201d as a buzzword. Primarily, it seems to be thrown around by business people who don\u2019t really understand much. But one thing I\u2019ve learned in explorations of data like this one and working in machine learning, is that there is something very powerful about larger amounts of data. There\u2019s something very qualitatively different. The fanfiction data I used was actually quite small, only a few hundred users, because of how I limited the amount I downloaded, but I think it still demonstrates the sorts of things that become possible as you have larger amounts of data. (To be honest, a much more compelling example is the progress that\u2019s been made in computer vision using ImageNet\u2026 But this still influenced my views.)"}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p79", "contents": "Digital Humanities: Digital humanities also seems to be a bit of a buzzword. But I hope this provides a simple example of the power that can come from applying a little bit of math and computer science to humanities problems."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p80", "contents": "Metadata and Privacy: In this essay, we analyzed stories by looking at whether they were favorited by the same users. There\u2019s a natural \u201cdual\u201d to this: analyzing users by looking at whether they favorited the same stories. This would give us a graph of connections between users and allow us to find clusters of users. But what if you use other forms of metadata? For example, we now know that the US government has metadata on who phones who. It seems very likely that many companies and governments have information on where your cellphone is as a function of time. All this can construct a graph of society. I can\u2019t really fathom how much one must be able to learn about someone from that. (And how easy it would be to misinterpret.)"}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p81", "contents": "Fanfiction Websites: I think there\u2019s a lot of potential for fanfiction websites to better serve their users based on the techniques outlined here. I\u2019d be really thrilled to see fanfiction.net or Archive Of Our Own adopt some of these ideas. Imagine being able to list a handful of stories in some category you\u2019re interested in and discover others? Or get good recommendations? The ideas are all pretty straightforward once you think of them. I\u2019d be very happy to talk to the groups behind different fanfiction websites and provide some help or share example code."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p82", "contents": "Deep Learning and NLP: Recently, there\u2019s been some really cool results in applying Deep Learning to Natural Language Processing. One would need a lot more data than I collected, and it would take more effort, but I bet one could do some really interesting things here."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p83", "contents": "t-SNE: t-Distributed Stochastic Neighbor Embedding, is an algorithm for visualizing the structure of high-dimensional data. It would be a much simpler approach to understanding the structure of fanfiction than the graph based one I used here, and probably give much better results. If I was starting again, I would use it."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p84", "contents": "Resources: In principle, I\u2019d really like to share my code and make it easy for people to replicate the work I described here. However, I think that would be really rude to fanfiction.net because it could result in lots of people scraping their website, and it seems likely many would remove my rate limiter. An alternative would be to share my extracted metadata, but, again, I think it would be really rude to do that without fanfiction.net\u2019s permission, and possibly a violation of their terms of service. So, in the end, I\u2019m not sharing any resources. That said, all of this can be done pretty easily."}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p85", "contents": "(This post is a fun experiment done primarily for amusement. I would be delighted to hear your comments and thoughts: you can comment inline or at the end. For typos, technical errors, or clarifications you would like to see added, you are encouraged to make a pull request on github. If you enjoyed this post, you might consider subscribing to my RSS feed.)"}
{"id": "https://colah.github.io/posts/2014-07-FFN-Graphs-Vis/_p86", "contents": "Thank you to Eliana Lorch, Taren Stinebrickner-Kauffman, Mary Becica, and Jacob Steinhardt for their comments and encouragement."}
{"id": "https://colah.github.io/../../archive.html_p0", "contents": "Bad request"}
{"id": "https://colah.github.io/../../archive.html_p1", "contents": "\n        Your browser sent a request that this server could not understand.\n        Sorry about that. Please check your request for errors and try again.\n        Contact us if the problem persists.\n      "}
{"id": "https://colah.github.io/../../posts/2015-08-Understanding-LSTMs/_p0", "contents": "Bad request"}
{"id": "https://colah.github.io/../../posts/2015-08-Understanding-LSTMs/_p1", "contents": "\n        Your browser sent a request that this server could not understand.\n        Sorry about that. Please check your request for errors and try again.\n        Contact us if the problem persists.\n      "}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p0", "contents": "Posted on August 31, 2015"}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p1", "contents": "Backpropagation is the key algorithm that makes training deep models computationally tractable. For modern neural networks, it can make training with gradient descent as much as ten million times faster, relative to a naive implementation. That\u2019s the difference between a model taking a week to train and taking 200,000 years."}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p2", "contents": "Beyond its use in deep learning, backpropagation is a powerful computational tool in many other areas, ranging from weather forecasting to analyzing numerical stability \u2013 it just goes by different names. In fact, the algorithm has been reinvented at least dozens of times in different fields (see Griewank (2010)). The general, application independent, name is \u201creverse-mode differentiation.\u201d"}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p3", "contents": "Fundamentally, it\u2019s a technique for calculating derivatives quickly. And it\u2019s an essential trick to have in your bag, not only in deep learning, but in a wide variety of numerical computing situations."}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p4", "contents": "Computational graphs are a nice way to think about mathematical expressions. For example, consider the expression \\(e=(a+b)*(b+1)\\). There are three operations: two additions and one multiplication. To help us talk about this, let\u2019s introduce two intermediary variables, \\(c\\) and \\(d\\) so that every function\u2019s output has a variable. We now have:"}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p8", "contents": "To create a computational graph, we make each of these operations, along with the input variables, into nodes. When one node\u2019s value is the input to another node, an arrow goes from one to another."}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p9", "contents": "These sorts of graphs come up all the time in computer science, especially in talking about functional programs. They are very closely related to the notions of dependency graphs and call graphs. They\u2019re also the core abstraction behind the popular deep learning framework Theano."}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p10", "contents": "We can evaluate the expression by setting the input variables to certain values and computing nodes up through the graph. For example, let\u2019s set \\(a=2\\) and \\(b=1\\):"}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p11", "contents": "The expression evaluates to \\(6\\)."}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p12", "contents": "If one wants to understand derivatives in a computational graph, the key is to understand derivatives on the edges. If \\(a\\) directly affects \\(c\\), then we want to know how it affects \\(c\\). If \\(a\\) changes a little bit, how does \\(c\\) change? We call this the partial derivative of \\(c\\) with respect to \\(a\\)."}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p13", "contents": "To evaluate the partial derivatives in this graph, we need the sum rule and the product rule:"}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p14", "contents": "\\[\\frac{\\partial}{\\partial a}(a+b) = \\frac{\\partial a}{\\partial a} + \\frac{\\partial b}{\\partial a} = 1\\]"}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p15", "contents": "\\[\\frac{\\partial}{\\partial u}uv = u\\frac{\\partial v}{\\partial u} + v\\frac{\\partial u}{\\partial u} = v\\]"}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p16", "contents": "Below, the graph has the derivative on each edge labeled."}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p17", "contents": "What if we want to understand how nodes that aren\u2019t directly connected affect each other? Let\u2019s consider how \\(e\\) is affected by \\(a\\). If we change \\(a\\) at a speed of 1, \\(c\\) also changes at a speed of \\(1\\). In turn, \\(c\\) changing at a speed of \\(1\\) causes \\(e\\) to change at a speed of \\(2\\). So \\(e\\) changes at a rate of \\(1*2\\) with respect to \\(a\\)."}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p18", "contents": "The general rule is to sum over all possible paths from one node to the other, multiplying the derivatives on each edge of the path together. For example, to get the derivative of \\(e\\) with respect to \\(b\\) we get:"}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p19", "contents": "\\[\\frac{\\partial e}{\\partial b}= 1*2 + 1*3\\]"}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p20", "contents": "This accounts for how b affects e through c and also how it affects it through d."}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p21", "contents": "This general \u201csum over paths\u201d rule is just a different way of thinking about the multivariate chain rule."}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p22", "contents": "The problem with just \u201csumming over the paths\u201d is that it\u2019s very easy to get a combinatorial explosion in the number of possible paths."}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p23", "contents": "In the above diagram, there are three paths from \\(X\\) to \\(Y\\), and a further three paths from \\(Y\\) to \\(Z\\). If we want to get the derivative \\(\\frac{\\partial Z}{\\partial X}\\) by summing over all paths, we need to sum over \\(3*3 = 9\\) paths:"}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p24", "contents": "\\[\\frac{\\partial Z}{\\partial X} = \\alpha\\delta + \\alpha\\epsilon + \\alpha\\zeta + \\beta\\delta + \\beta\\epsilon + \\beta\\zeta + \\gamma\\delta + \\gamma\\epsilon + \\gamma\\zeta\\]"}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p25", "contents": "The above only has nine paths, but it would be easy to have the number of paths to grow exponentially as the graph becomes more complicated."}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p26", "contents": "Instead of just naively summing over the paths, it would be much better to factor them:"}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p27", "contents": "\\[\\frac{\\partial Z}{\\partial X} = (\\alpha + \\beta + \\gamma)(\\delta + \\epsilon + \\zeta)\\]"}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p28", "contents": "This is where \u201cforward-mode differentiation\u201d and \u201creverse-mode differentiation\u201d come in. They\u2019re algorithms for efficiently computing the sum by factoring the paths. Instead of summing over all of the paths explicitly, they compute the same sum more efficiently by merging paths back together at every node. In fact, both algorithms touch each edge exactly once!"}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p29", "contents": "Forward-mode differentiation starts at an input to the graph and moves towards the end. At every node, it sums all the paths feeding in. Each of those paths represents one way in which the input affects that node. By adding them up, we get the total way in which the node is affected by the input, it\u2019s derivative."}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p30", "contents": "Though you probably didn\u2019t think of it in terms of graphs, forward-mode differentiation is very similar to what you implicitly learned to do if you took an introduction to calculus class."}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p31", "contents": "Reverse-mode differentiation, on the other hand, starts at an output of the graph and moves towards the beginning. At each node, it merges all paths which originated at that node."}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p32", "contents": "Forward-mode differentiation tracks how one input affects every node. Reverse-mode differentiation tracks how every node affects one output. That is, forward-mode differentiation applies the operator \\(\\frac{\\partial}{\\partial X}\\) to every node, while reverse mode differentiation applies the operator \\(\\frac{\\partial Z}{\\partial}\\) to every node.1"}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p33", "contents": "At this point, you might wonder why anyone would care about reverse-mode differentiation. It looks like a strange way of doing the same thing as the forward-mode. Is there some advantage?"}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p34", "contents": "Let\u2019s consider our original example again:"}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p35", "contents": "We can use forward-mode differentiation from \\(b\\) up. This gives us the derivative of every node with respect to \\(b\\)."}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p36", "contents": "We\u2019ve computed \\(\\frac{\\partial e}{\\partial b}\\), the derivative of our output with respect to one of our inputs."}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p37", "contents": "What if we do reverse-mode differentiation from \\(e\\) down? This gives us the derivative of \\(e\\) with respect to every node:"}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p38", "contents": "When I say that reverse-mode differentiation gives us the derivative of e with respect to every node, I really do mean every node. We get both \\(\\frac{\\partial e}{\\partial a}\\) and \\(\\frac{\\partial e}{\\partial b}\\), the derivatives of \\(e\\) with respect to both inputs. Forward-mode differentiation gave us the derivative of our output with respect to a single input, but reverse-mode differentiation gives us all of them."}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p39", "contents": "For this graph, that\u2019s only a factor of two speed up, but imagine a function with a million inputs and one output. Forward-mode differentiation would require us to go through the graph a million times to get the derivatives. Reverse-mode differentiation can get them all in one fell swoop! A speed up of a factor of a million is pretty nice!"}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p40", "contents": "When training neural networks, we think of the cost (a value describing how bad a neural network performs) as a function of the parameters (numbers describing how the network behaves). We want to calculate the derivatives of the cost with respect to all the parameters, for use in gradient descent. Now, there\u2019s often millions, or even tens of millions of parameters in a neural network. So, reverse-mode differentiation, called backpropagation in the context of neural networks, gives us a massive speed up!"}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p41", "contents": "(Are there any cases where forward-mode differentiation makes more sense? Yes, there are! Where the reverse-mode gives the derivatives of one output with respect to all inputs, the forward-mode gives us the derivatives of all outputs with respect to one input. If one has a function with lots of outputs, forward-mode differentiation can be much, much, much faster.)"}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p42", "contents": "When I first understood what backpropagation was, my reaction was: \u201cOh, that\u2019s just the chain rule! How did it take us so long to figure out?\u201d I\u2019m not the only one who\u2019s had that reaction. It\u2019s true that if you ask \u201cis there a smart way to calculate derivatives in feedforward neural networks?\u201d the answer isn\u2019t that difficult."}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p43", "contents": "But I think it was much more difficult than it might seem. You see, at the time backpropagation was invented, people weren\u2019t very focused on the feedforward neural networks that we study. It also wasn\u2019t obvious that derivatives were the right way to train them. Those are only obvious once you realize you can quickly calculate derivatives. There was a circular dependency."}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p44", "contents": "Worse, it would be very easy to write off any piece of the circular dependency as impossible on casual thought. Training neural networks with derivatives? Surely you\u2019d just get stuck in local minima. And obviously it would be expensive to compute all those derivatives. It\u2019s only because we know this approach works that we don\u2019t immediately start listing reasons it\u2019s likely not to."}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p45", "contents": "That\u2019s the benefit of hindsight. Once you\u2019ve framed the question, the hardest work is already done."}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p46", "contents": "Derivatives are cheaper than you think. That\u2019s the main lesson to take away from this post. In fact, they\u2019re unintuitively cheap, and us silly humans have had to repeatedly rediscover this fact. That\u2019s an important thing to understand in deep learning. It\u2019s also a really useful thing to know in other fields, and only more so if it isn\u2019t common knowledge."}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p47", "contents": "Are there other lessons? I think there are."}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p48", "contents": "Backpropagation is also a useful lens for understanding how derivatives flow through a model. This can be extremely helpful in reasoning about why some models are difficult to optimize. The classic example of this is the problem of vanishing gradients in recurrent neural networks."}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p49", "contents": "Finally, I claim there is a broad algorithmic lesson to take away from these techniques. Backpropagation and forward-mode differentiation use a powerful pair of tricks (linearization and dynamic programming) to compute derivatives more efficiently than one might think possible. If you really understand these techniques, you can use them to efficiently calculate several other interesting expressions involving derivatives. We\u2019ll explore this in a later blog post."}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p50", "contents": "This post gives a very abstract treatment of backpropagation. I strongly recommend reading Michael Nielsen\u2019s chapter on it for an excellent discussion, more concretely focused on neural networks."}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p51", "contents": "Thank you to Greg Corrado, Jon Shlens, Samy Bengio and Anelia Angelova for taking the time to proofread this post."}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p52", "contents": "Thanks also to Dario Amodei, Michael Nielsen and Yoshua Bengio for discussion of approaches to explaining backpropagation. Also thanks to all those who tolerated me practicing explaining backpropagation in talks and seminar series!"}
{"id": "https://colah.github.io/posts/2015-08-Backprop/_p53", "contents": "This might feel a bit like dynamic programming. That\u2019s because it is!\u21a9"}
{"id": "https://colah.github.io/../../posts/tags/functional_programming.html_p0", "contents": "Bad request"}
{"id": "https://colah.github.io/../../posts/tags/functional_programming.html_p1", "contents": "\n        Your browser sent a request that this server could not understand.\n        Sorry about that. Please check your request for errors and try again.\n        Contact us if the problem persists.\n      "}
{"id": "https://colah.github.io/../../posts/tags/Haskell.html_p0", "contents": "Bad request"}
{"id": "https://colah.github.io/../../posts/tags/Haskell.html_p1", "contents": "\n        Your browser sent a request that this server could not understand.\n        Sorry about that. Please check your request for errors and try again.\n        Contact us if the problem persists.\n      "}
{"id": "https://colah.github.io/../../posts/2014-07-FFN-Graphs-Vis/_p0", "contents": "Bad request"}
{"id": "https://colah.github.io/../../posts/2014-07-FFN-Graphs-Vis/_p1", "contents": "\n        Your browser sent a request that this server could not understand.\n        Sorry about that. Please check your request for errors and try again.\n        Contact us if the problem persists.\n      "}
{"id": "https://colah.github.io/../../posts/tags/visualization.html_p0", "contents": "Bad request"}
{"id": "https://colah.github.io/../../posts/tags/visualization.html_p1", "contents": "\n        Your browser sent a request that this server could not understand.\n        Sorry about that. Please check your request for errors and try again.\n        Contact us if the problem persists.\n      "}
{"id": "https://colah.github.io/../../posts/2015-02-DataList-Illustrated/_p0", "contents": "Bad request"}
{"id": "https://colah.github.io/../../posts/2015-02-DataList-Illustrated/_p1", "contents": "\n        Your browser sent a request that this server could not understand.\n        Sorry about that. Please check your request for errors and try again.\n        Contact us if the problem persists.\n      "}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p0", "contents": "Posted on September  3, 2015"}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p1", "contents": "Deep learning, despite its remarkable successes, is a young field. While models called artificial neural networks have been studied for decades, much of that work seems only tenuously connected to modern results."}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p2", "contents": "It\u2019s often the case that young fields start in a very ad-hoc manner. Later, the mature field is understood very differently than it was understood by its early practitioners. For example, in taxonomy, people have grouped plants and animals for thousands of years, but the way we understood what we were doing changed a lot in light of evolution and molecular biology. In chemistry, we have explored chemical reactions for a long time, but what we understood ourselves to do changed a lot with the discovery of irreducible elements, and again later with models of the atom. Those are grandiose examples, but the history of science and mathematics has seen this pattern again and again, on many different scales."}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p3", "contents": "It seems quite likely that deep learning is in this ad-hoc state."}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p4", "contents": "At the moment, deep learning is held together by an extremely successful tool. This tool doesn\u2019t seem fundamental; it\u2019s something we\u2019ve stumbled on, with seemingly arbitrary details that change regularly. As a field, we don\u2019t yet have some unifying insight or shared understanding. In fact, the field has several competing narratives!"}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p5", "contents": "I think it is very likely that, reflecting back in 30 years, we will see deep learning very differently."}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p6", "contents": "If we think we\u2019ll probably see deep learning very differently in 30 years, that suggests an interesting question: how are we going to see it? Of course, no one can actually know how we\u2019ll come to understand the field. But it is interesting to speculate."}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p7", "contents": "At present, three narratives are competing to be the way we understand deep learning. There\u2019s the neuroscience narrative, drawing analogies to biology. There\u2019s the representations narrative, centered on transformations of data and the manifold hypothesis. Finally, there\u2019s a probabilistic narrative, which interprets neural networks as finding latent variables. These narratives aren\u2019t mutually exclusive, but they do present very different ways of thinking about deep learning."}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p8", "contents": "This essay extends the representations narrative to a new answer: deep learning studies a connection between optimization and functional programming."}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p9", "contents": "In this view, the representations narrative in deep learning corresponds to type theory in functional programming. It sees deep learning as the junction of two fields we already know to be incredibly rich. What we find, seems so beautiful to me, feels so natural, that the mathematician in me could believe it to be something fundamental about reality."}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p10", "contents": "This is an extremely speculative idea. I am not arguing that it is true. I wish to argue only that it is plausible, that one could imagine deep learning evolving in this direction. To be clear: I am primarily making an aesthetic argument, rather than an argument of fact. I wish to show that this is a natural and elegant idea, encompassing what we presently call deep learning."}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p11", "contents": "The distinctive property of deep learning is that it studies deep neural networks \u2013 neural networks with many layers. Over the course of multiple layers, these models progressively bend data, warping it into a form where it is easy to solve the given task."}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p12", "contents": "The details of these layers change every so often.1 What has remained constant is that there is a sequence of layers."}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p13", "contents": "Each layer is a function, acting on the output of a previous layer. As a whole, the network is a chain of composed functions. This chain of composed functions is optimized to perform a task."}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p14", "contents": "Every model in deep learning that I am aware of involves optimizing composed functions. I believe this is the heart of what we are studying."}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p15", "contents": "With every layer, neural networks transform data, molding it into a form that makes their task easier to do. We call these transformed versions of data \u201crepresentations.\u201d"}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p16", "contents": "Representations correspond to types."}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p17", "contents": "At their crudest, types in computer science are a way of embedding some kind of data in \\(n\\) bits. Similarly, representations in deep learning are a way to embed a data manifold in \\(n\\) dimensions."}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p18", "contents": "Just as two functions can only be composed together if their types agree, two layers can only be composed together when their representations agree. Data in the wrong representation is nonsensical to a neural network. Over the course of training, adjacent layers negotiate the representation they will communicate in; the performance of the network depends on data being in the representation the network expects."}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p19", "contents": "In the case of very simple neural network architectures, where there\u2019s just a linear sequence of layers, this isn\u2019t very interesting. The representation of one layer\u2019s output needs to match the representation of the next layer\u2019s input \u2013 so what? It\u2019s a trivial and boring requirement."}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p20", "contents": "But many neural networks have more complicated architectures where this becomes a more interesting constraint. For a very simple example, let\u2019s imagine a neural network with multiple similar kinds of inputs, which performs multiple, related tasks. Perhaps it takes in RGB images and also grayscale images. Maybe it\u2019s looking at pictures of people, and trying to predict age and gender. Because the similarities between the kinds of inputs and between the kinds of tasks, it can be helpful to do all of this in one model, so that training data helps them all. The result is multiple input layers mapping into one representation, and multiple outputs mapping from the same representation."}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p21", "contents": "Perhaps this example seems a bit contrived, but mapping different kinds of data into the same representation can lead to some pretty remarkable things. For example, by mapping words from two languages into one representation, we can find corresponding words that we didn\u2019t know were translations when we started. And by mapping images and words into the same representation, we can classify images of classes we\u2019ve never seen!"}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p22", "contents": "Representations and types can be seen as the basic building blocks for deep learning and functional programming respectively. One of the major narratives of deep learning, the manifolds and representations narrative, is entirely centered on neural networks bending data into new representations. The known connection between geometry, logic, topology, and functional programming suggests that the connections between representations and types may be of fundamental significance."}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p23", "contents": "One of the key insights behind modern neural networks is the idea that many copies of one neuron can be used in a neural network."}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p24", "contents": "In programming, the abstraction of functions is essential. Instead of writing the same code dozens, hundreds, or even thousands of times, we can write it once and use it as we need it. Not only does this massively reduce the amount of code we need to write and maintain, speeding the development process, but it also reduces the risk of us introducing bugs, and makes the mistakes we do make easier to catch."}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p25", "contents": "Using multiple copies of a neuron in different places is the neural network equivalent of using functions. Because there is less to learn, the model learns more quickly and learns a better model. This technique \u2013 the technical name for it is \u2018weight tying\u2019 \u2013 is essential to the phenomenal results we\u2019ve recently seen from deep learning."}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p26", "contents": "Of course, one can\u2019t just arbitrarily put copies of neurons all over the place. For the model to work, you need to do it in a principled way, exploiting some structure in your data. In practice, there are a handful of patterns that are widely used, such as recurrent layers and convolutional layers."}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p27", "contents": "These neural network patterns are just higher order functions \u2013 that is, functions which take functions as arguments. Things like that have been studied extensively in functional programming. In fact, many of these network patterns correspond to extremely common functions, like fold. The only unusual thing is that, instead of receiving normal functions as arguments, they receive chunks of neural network.2"}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p28", "contents": "Encoding Recurrent Neural Networks are just folds. They\u2019re often used to allow a neural network to take a variable length list as input, for example taking a sentence as input."}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p29", "contents": "Generating Recurrent Neural Networks are just unfolds. They\u2019re often used to allow a neural network to produce a list of outputs, such as words in a sentence."}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p30", "contents": "General Recurrent Neural Networks are accumulating maps. They\u2019re often used when we\u2019re trying to make predictions in a sequence. For example, in voice recognition, we might wish to predict a phenome for every time step in an audio segment, based on past context."}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p31", "contents": "Bidirectional Recursive Neural Networks are a more obscure variant, which I mention primarily for flavor. In functional programming terms, they are a left and a right accumulating map zipped together. They\u2019re used to make predictions over a sequence with both past and future context."}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p32", "contents": "Convolutional Neural Networks are a close relative of map. A normal map applies a function to every element. Convolutional neural networks also look at neighboring elements, applying a function to a small window around every element.3"}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p33", "contents": "Two dimensional convolutional neural networks are particularly notable. They have been behind recent successes in computer vision. (More on conv nets.)"}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p34", "contents": "Recursive Neural Networks (\u201cTreeNets\u201d) are catamorphisms, a generalization of folds. They consume a data structure from the bottom up. They\u2019re mostly used for natural language processing, to allow neural networks to operate on parse trees."}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p35", "contents": "The above examples demonstrate how common patterns in neural networks correspond to very natural, simple functional programs."}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p36", "contents": "These patterns are building blocks which can be combined together into larger networks. Like the building blocks, these combinations are functional programs, with chunks of neural network throughout. The functional program provides high level structure, while the chunks are flexible pieces that learn to perform the actual task within the framework provided by the functional program."}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p37", "contents": "These combinations of building blocks can do really, really remarkable things. I\u2019d like to look at a few examples."}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p38", "contents": "Sutskever, et al. (2014) perform state of the art English to French translation by combining an encoding RNN and a generating RNN. In functional programming terms, they essentially fold over the (backwards) English sentence and then unfold to produce the French translation."}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p39", "contents": "Vinyals, et al. (2014) generate image captions with a convolutional network and a generating RNN. Essentially, they consume the image with a convolutional network, and then unfold the resulting vector into a sentence describing it."}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p40", "contents": "These sorts of models can be seen as a kind of differentiable functional programming."}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p41", "contents": "But it\u2019s not just an abstract thing! They\u2019re imbued with the flavor of functional programming, even if people don\u2019t use that language. When I hear colleagues talk at a high level about their models, it has a very different feeling to it than people talking about more classical models. People talk about things in lots of different ways, of course \u2013 there\u2019s lots of variance in how people see deep learning \u2013 but there\u2019s often an undercurrent that feels very similar to functional programming conversations."}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p42", "contents": "It feels like a new kind of programming altogether, a kind of differentiable functional programming. One writes a very rough functional program, with these flexible, learnable pieces, and defines the correct behavior of the program with lots of data. Then you apply gradient descent, or some other optimization algorithm. The result is a program capable of doing remarkable things that we have no idea how to create directly, like generating captions describing images."}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p43", "contents": "It\u2019s the natural intersection of functional programming and optimization, and I think it\u2019s beautiful."}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p44", "contents": "I find this idea really beautiful. At the same time, this is a pretty strange article and I feel a bit weird posting it. I\u2019m very strongly presenting a speculative idea with little support behind it besides my own enthusiasm. Honestly, adopting the most objective perspective I can, I expect this idea is wrong, because most untested ideas are wrong. But it could be right, and I think it\u2019s worth talking about."}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p45", "contents": "Beyond that, I\u2019m not really the right person to explore a lot of the directions this suggests \u2013 for example, one of the obvious things to do is to analyze neural networks from a homotopy type theory perspective, but I don\u2019t have the relevant background. This is an idea that\u2019s begging for broader discussion. It really seems like publishing this is the right thing to do."}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p46", "contents": "Finally, I\u2019m hoping this essay might stir up more discussion and thoughts about what deep learning is really about. I think there\u2019s an important discussion waiting to be had."}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p47", "contents": "Besides, what\u2019s the point of writing a blog if I can\u2019t speculate? Hopefully, I\u2019ve been able to appropriately balance my excitement with my uncertainty."}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p48", "contents": "Firstly, I\u2019m incredibly grateful to Greg Corrado and Aaron Courville. They are the deep learning researchers I know who most strongly empathize with these ideas, and I\u2019m really grateful for their intellectual support."}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p49", "contents": "Several other people have had really extended and helpful conversations with me. Sebastian Zany spent several hours talking about type theory and neural networks with me. Michael Nielsen gave thorough feedback on a draft of this essay. Chas Leichner thought really deeply about these ideas, and was very encouraging. A big thank you to the three of them!"}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p50", "contents": "I\u2019m also thankful for the comments of James Koppel, Luke Vilnis, Sam Bowman, Greg Brockman and Rob Gilson. Finally, I\u2019ve spoken with a number of other people about these ideas in the last few months \u2013 thanks to all of them!"}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p51", "contents": "For example, the once ubiquitous sigmoid layer has been substantially replaced by ReLU layers.\u21a9"}
{"id": "https://colah.github.io/posts/2015-09-NN-Types-FP/_p52", "contents": "I think it\u2019s actually kind of surprising that these sort of models are possible at all, and it\u2019s because of a surprising fact. Many higher order functions, given differentiable functions as arguments, produce a function which is differentiable almost everywhere. Further, given the derivatives of argument functions, you can simply use chain rule to calculate the derivative of the resulting function.\u21a9"}
{"id": "https://colah.github.io/posts/2015-02-DataList-Illustrated/_p0", "contents": "Posted on February 12, 2015"}
{"id": "https://colah.github.io/posts/2015-02-DataList-Illustrated/_p1", "contents": "functional programming, Haskell, visualization"}
{"id": "https://colah.github.io/posts/2015-02-DataList-Illustrated/_p2", "contents": "Data.List is a standard Haskell library. It provides lots of really helpful functions for working with lists."}
{"id": "https://colah.github.io/posts/2015-02-DataList-Illustrated/_p3", "contents": "In particular, Data.List provides functions for particular patterns of recursion on lists. Some of them, such as map, are familiar to many programmers. But there are lots of subtle variations, and it\u2019s hard to keep them all straight."}
{"id": "https://colah.github.io/posts/2015-02-DataList-Illustrated/_p4", "contents": "The goal of this blog post is to provide a convenient reference to quickly understand all these different functions, with a couple of pretty pictures."}
{"id": "https://colah.github.io/posts/2015-02-DataList-Illustrated/_p5", "contents": "Folds combine all the elements of a list into a single value. For example, if you want to add up all the elements of a list you could fold the addition function over the list."}
{"id": "https://colah.github.io/posts/2015-02-DataList-Illustrated/_p6", "contents": "There are two choices one needs to make when doing a fold:"}
{"id": "https://colah.github.io/posts/2015-02-DataList-Illustrated/_p7", "contents": "There are four versions of fold, corresponding to each pair of choices."}
{"id": "https://colah.github.io/posts/2015-02-DataList-Illustrated/_p8", "contents": "\nfoldl :: (b -> a -> b)\n"}
{"id": "https://colah.github.io/posts/2015-02-DataList-Illustrated/_p9", "contents": " \n-> b -> [a] -> b\n"}
{"id": "https://colah.github.io/posts/2015-02-DataList-Illustrated/_p10", "contents": "\nfoldr :: (a -> b -> b)\n"}
{"id": "https://colah.github.io/posts/2015-02-DataList-Illustrated/_p11", "contents": " \n-> b -> [a] -> b\n"}
{"id": "https://colah.github.io/posts/2015-02-DataList-Illustrated/_p12", "contents": "\nfoldl1 :: (a -> a -> a)\n"}
{"id": "https://colah.github.io/posts/2015-02-DataList-Illustrated/_p13", "contents": " \n-> [a] -> a\n"}
{"id": "https://colah.github.io/posts/2015-02-DataList-Illustrated/_p14", "contents": "\nfoldr1 :: (a -> a -> a)\n"}
{"id": "https://colah.github.io/posts/2015-02-DataList-Illustrated/_p15", "contents": " \n-> [a] -> a\n"}
{"id": "https://colah.github.io/posts/2015-02-DataList-Illustrated/_p16", "contents": "Scans are kind of like folds with history. As they combine the elements of the input list, they remember every intermediary step and produce a list from those."}
{"id": "https://colah.github.io/posts/2015-02-DataList-Illustrated/_p17", "contents": "There is a scan corresponding to every version of fold."}
{"id": "https://colah.github.io/posts/2015-02-DataList-Illustrated/_p18", "contents": "\nscanl :: (b -> a -> b)\n"}
{"id": "https://colah.github.io/posts/2015-02-DataList-Illustrated/_p19", "contents": " \n-> b -> [a] -> [b]\n"}
{"id": "https://colah.github.io/posts/2015-02-DataList-Illustrated/_p20", "contents": "\nscanr :: (a -> b -> b)\n"}
{"id": "https://colah.github.io/posts/2015-02-DataList-Illustrated/_p21", "contents": " \n-> b -> [a] -> [b]\n"}
{"id": "https://colah.github.io/posts/2015-02-DataList-Illustrated/_p22", "contents": "\nscanl1 :: (a -> a -> a)\n"}
{"id": "https://colah.github.io/posts/2015-02-DataList-Illustrated/_p23", "contents": " \n-> [a] -> [a]\n"}
{"id": "https://colah.github.io/posts/2015-02-DataList-Illustrated/_p24", "contents": "\nscanr1 :: (a -> a -> a)\n"}
{"id": "https://colah.github.io/posts/2015-02-DataList-Illustrated/_p25", "contents": " \n-> [a] -> [a]\n"}
{"id": "https://colah.github.io/posts/2015-02-DataList-Illustrated/_p26", "contents": "Maps look at every element of a list and produce a corresponding output."}
{"id": "https://colah.github.io/posts/2015-02-DataList-Illustrated/_p27", "contents": "The basic map function just looks at the list element, but there are also accumulating maps that either get information from the left or right."}
{"id": "https://colah.github.io/posts/2015-02-DataList-Illustrated/_p28", "contents": "\nmap :: (a -> c) -> [a] -> [c]\n"}
{"id": "https://colah.github.io/posts/2015-02-DataList-Illustrated/_p29", "contents": "\nmapAccumL ::\n"}
{"id": "https://colah.github.io/posts/2015-02-DataList-Illustrated/_p30", "contents": "\n(b -> a -> (b, c)) ->\n"}
{"id": "https://colah.github.io/posts/2015-02-DataList-Illustrated/_p31", "contents": "\nb -> [a] -> (b, [c])\n"}
{"id": "https://colah.github.io/posts/2015-02-DataList-Illustrated/_p32", "contents": "\nmapAccumR ::\n"}
{"id": "https://colah.github.io/posts/2015-02-DataList-Illustrated/_p33", "contents": "\n(b -> a -> (b, c)) ->\n"}
{"id": "https://colah.github.io/posts/2015-02-DataList-Illustrated/_p34", "contents": "\nb -> [a] -> (b, [c])\n"}
{"id": "https://colah.github.io/posts/2015-02-DataList-Illustrated/_p35", "contents": "Accumulating maps are kind of elegant because they contain a lot of other functions. For example, if you let \\(b=()\\), you get regular map. And if you let \\(c=()\\) you get foldr/foldl. Finally, if you let \\(a=()\\), you get something very close to unfold from the following section."}
{"id": "https://colah.github.io/posts/2015-02-DataList-Illustrated/_p36", "contents": "Where folds take a list and produce a single value, unfolds take a single value and produce a list."}
{"id": "https://colah.github.io/posts/2015-02-DataList-Illustrated/_p37", "contents": "Data.List only provides unfoldr. It is called unfoldr because it is the dual of foldr, even though it actually starts on the left."}
{"id": "https://colah.github.io/posts/2015-02-DataList-Illustrated/_p38", "contents": "\nunfoldr :: (b -> Maybe (c, b))\n"}
{"id": "https://colah.github.io/posts/2015-02-DataList-Illustrated/_p39", "contents": " \n-> b -> [c]\n"}
{"id": "https://colah.github.io/../../posts/2015-01-Visualizing-Representations/_p0", "contents": "Bad request"}
{"id": "https://colah.github.io/../../posts/2015-01-Visualizing-Representations/_p1", "contents": "\n        Your browser sent a request that this server could not understand.\n        Sorry about that. Please check your request for errors and try again.\n        Contact us if the problem persists.\n      "}
{"id": "https://colah.github.io/../../posts/2014-12-Groups-Convolution/_p0", "contents": "Bad request"}
{"id": "https://colah.github.io/../../posts/2014-12-Groups-Convolution/_p1", "contents": "\n        Your browser sent a request that this server could not understand.\n        Sorry about that. Please check your request for errors and try again.\n        Contact us if the problem persists.\n      "}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p0", "contents": "Posted on October 14, 2015"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p1", "contents": "I love the feeling of having a new way to think about the world. I especially love when there\u2019s some vague idea that gets formalized into a concrete concept. Information theory is a prime example of this."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p2", "contents": "Information theory gives us precise language for describing a lot of things. How uncertain am I? How much does knowing the answer to question A tell me about the answer to question B? How similar is one set of beliefs to another? I\u2019ve had informal versions of these ideas since I was a young child, but information theory crystallizes them into precise, powerful ideas. These ideas have an enormous variety of applications, from the compression of data, to quantum physics, to machine learning, and vast fields in between."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p3", "contents": "Unfortunately, information theory can seem kind of intimidating. I don\u2019t think there\u2019s any reason it should be. In fact, many core ideas can be explained completely visually!"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p4", "contents": "Before we dive into information theory, let\u2019s think about how we can visualize simple probability distributions. We\u2019ll need this later on, and it\u2019s convenient to address now. As a bonus, these tricks for visualizing probability are pretty useful in and of themselves!"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p5", "contents": "I\u2019m in California. Sometimes it rains, but mostly there\u2019s sun! Let\u2019s say it\u2019s sunny 75% of the time. It\u2019s easy to make a picture of that:"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p6", "contents": "Most days, I wear a t-shirt, but some days I wear a coat. Let\u2019s say I wear a coat 38% of the time. It\u2019s also easy to make a picture for that!"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p7", "contents": "What if I want to visualize both at the same time? We\u2019ll, it\u2019s easy if they don\u2019t interact \u2013 if they\u2019re what we call independent. For example, whether I wear a t-shirt or a raincoat today doesn\u2019t really interact with what the weather is next week. We can draw this by using one axis for one variable and one for the other:"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p8", "contents": "Notice the straight vertical and horizontal lines going all the way through. That\u2019s what independence looks like! 1 The probability I\u2019m wearing a coat doesn\u2019t change in response to the fact that it will be raining in a week. In other words, the probability that I\u2019m wearing a coat and that it will rain next week is just the probability that I\u2019m wearing a coat, times the probability that it will rain. They don\u2019t interact."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p9", "contents": "When variables interact, there\u2019s extra probability for particular pairs of variables and missing probability for others. There\u2019s extra probability that I\u2019m wearing a coat and it\u2019s raining because the variables are correlated, they make each other more likely. It\u2019s more likely that I\u2019m wearing a coat on a day that it rains than the probability I wear a coat on one day and it rains on some other random day."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p10", "contents": "Visually, this looks like some of the squares swelling with extra probability, and other squares shrinking because the pair of events is unlikely together:"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p11", "contents": "But while that might look kind of cool, it\u2019s isn\u2019t very useful for understanding what\u2019s going on."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p12", "contents": "Instead, let\u2019s focus on one variable like the weather. We know how probable it is that it\u2019s sunny or raining. For both cases, we can look at the conditional probabilities. How likely am I to wear a t-shirt if it\u2019s sunny? How likely am I to wear a coat if it\u2019s raining?"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p13", "contents": "There\u2019s a 25% chance that it\u2019s raining. If it is raining, there\u2019s a 75% chance that I\u2019d wear a coat. So, the probability that it is raining and I\u2019m wearing a coat is 25% times 75% which is approximately 19%. The probability that it\u2019s raining and I\u2019m wearing a coat is the probability that it is raining, times the probability that I\u2019d wear a coat if it is raining. We write this:"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p14", "contents": "\\[p(\\text{rain}, \\text{coat}) = p(\\text{rain}) \\cdot p(\\text{coat} ~|~ \\text{rain})\\]"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p15", "contents": "This is a single case of one of the most fundamental identities of probability theory:"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p16", "contents": "\\[p(x,y) = p(x)\\cdot p(y|x)\\]"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p17", "contents": "We\u2019re factoring the distribution, breaking it down into the product of two pieces. First we look at the probability that one variable, like the weather, will take on a certain value. Then we look at the probability that another variable, like my clothing, will take on a certain value conditioned on the first variable."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p18", "contents": "The choice of which variable to start with is arbitrary. We could just as easily start by focusing on my clothing and then look at the weather conditioned on it. This might feel a bit less intuitive, because we understand that there\u2019s a causal relationship of the weather influencing what I wear and not the other way around\u2026 but it still works!"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p19", "contents": "Let\u2019s go through an example. If we pick a random day, there\u2019s a 38% chance that I\u2019d be wearing a coat. If we know that I\u2019m wearing a coat, how likely is it that it\u2019s raining? Well, I\u2019m more likely to wear a coat in the rain than in the sun, but rain is kind of rare in California, and so it works out that there\u2019s a 50% chance that it\u2019s raining. And so, the probability that it\u2019s raining and I\u2019m wearing a coat is the probability that I\u2019m wearing a coat (38%), times the probability that it would be raining if I was wearing a coat (50%) which is approximately 19%."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p20", "contents": "\\[p(\\text{rain}, \\text{coat}) = p(\\text{coat}) \\cdot p(\\text{rain} ~|~ \\text{coat})\\]"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p21", "contents": "This gives us a second way to visualize the exact same probability distribution."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p22", "contents": "Note that the labels have slightly different meanings than in the previous diagram: t-shirt and coat are now marginal probabilities, the probability of me wearing that clothing without consideration of the weather. On the other hand, there are now two rain and sunny labels, for the probabilities of them conditional on me wearing a t-shirt and me wearing a coat respectively."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p23", "contents": "(You may have heard of Bayes\u2019 Theorem. If you want, you can think of it as the way to translate between these two different ways of displaying the probability distribution!)"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p24", "contents": "Are these tricks for visualizing probability distributions actually helpful? I think they are! It will be a little while before we use them for visualizing information theory, so I\u2019d like to go on a little tangent and use them to explore Simpson\u2019s paradox. Simpson\u2019s paradox is an extremely unintuitive statistical situation. It\u2019s just really hard to understand at an intuitive level. Michael Nielsen wrote a lovely essay, Reinventing Explanation, which explored different ways to explain it. I\u2019d like to try and take my own shot at it, using the tricks we developed in the previous section."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p25", "contents": "Two treatments for kidney stones are tested. Half the patients are given treatment A while the other half are given treatment B. The patients who received treatment B were more likely to survive than those who received treatment A."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p26", "contents": "However, patients with small kidney stones were more likely to survive if they took treatment A. Patients with large kidney stones were also more likely to survive if they took treatment A! how can this be?"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p27", "contents": "The core of the issue is that the study wasn\u2019t properly randomized. The patients who received treatment A were likely to have large kidney stones, while the patients who received treatment B were more likely to have small kidney stones."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p28", "contents": "As it turns out, patients with small kidney stones are much more likely to survive in general."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p29", "contents": "To understand this better, we can combine the two previous diagrams. The result is a 3-dimensional diagram with the survival rate split apart for small and large kidney stones."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p30", "contents": "We can now see that in both the small case and the large case, Treatment A beats Treatment B. Treatment B only seemed better because the patients it was applied to were more likely to survive in the first place!"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p31", "contents": "Now that we have ways of visualizing probability, we can dive into information theory."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p32", "contents": "Let me tell you about my imaginary friend, Bob. Bob really likes animals. He constantly talks about animals. In fact, he only ever says four words: \u201cdog\u201d, \u201ccat\u201d, \u201cfish\u201d and \u201cbird\u201d."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p33", "contents": "A couple weeks ago, despite being a figment of my imagination, Bob moved to Australia. Further, he decided he only wanted to communicate in binary. All my (imaginary) messages from Bob look like this:"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p34", "contents": "To communicate, Bob and I have to establish a code, a way of mapping words into sequences of bits."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p35", "contents": "To send a message, Bob replaces each symbol (word) with the corresponding codeword, and then concatenates them together to form the encoded string."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p36", "contents": "Unfortunately, communication services in imaginary-Australia are expensive. I have to pay $5 per bit of every message I receive from Bob. Have I mentioned that Bob likes to talk a lot? To prevent me from going bankrupt, Bob and I decided we should investigate whether there was some way we could make our average message length shorter."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p37", "contents": "As it turns out Bob doesn\u2019t say all words equally often. Bob really likes dogs. He talks about dogs all the time. On occasion, he\u2019ll talk about other animals \u2013 especially the cat his dog likes to chase \u2013 but mostly he talks about dogs. Here\u2019s a graph of his word frequency:"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p38", "contents": "That seems promising. Our old code uses codewords that are 2 bits long, regardless of how common they are."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p39", "contents": "There\u2019s a nice way to visualize this. In the following diagram, we use the vertical axis to visualize the probability of each word, \\(p(x)\\), and the horizontal axis to visualize the length of the corresponding codeword, \\(L(x)\\). Notice that the area is the average length of a codeword we send \u2013 in this case 2 bits."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p40", "contents": "Perhaps we could be very clever and make a variable-length code where codewords for common words are made especially short. The challenge is that there\u2019s competition between codewords \u2013 making some shorter forces us to make others longer. To minimize the message length, we\u2019d ideally like all codewords to be short, but we especially want the commonly used ones to be. So the resulting code has shorter codewords for common words (like \u201cdog\u201d) and longer codewords for less common words (like \u201cbird\u201d)."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p41", "contents": "Let\u2019s visualize this again. Notice that the most common codeword became shorter, even as the uncommon ones became longer. The result was, on net, a smaller amount of area. This corresponds to a smaller expected codeword length. On average, the length of a codeword is now 1.75 bits!"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p42", "contents": "(You may wonder: why not use 1 by itself as a codeword? Sadly, this would cause ambiguity when we decode encoded strings. We\u2019ll talk about this more shortly.)"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p43", "contents": "It turns out that this code is the best possible code. There is no code which, for this distribution, will give us an average codeword length of less than 1.75 bits."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p44", "contents": "There is simply a fundamental limit. Communicating what word was said, what event from this distribution occurred, requires us to communicate at least 1.75 bits on average. No matter how clever our code, it\u2019s impossible to get the average message length to be less. We call this fundamental limit the entropy of the distribution \u2013 we\u2019ll discuss it in much more detail shortly."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p45", "contents": "If we want to understand this limit, the crux of the matter is understanding the trade off between making some codewords short and others long. Once we understand that, we\u2019ll be able to understand what the best possible codes are like."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p46", "contents": "There are two codes with a length of 1 bit: 0 and 1. There are four codes with a length of 2 bits: 00, 01, 10, and 11. Every bit you add on doubles the number of possible codes."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p47", "contents": "We\u2019re interested in variable-length codes, where some codewords are longer than others. We might have simple situations where we have eight codewords that are 3 bits long. We might also have more complicated mixtures, like two codewords of length 2, and four codewords of length 3. What decides how many codewords we can have of different lengths?"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p48", "contents": "Recall that Bob turns his messages into encoded strings by replacing each word with its codeword and concatenating them."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p49", "contents": "There\u2019s a slightly subtle issue one needs to be careful of, when crafting a variable length code. How do we split the encoded string back into the codewords? When all the codewords are the same length, it\u2019s easy \u2013 just split the string every couple of steps. But since there are codewords of different lengths, we need to actually pay attention to the content."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p50", "contents": "We really want our code to be uniquely decodable, with only one way to decode an encoded string. We never want it to be ambiguous which codewords make up the encoded string. If we had some special \u201cend of codeword\u201d symbol, this would be easy.2 But we don\u2019t \u2013 we\u2019re only sending 0s and 1s. We need to be able to look at a sequence of concatenated codewords and tell where each one stops."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p51", "contents": "It\u2019s very possible to make codes that aren\u2019t uniquely decodable. For example, imagine that 0 and 01 were both codewords. Then it would be unclear what the first codeword of the encoded string 0100111 is \u2013 it could be either! The property we want is that if we see a particular codeword, there shouldn\u2019t be some longer version that is also a codeword. Another way of putting this is that no codeword should be the prefix of another codeword. This is called the prefix property, and codes that obey it are called prefix codes."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p52", "contents": "One useful way to think about this is that every codeword requires a sacrifice from the space of possible codewords. If we take the codeword 01, we lose the ability to use any codewords it\u2019s a prefix of. We can\u2019t use 010 or 011010110 anymore because of ambiguity \u2013 they\u2019re lost to us."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p53", "contents": "Since a quarter of all codewords start with 01, we\u2019ve sacrificed a quarter of all possible codewords. That\u2019s the price we pay in exchange for having one codeword that\u2019s only 2 bits long! In turn this sacrifice means that all the other codewords need to be a bit longer. There\u2019s always this sort of trade off between the lengths of the different codewords. A short codeword requires you to sacrifice more of the space of possible codewords, preventing other codewords from being short. What we need to figure out is what the right trade off to make is!"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p54", "contents": "You can think of this like having a limited budget to spend on getting short codewords. We pay for one codeword by sacrificing a fraction of possible codewords."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p55", "contents": "The cost of buying a codeword of length 0 is 1, all possible codewords \u2013 if you want to have a codeword of length 0, you can\u2019t have any other codeword. The cost of a codeword of length 1, like \u201c0\u201d, is 1/2 because half of possible codewords start with \u201c0\u201d. The cost of a codeword of length 2, like \u201c01\u201d, is 1/4 because a quarter of all possible codewords start with \u201c01\u201d. In general, the cost of codewords decreases exponentially with the length of the codeword."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p56", "contents": "Note that if the cost decays as a (natural) exponential, it is both the height and the area!3"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p57", "contents": "We want short codewords because we want short average message lengths. Each codeword makes the average message length longer by its probability times the length of the codeword. For example, if we need to send a codeword that is 4 bits long 50% of the time, our average message length is 2 bits longer than it would be if we weren\u2019t sending that codeword. We can picture this as a rectangle."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p58", "contents": "These two values are related by the length of the codeword. The amount we pay decides the length of the codeword. The length of the codeword controls how much it adds to the average message length. We can picture the two of these together, like so."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p59", "contents": "Short codewords reduce the average message length but are expensive, while long codewords increase the average message length but are cheap."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p60", "contents": "What\u2019s the best way to use our limited budget? How much should we spend on the codeword for each event?"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p61", "contents": "Just like one wants to invest more in tools that one uses regularly, we want to spend more on frequently used codewords. There\u2019s one particularly natural way to do this: distribute our budget in proportion to how common an event is. So, if one event happens 50% of the time, we spend 50% of our budget buying a short codeword for it. But if an event only happens 1% of the time, we only spend 1% of our budget, because we don\u2019t care very much if the codeword is long."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p62", "contents": "That\u2019s a pretty natural thing to do, but is it the optimal thing to do? It is, and I\u2019ll prove it!"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p63", "contents": "The following proof is visual and should be accessible, but will take work to get through and is definitely the hardest part of this essay. Readers should feel free to skip to accept this as a given and jump to the next section."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p64", "contents": "Let\u2019s picture a concrete example where we need to communicate which of two possible events happened. Event \\(a\\) happens \\(p(a)\\) of the time and event \\(b\\) happens \\(p(b)\\) of the time. We distribute our budget in the natural way described above, spending \\(p(a)\\) of our budget on getting \\(a\\) a short codeword, and \\(p(b)\\) on getting \\(b\\) a short codeword."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p65", "contents": "The cost and length contribution boundaries nicely line up. Does that mean anything?"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p66", "contents": "Well, consider what happens to the cost and the length contribution if we slightly change the length of the codeword. If we slightly increase the length of the codeword, the message length contribution will increase in proportion to its height at the boundary, while the cost will decrease in proportion to its height at the boundary."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p67", "contents": "So, the cost to make the codeword for \\(a\\) shorter is \\(p(a)\\). At the same time, we don\u2019t care about the length of each codeword equally, we care about them in proportion to how much we have to use them. In the case of \\(a\\), that is \\(p(a)\\). The benefit to us of making the codeword for \\(a\\) a bit shorter is \\(p(a)\\)."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p68", "contents": "It\u2019s interesting that both derivatives are the same. It means that our initial budget has the interesting property that, if you had a bit more to spend, it would be equally good to invest in making any codeword shorter. What we really care about, in the end, is the benefit/cost ratio \u2013 that\u2019s what decides what we should invest more in. In this case, the ratio is \\(\\frac{p(a)}{p(a)}\\), which is equal to one. This is independent of the value of \\(p(a)\\) \u2013 it\u2019s always one. And we can apply the same argument to other events. The benefit/cost is always one, so it makes equal sense to invest a bit more in any of them."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p69", "contents": "Infinitesimally, it doesn\u2019t make sense to change the budget. But that isn\u2019t a proof that it\u2019s the best budget. To prove that, we\u2019ll consider a different budget, where we spend a bit extra on one codeword at the expense of another. We\u2019ll invest \\(\\epsilon\\) less in \\(b\\), and invest it in \\(a\\) instead. This makes the codeword for \\(a\\) a bit shorter, and the codeword for \\(b\\) a bit longer."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p70", "contents": "Now the cost of buying a shorter codeword for \\(a\\) is \\(p(a) + \\epsilon\\), and the cost of buying a shorter codeword for \\(b\\) is \\(p(b) - \\epsilon\\). But the benefits are still the same. This leads the benefit cost ratio for buying \\(a\\) to be \\(\\frac{p(a)}{p(a) + \\epsilon}\\) which is less than one. On the other hand, the benefit cost ratio of buying \\(b\\) is \\(\\frac{p(b)}{p(b) - \\epsilon}\\) which is greater than one."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p71", "contents": "The prices are no longer balanced. \\(b\\) is a better deal than \\(a\\). The investors scream: \u201cBuy \\(b\\)! Sell \\(a\\)!\u201d We do this, and end back at our original budget plan. All budgets can be improved by shifting towards our original plan."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p72", "contents": "The original budget \u2013 investing in each codeword in proportion to how often we use it \u2013 wasn\u2019t just the natural thing to do, it was the optimal thing to do. (While this proof only works for two codewords, it easily generalizes to more.)"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p73", "contents": "(A careful reader may have noticed that it is possible for our optimal budget to suggest codes where codewords have fractional lengths. That seems pretty concerning! What does it mean? Well, of course, in practice, if you want to communicate by sending a single codeword, you have to round. But as we\u2019ll see later, there\u2019s a very real sense in which it is possible to send fractional codewords when we send many at a time! I\u2019ll ask you be patient with me on this for now!)"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p74", "contents": "Recall that the cost of a message of length \\(L\\) is \\(\\frac{1}{2^L}\\). We can invert this to get the length of a message that costs a given amount: \\(\\log_2\\left(\\frac{1}{\\text{cost}}\\right)\\). Since we spend \\(p(x)\\) on a codeword for \\(x\\), the length is \\(\\log_2\\left(\\frac{1}{p(x)}\\right)\\). Those are the best choices of lengths."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p75", "contents": "Earlier, we discussed how there is a fundamental limit to how short one can get the average message to communicate events from a particular probability distribution, \\(p\\). This limit, the average message length using the best possible code, is called the entropy of \\(p\\), \\(H(p)\\). Now that we know the optimal lengths of the codewords, we can actually calculate it!"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p76", "contents": "\\[H(p) = \\sum_x p(x)\\log_2\\left(\\frac{1}{p(x)}\\right)\\]"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p77", "contents": "(People often write entropy as \\(H(p) = - \\sum p(x)\\log_2(p(x))\\) using the identity \\(\\log(1/a) = -\\log(a)\\). I think the first version is more intuitive, and will continue to use it in this essay.)"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p78", "contents": "No matter what I do, on average I need to send at least that number of bits if I want to communicate which event occurred."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p79", "contents": "The average amount of information needed to communicate something has clear implications for compression. But are there other reasons we should care about it? Yes! It describes how uncertain I am and gives a way to quantify information."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p80", "contents": "If I knew for sure what was going to happen, I wouldn\u2019t have to send a message at all! If there\u2019s two things that could happen with 50% probability, I only need to send 1 bit. But if there\u2019s 64 different things that could happen with equal probability, I\u2019d have to send 6 bits. The more concentrated the probability, the more I can craft a clever code with short average messages. The more diffuse the probability, the longer my messages have to be."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p81", "contents": "The more uncertain the outcome, the more I learn, on average, when I find out what happened."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p82", "contents": "Shortly before his move to Australia, Bob married Alice, another figment of my imagination. To the surprise of myself, and also the other characters in my head, Alice was not a dog lover. She was a cat lover. Despite this, the two of them were able to find common ground in their shared obsession with animals and very limited vocabulary size."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p83", "contents": "The two of them say the same words, just at different frequencies. Bob talks about dogs all the time, Alice talks about cats all the time."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p84", "contents": "Initially, Alice sent me messages using Bob\u2019s code. Unfortunately, her messages were longer than they needed to be. Bob\u2019s code was optimized to his probability distribution. Alice has a different probability distribution, and the code is suboptimal for it. While the average length of a codeword when Bob uses his own code is 1.75 bits, when Alice uses his code it's 2.25. It would be worse if the two weren\u2019t so similar!"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p85", "contents": "This length \u2013 the average length of communicating an event from one distribution with the optimal code for another distribution \u2013 is called the cross-entropy. Formally, we can define cross-entropy as:4 \\[H_p(q) = \\sum_x q(x)\\log_2\\left(\\frac{1}{p(x)}\\right)\\]"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p86", "contents": "In this case, it\u2019s the cross-entropy of Alice the cat-lovers word frequency with respect to the Bob the dog-lovers word frequency."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p87", "contents": "To keep the cost of our communications down, I asked Alice to use her own code. To my relief, this pushed down her average message length. But it introduced a new problem: sometimes Bob would accidentally use Alice\u2019s code. Surprisingly, it\u2019s worse for Bob to accidentally use Alice's code than for Alice to use his!"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p88", "contents": "So, now we have four possibilities:"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p89", "contents": "This isn\u2019t necessarily as intuitive as one might think. For example, we can see that \\(H_p(q) \\neq H_q(p)\\). Is there some way we can see how these four values relate to each other?"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p90", "contents": "In the following diagram, each subplot represents one of these 4 possibilities. Each subplot visualizes average message length the same way our previous diagrams did. They are organized in a square, so that if the messages are coming from the same distribution the plots are beside each other, and if they use the same codes they are on top of each other. This allows you to kind of visually slide the distributions and codes together."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p91", "contents": "Can you see why \\(H_p(q) \\neq H_q(p)\\)? \\(H_q(p)\\) is large because there is an event (blue) which is very common under \\(p\\) but gets a long code because it is very uncommon under \\(q\\). On the other hand, common events under \\(q\\) are less common under \\(p\\), but the difference is less drastic, so \\(H_p(q)\\) isn\u2019t as high."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p92", "contents": "Cross-entropy isn\u2019t symmetric."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p93", "contents": "So, why should you care about cross-entropy? Well, cross-entropy gives us a way to express how different two probability distributions are. The more different the distributions \\(p\\) and \\(q\\) are, the more the cross-entropy of \\(p\\) with respect to \\(q\\) will be bigger than the entropy of \\(p\\)."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p94", "contents": "Similarly, the more different \\(p\\) is from \\(q\\), the more the cross-entropy of \\(q\\) with respect to \\(p\\) will be bigger than the entropy of \\(q\\)."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p95", "contents": "The really interesting thing is the difference between the entropy and the cross-entropy. That difference is how much longer our messages are because we used a code optimized for a different distribution. If the distributions are the same, this difference will be zero. As the difference grows, it will get bigger."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p96", "contents": "We call this difference the Kullback\u2013Leibler divergence, or just the KL divergence. The KL divergence of \\(p\\) with respect to \\(q\\), \\(D_q(p)\\),5 is defined:6"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p97", "contents": "\\[D_q(p) = H_q(p) - H(p)\\]"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p98", "contents": "The really neat thing about KL divergence is that it\u2019s like a distance between two distributions. It measures how different they are! (If you take that idea seriously, you end up with information geometry.)"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p99", "contents": "Cross-Entropy and KL divergence are incredibly useful in machine learning. Often, we want one distribution to be close to another. For example, we might want a predicted distribution to be close to the ground truth. KL divergence gives us a natural way to do this, and so it shows up everywhere."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p100", "contents": "Let\u2019s return to our weather and clothing example from earlier:"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p101", "contents": "My mother, like many parents, sometimes worries that I don\u2019t dress appropriately for the weather. (She has reasonable cause for suspicion \u2013 I have often failed to wear coats in winter.) So, she often wants to know both the weather and what clothing I\u2019m wearing. How many bits do I have to send her to communicate this?"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p102", "contents": "Well, the easy way to think about this is to flatten the probability distribution:"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p103", "contents": "Now we can figure out the optimal codewords for events of these probabilities and compute the average message length:"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p104", "contents": "We call this the joint entropy of \\(X\\) and \\(Y\\), defined"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p105", "contents": "\\[H(X,Y) = \\sum_{x,y} p(x,y) \\log_2\\left(\\frac{1}{p(x,y)}\\right)\\]"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p106", "contents": "This is the exact same as our normal definition, except with two variables instead of one."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p107", "contents": "A slightly nicer way to think about this is to avoid flattening the distribution, and just think of the code lengths as a third dimension. Now the entropy is the volume!"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p108", "contents": "But suppose my mom already knows the weather. She can check it on the news. Now how much information do I need to provide?"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p109", "contents": "It seems like I need to send however much information I need to communicate the clothes I\u2019m wearing. But I actually need to send less, because the weather strongly implies what clothing I\u2019ll wear! Let\u2019s consider the case where it\u2019s raining and where it\u2019s sunny separately."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p110", "contents": "In both cases, I don\u2019t need to send very much information on average, because the weather gives me a good guess at what the right answer will be. When it\u2019s sunny, I can use a special sunny-optimized code, and when it\u2019s raining I can use a raining optimized code. In both cases, I send less information than if I used a generic code for both. To get the average amount of information I need to send my mother, I just put these two cases together\u2026"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p111", "contents": "We call this the conditional entropy. If you formalize it into an equation, you get:"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p112", "contents": "\\[H(X|Y) = \\sum_y p(y) \\sum_x p(x|y) \\log_2\\left(\\frac{1}{p(x|y)}\\right)\\] \\[~~~~ = \\sum_{x,y} p(x,y) \\log_2\\left(\\frac{1}{p(x|y)}\\right)\\]"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p113", "contents": "In the previous section, we observed that knowing one variable can mean that communicating another variable requires less information."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p114", "contents": "One nice way to think about this is to imagine amounts of information as bars. These bars overlap if there\u2019s shared information between them. For example, some of the information in \\(X\\) and \\(Y\\) is shared between them, so \\(H(X)\\) and \\(H(Y)\\) are overlapping bars. And since \\(H(X,Y)\\) is the information in both, it\u2019s the union of the bars \\(H(X)\\) and \\(H(Y)\\).7"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p115", "contents": "Once we think about things this way, a lot of things become easier to see."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p116", "contents": "For example, we previously noted it takes more information to communicate both \\(X\\) and \\(Y\\) (the \u201cjoint entropy,\u201d \\(H(X,Y)\\)) than it takes to just communicate \\(X\\) (the \u201cmarginal entropy,\u201d \\(H(X)\\)). But if you already know \\(Y\\), then it takes less information to communicate \\(X\\) (the \u201cconditional entropy,\u201d \\(H(X|Y)\\)) than it would if you didn\u2019t!"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p117", "contents": "That sounds a bit complicated, but it\u2019s very simple when we think about it from the bar perspective. \\(H(X|Y)\\) is the information we need to send to communicate \\(X\\) to someone who already knows \\(Y\\), the information in \\(X\\) which isn\u2019t also in \\(Y\\). Visually, that means \\(H(X|Y)\\) is the part of \\(H(X)\\) bar which doesn\u2019t overlap with \\(H(Y)\\)."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p118", "contents": "You can now read the inequality \\(H(X,Y) \\geq H(X) \\geq H(X|Y)\\) right off the following diagram."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p119", "contents": "Another identity is that \\(H(X,Y) = H(Y) + H(X|Y)\\). That is, the information in \\(X\\) and \\(Y\\) is the information in \\(Y\\) plus the information in \\(X\\) which is not in \\(Y\\)."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p120", "contents": "Again, it\u2019s difficult to see in the equations, but easy to see if you\u2019re thinking in terms of these overlapping bars of information."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p121", "contents": "At this point, we\u2019ve broken the information in \\(X\\) and \\(Y\\) up in several ways. We have the information in each variable, \\(H(X)\\) and \\(H(Y)\\). We have the the union of the information in both, \\(H(X,Y)\\). We have the information which is in one but not the other, \\(H(X|Y)\\) and \\(H(Y|X)\\). A lot of this seems to revolve around the information shared between the variables, the intersection of their information. We call this \u201cmutual information,\u201d \\(I(X,Y)\\), defined as:8 \\[I(X,Y) = H(X) + H(Y) - H(X,Y)\\] This definition works because \\(H(X) + H(Y)\\) has two copies of the mutual information, since it\u2019s in both \\(X\\) and \\(Y\\), while \\(H(X,Y)\\) only has one. (Consider the previous bar diagram.)"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p122", "contents": "Closely related to the mutual information is the variation of information. The variation of information is the information which isn\u2019t shared between the variables. We can define it like so: \\[V(X,Y) = H(X,Y) - I(X,Y)\\] Variation of information is interesting because it gives us a metric, a notion of distance, between different variables. The variation of information between two variables is zero if knowing the value of one tells you the value of the other and increases as they become more independent."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p123", "contents": "How does this relate to KL divergence, which also gave us a notion of distance? Well, KL divergence gives us a distance between two distributions over the same variable or set of variables. In contrast, variation of information gives us distance between two jointly distributed variables. KL divergence is between distributions, variation of information within a distribution."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p124", "contents": "We can bring this all together into a single diagram relating all these different kinds of information:"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p125", "contents": "A very unintuitive thing about information theory is that we can have fractional numbers of bits. That\u2019s pretty weird. What does it mean to have half a bit?"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p126", "contents": "Here\u2019s the easy answer: often, we\u2019re interested in the average length of a message rather than any particular message length. If half the time one sends a single bit, and half the time one sends two bits, on average one sends one and a half bits. There\u2019s nothing strange about averages being fractional."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p127", "contents": "But that answer is really dodging the issue. Often, the optimal lengths of codewords are fractional. What do those mean?"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p128", "contents": "To be concrete, let\u2019s consider a probability distribution where one event, \\(a\\), happens 71% of the time and another event, \\(b\\), occurs 29% of the time."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p129", "contents": "The optimal code would use 0.5 bits to represent \\(a\\), and 1.7 bits to represent \\(b\\). Well, if we want to send a single one of these codewords, it simply isn\u2019t possible. We\u2019re forced to round to a whole number of bits, and send on average 1 bit."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p130", "contents": "\u2026 But if we\u2019re sending multiple messages at once, it turns out that we can do better. Let\u2019s consider communicating two events from this distribution. If we sent them independently, we\u2019d need to send two bits. Can we do better?"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p131", "contents": "Half the time, we need to communicate \\(aa\\), \\(21\\%\\) of the time we need to send \\(ab\\) or \\(ba\\), and \\(8\\%\\) of the time we need to communicate \\(bb\\). Again, the ideal code involves fractional numbers of bits."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p132", "contents": "If we round the codeword lengths, we\u2019ll get something like this:"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p133", "contents": "This codes give us an average message length of 1.8 bits. That\u2019s less than the 2 bits when we send them independently. Another way of thinking of this is that we\u2019re sending 0.9 bits on average for each event. If we were to send more events at once, it would become smaller still. As \\(n\\) tends to infinity, the overhead due to rounding our code would vanish, and the number of bits per codeword would approach the entropy."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p134", "contents": "Further, notice that the ideal codeword length for \\(a\\) was 0.5 bits, and the ideal codeword length for \\(aa\\) was 1 bit. Ideal codeword lengths add, even when they\u2019re fractional! So, if we communicate a lot of events at once, the lengths will add."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p135", "contents": "There is a very real sense in which one can have fractional numbers of bits of information, even if actual codes can only use whole numbers."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p136", "contents": "(In practice, people use particular coding schemes which are efficient to different extents. Huffman coding, which is basically the kind of code we've sketched out here, doesn't handle fractional bits very gracefully -- you have to group symbols, like we did above, or use more complicated tricks to approach the entropy limit. Arithmetic coding is a bit different, but elegantly handles fractional bits to be asymptotically optimal.)"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p137", "contents": "If we care about communicating in a minimum number of bits, these ideas are clearly fundamental. If we care about compressing data, information theory addresses the core questions and gives us the fundamentally right abstractions. But what if we don\u2019t care \u2013 are they anything other than curiosities?"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p138", "contents": "Ideas from information theory turn up in lots of contexts: machine learning, quantum physics, genetics, thermodynamics, and even gambling. Practitioners in these fields typically don\u2019t care about information theory because they want to compress information. They care because it has a compelling connection to their field. Quantum entanglement can be described with entropy.9 Many results in statistical mechanics and thermodynamics can be derived by assuming maximum entropy about the things you don\u2019t know.10 A gambler\u2019s wins or losses are directly connected to KL divergence, in particular iterated setups.11"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p139", "contents": "Information theory turns up in all these places because it offers concrete, principled formalizations for many things we need to express. It gives us ways of measuring and expressing uncertainty, how different two sets of beliefs are, and how much an answer to one question tells us about others: how diffuse probability is, the distance between probability distributions, and how dependent two variables are. Are there alternative, similar ideas? Sure. But the ideas from information theory are clean, they have really nice properties, and a principled origin. In some cases, they\u2019re precisely what you care about, and in other cases they\u2019re a convenient proxy in a messy world."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p140", "contents": "Machine learning is what I know best, so let\u2019s talk about that for a minute. A very common kind of task in machine learning is classification. Let\u2019s say we want to look at a picture and predict whether it\u2019s a picture of a dog or a cat. Our model might say something like \u201cthere\u2019s a 80% chance this image is a dog, and a 20% chance it\u2019s a cat.\u201d Let\u2019s say the correct answer is dog \u2013 how good or bad is it that we only said there was an 80% chance it was a dog? How much better would it have been to say 85%?"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p141", "contents": "This is an important question because we need some notion of how good or bad our model is, in order to optimize it to do well. What should we optimize? The correct answer really depends on what we\u2019re using the model for: Do we only care about whether the top guess was right, or do we care about how confident we are in the correct answer? How bad is it to be confidently wrong? There isn\u2019t one right answer to this. And often it isn\u2019t possible to know the right answer, because we don\u2019t know how the model will be used in a precise enough way to formalize what we ultimately care about. The result is that there are situations where cross-entropy really is precisely what we care about, but that isn\u2019t always the case. Much more often we don\u2019t know exactly what we care about and cross-entropy is a really nice proxy.12"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p142", "contents": "Information gives us a powerful new framework for thinking about the world. Sometimes it perfectly fits the problem at hand; other times it\u2019s not an exact fit, but still extremely useful. This essay has only scratched the surface of information theory \u2013 there are major topics, like error-correcting codes, that we haven\u2019t touched at all \u2013 but I hope I\u2019ve shown that information theory is a beautiful subject that doesn\u2019t need to be intimidating."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p143", "contents": "To help me become a better writer, please consider filling out this feedback form."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p144", "contents": "Claude Shannon\u2019s original paper on information theory, A Mathematical Theory of Communication, is remarkably accessible. (This seems to be a recurring pattern in early information theory papers. Was it the era? A lack of page limits? A culture emanating from Bell Labs?)"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p145", "contents": "Cover & Thomas\u2019 Elements of Information Theory seems to be the standard reference. I found it helpful."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p146", "contents": "I\u2019m very grateful to Dan Man\u00e9, David Andersen, Emma Pierson and Dario Amodei for taking time to give really incredibly detailed and extensive comments on this essay. I\u2019m also grateful for the comments of Michael Nielsen, Greg Corrado, Yoshua Bengio, Aaron Courville, Nick Beckstead, Jon Shlens, Andrew Dai, Christian Howard, and Martin Wattenberg."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p147", "contents": "Thanks also to my first two neural network seminar series for acting as guinea pigs for these ideas."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p148", "contents": "Finally, thanks to the readers who caught errors and omissions. In particular, thanks to Connor Zwick, Kai Arulkumaran, Jonathan Heusser, Otavio Good, and an anonymous commenter."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p154", "contents": "It\u2019s fun to use this to visualize naive Bayesian classifiers, which assume independence\u2026\u21a9"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p155", "contents": "But horribly inefficient! If we have an extra symbol to use in our codes, only using it at the end of codewords like this would be a terrible waste.\u21a9"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p156", "contents": "I\u2019m cheating a little here. I\u2019ve been using an exponential of base 2 where this is not true, and am going to switch to a natural exponential. This saves us having a lot of \\(log(2)\\)s in our proof, and makes it visually a lot nicer.\u21a9"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p157", "contents": "Note that this notation for cross-entropy is non-standard. The normal notation is \\(H(p,q)\\). This notation is horrible for two reasons. Firstly, the exact same notation is also used for joint entropy. Secondly, it makes it seem like cross-entropy is symmetric. This is ridiculous, and I\u2019ll be writing \\(H_q(p)\\) instead.\u21a9"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p158", "contents": "Also non-standard notation.\u21a9"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p159", "contents": "If you expand the definition of KL divergence, you get: \\[D_q(p) = \\sum_x p(x)\\log_2\\left(\\frac{p(x)}{q(x)} \\right)\\] That might look a bit strange. How should we interpret it? Well, \\(\\log_2\\left(\\frac{p(x)}{q(x)} \\right)\\) is just the difference between how many bits a code optimized for \\(q\\) and a code optimized for \\(p\\) would use to represent \\(x\\). The expression as a whole is the expected difference in how many bits the two codes would use.\u21a9"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p160", "contents": "This builds off the set interpretation of information theory layed out in Raymond W. Yeung\u2019s paper A New Outlook on Shannon\u2019s Information Measures.\u21a9"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p161", "contents": "If you expand the definition of mutual information out, you get:"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p162", "contents": "\\[I(X,Y) = \\sum_{x,y} p(x,y) \\log_2\\left(\\frac{p(x,y)}{p(x)p(y)} \\right)\\]"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p163", "contents": "That looks suspiciously like KL divergence!"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p164", "contents": "What\u2019s going on? Well, it is KL divergence. It\u2019s the KL divergence of P(X,Y) and its naive approximation P(X)P(Y). That is, it\u2019s the number of bits you save representing \\(X\\) and \\(Y\\) if you understand the relationship between them instead of assuming they\u2019re independent."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p165", "contents": "One cute way to visualize this is to literally picture the ratio between a distribution and its naive approximation:"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p166", "contents": "There\u2019s an entire field of quantum information theory. I know precisely nothing about the subject, but I\u2019d bet with extremely high confidence, based on Michael\u2019s other work, that Michael Nielsen and Issac Chuang\u2019s Quantum Computation and Quantum Information is an excellent introduction.\u21a9"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p167", "contents": "As someone who knows nothing about statistical physics, I\u2019ll very nervously try to sketch its connection to information theory as I understand it."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p168", "contents": "After Shannon discovered information theory, many noted suspicious similarities between equations in thermodynamics and equations in information theory. E.T. Jaynes found a very deep and principled connection. Suppose you have some system, and take some measurements like the pressure and temperature. How probable should you think a particular state of the system is? Jaynes suggested we should assume the probability distribution which, subject to the constraints of our measurement, maximizes the entropy. (Note that this \u201cprinciple of maximum entropy\u201d is much more general than physics!) That is, we should assume the possibility with the most unknown information. Many results can be derived from this perspective."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p169", "contents": "(Reading the first few sections of Jaynes\u2019 papers (part 1, part 2) I was impressed by how accessible they seem.)"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p170", "contents": "If you\u2019re interested in this connection but don\u2019t want to work through the original papers, there\u2019s a section in Cover & Thomas which derives a statistical version of the Second Law of Thermodynamics from Markov Chains!\u21a9"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p171", "contents": "The connection between information theory and gambling was originally laid out by John Kelly in his paper \u2018A New Interpretation of Information Rate.\u2019 It\u2019s a remarkably accessible paper, although it requires a few ideas we didn\u2019t develop in this essay."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p172", "contents": "Kelly had an interesting motivation for his work. He noticed that entropy was being used in many cost functions which had no connection to encoding information and wanted some principled reason for it. In writing this essay, I\u2019ve been troubled by the same thing, and have really appreciated Kelly\u2019s work as an additional perspective. That said, I don\u2019t find it completely convincing: Kelly only winds up with entropy because he considers iterated betting where one reinvests all their capital each bet. Different setups do not lead to entropy."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p173", "contents": "A nice discussion of Kelly\u2019s connection between betting and information theory can be found in the standard reference on information theory, Cover & Thomas\u2019 \u2018Elements of Information Theory.\u2019\u21a9"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p174", "contents": "It doesn\u2019t resolve the issue, but I can\u2019t resist offering a small further defense of KL divergence."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p175", "contents": "There\u2019s a result which Cover & Thomas call Stein\u2019s Lemma, although it seems unrelated to the result generally called Stein\u2019s Lemma. At a high level, it goes like this:"}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p176", "contents": "Suppose you have some data which you know comes from one of two probability distributions. How confidently can you determine which of the two distributions it came from? In general, as you get more data points, your confidence should increase exponentially. For example, on average you might become 1.5 times as confident about which distribution is the truth for every data point you see."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p177", "contents": "How much your confidence gets multiplied depends on how different the distributions are. If they are very different, you might very quickly become confident. But if they\u2019re only very slightly different, you might need to see lots of data before you have even a mildly confident answer."}
{"id": "https://colah.github.io/posts/2015-09-Visual-Information/_p178", "contents": "Stein\u2019s Lemma says, roughly, that the amount you multiply by is controlled by the KL divergence. (There\u2019s some subtlety about the trade off between false-positives and false-negatives.) This seems like a really good reason to care about KL divergence!\u21a9"}
{"id": "https://colah.github.io/posts/2019-05-Collaboration/_p0", "contents": "Posted on May 30, 2019"}
{"id": "https://colah.github.io/posts/2019-05-Collaboration/_p1", "contents": "\nTL;DR: This essay makes a lot of suggestions, but the most useful/non-obvious/actionable are likely:\n  (1) Be generous.\n  (2) Use author contribution statements.\n  (3) Put \"author order not finalized\" if it hasn't been.\n"}
{"id": "https://colah.github.io/posts/2019-05-Collaboration/_p2", "contents": "\n    A lot of the best research in machine learning comes from collaborations.\n    In fact, many of the most significant papers in the last few years (TensorFlow, AlphaGo, etc) come from collaborations of 20+ people.\n    These collaborations are made possible by goodwill and trust between researchers.\n  "}
{"id": "https://colah.github.io/posts/2019-05-Collaboration/_p3", "contents": "\n    This goodwill and trust is a precious shared resource, and it can be a fragile thing.\n    When people work together, it\u2019s easy to have conflict, especially around attribution and credit.\n    If dealt with poorly, attribution issues can fester.\n    I'm aware of several cases of collaborations dieing, or people leaving teams and organizations,\n    where and the underlying issue was hurt feelings and lost trust around collaboration.\n    This strikes me as rather sad.\n  "}
{"id": "https://colah.github.io/posts/2019-05-Collaboration/_p4", "contents": "\n    We often talk about credit issues in kind of binary terms.\n    But if the thing we care about is this shared trust, I think it's not enough to just avoid doing anything wrong.\n    We must also avoid any feeling or appearance of unfairness.\n    In fact, we'd ideally actively cultivate the opposite, to behave in ways that add back to the pool of goodwill.\n  "}
{"id": "https://colah.github.io/posts/2019-05-Collaboration/_p5", "contents": "\n    We should also be mindful that credit issues can easily be perceived as, and likely often are, linked to privilege or power gradients.\n    This might be gender or race, but it can also be things like being a remote collaborator (ie. geographically removed from others), being an engineer or designer instead of a researcher, or being at a lower level professionally.\n    A perception that junior collaborators or those from under-represented minorities are taken advantage not only harms the research community, but the larger cause of making sure all humans are treated fairly.\n  "}
{"id": "https://colah.github.io/posts/2019-05-Collaboration/_p6", "contents": "\n    This is a hard problem, and one that lots of people have thought about before.\n    I don't have any perfect answers, but I'd like to share some personal working principles.\n    I\u2019d love your comments and feedback!\n  "}
{"id": "https://colah.github.io/posts/2019-05-Collaboration/_p7", "contents": "\n    Most attribution issues are mistakes rather than malice.\n    Adopting good collaboration practices is our best defense against making hurtful errors!\n  "}
{"id": "https://colah.github.io/posts/2019-05-Collaboration/_p8", "contents": "\n    More fundamentally, any place you can pre-commit yourself to treating others well is an opportunity to invest in trust and goodwill.\n    If you make ensuring that others are treated well a priority, those around you will notice,\n    and it will foster emotional safety within your working group.\n    It may also make others want to follow your example.\n  "}
{"id": "https://colah.github.io/posts/2019-05-Collaboration/_p9", "contents": "\n    Authorship order can be a tricky topic.\n    It often helps to make credit distribution more nuanced, flexible, and continuous.\n  "}
{"id": "https://colah.github.io/posts/2019-05-Collaboration/_p10", "contents": "\n    Ideally, we'd avoid all attribution conflicts with communication, thoughtfulness, and good collaboration practices.\n    But we're not perfect, and we live in the real world, so conflicts do happen.\n    Bringing someone else in to mediate the conflict is often the best path forward.\n  "}
{"id": "https://colah.github.io/posts/2019-05-Collaboration/_p11", "contents": "Be especially careful if any of the following risk factors are present:"}
{"id": "https://colah.github.io/posts/2019-05-Collaboration/_p12", "contents": "\n    Citation serves an important dual role: helping readers find related work, and allocating credit within the research community.\n    This second role makes it especially important to handle well: it can genuinely affect people\u2019s careers.\n    In some cases, it may even effect their lives in more profound ways such as immigration status.\n  "}
{"id": "https://colah.github.io/posts/2019-05-Collaboration/_p13", "contents": "\n  You can even go a step beyond \"citation\" in the traditional sense\n  by actively looking for opportunities to publicly praise and draw attention to the work of others.\n  Of course, you don't want to be ungenuine.\n  But if you're like me, you're often impressed by things that others do and never mention it publicly!\n  This is a skill I'd like to practice more.\n"}
{"id": "https://colah.github.io/posts/2019-05-Collaboration/_p14", "contents": "\n    While it's important and admirable to try and assume good faith,\n    it's also important to not have that assumption trap you in unhealthy situations.\n    You don't have an obligation to participate in collaborations that make you feel bad.\n    This is true even if your collaborators are perfectly nice, well-intentioned people,\n    or if you feel uncertain about whether they are.\n    Someone doesn't have to be a malicious plagiarist for the working relationship to not be a good fit.\n  "}
{"id": "https://colah.github.io/posts/2019-05-Collaboration/_p15", "contents": "\nExample:\n    X- was collaborating with researchers in another city.\n    Over the course of several projects, X- felt like their work was used without them being credited.\n    They felt unable to resolve the situation through conversations.\n    Although X- knew that the cause might be distance and poor communication,\n    they felt hurt and began to experience anxiety sharing unpublished work with these collaborators.\n    In the end, X- decided it would be healthier for them to stop working together.\n  "}
{"id": "https://colah.github.io/posts/2019-05-Collaboration/_p16", "contents": "\n    This document also would not be possible without many frank conversations with researchers who prefer to remain anonymous about difficult authorship and credit situations they've experienced.\n  "}
{"id": "https://colah.github.io/posts/2019-05-Collaboration/_p17", "contents": "\n    My thinking on this topic also greatly benefited from conversations with Michael Page and Catherine Olsson, as well as the advice of Michael Nielsen when I was navigating a challenging situation of my own.\n    Finally, I\u2019m grateful for the comments and feedback of Shan Carter, Ian Johnson, Dario Amodei, Holden Karnofsky, Anna Goldie, Smitha Milli, Nick Beckstead, Arvind Satyanarayan, Yomna Nasser, Matt Hoffman, Emma Pierson, Martin Abadi, Greg Corrado, Amy McDonald, Jeff Dean, Samy Bengio, Sageev Oore, Konstantinos Bousmalis, Peter Liu, Andrew Dai, Jasper Snoek, Delip Rao, and Vincent Vanhoucke.\n    None of these acknowledgments should be seen as that person endorsing the views in this essay.\n  "}
{"id": "https://colah.github.io/posts/2019-05-Collaboration/_p18", "contents": "\n    The wording \"pool of goodwill\" is inspired by Marilynne Robinson's \"reservoir of goodness.\"\n  "}
{"id": "https://colah.github.io/posts/2020-05-University/_p0", "contents": "Posted on May 30th, 2020"}
{"id": "https://colah.github.io/posts/2020-05-University/_p1", "contents": "\n\n    This essay collects thoughts I've had over several years, and doesn't\n    attempt to address present changes in whether it makes sense to attend\n    university due to COVID.\n  \n"}
{"id": "https://colah.github.io/posts/2020-05-University/_p2", "contents": "\n    I\u2019ve been somewhat successful as a researcher without an undergraduate degree or PhD.\n    As a result, I often have people ask me about whether it\u2019s possible to be successful without going to university,\n    whether they personally should, or whether I can help them persuade their parents.\n    At this point, I've probably received around a hundred emails on this topic.\n    It's still hard to know how to respond.\n  "}
{"id": "https://colah.github.io/posts/2020-05-University/_p3", "contents": "\n    I\u2019m reluctant to encourage people I don\u2019t know well to take non-traditional paths,\n    because I think they\u2019re riskier and depend a lot on the person.\n    In fact, I suspect that many of the people who write to me would be well served by going to university.\n    At the same time, I think alternative paths can be a great choice for some people\n    -- in some cases, significantly better than going to university.\n  "}
{"id": "https://colah.github.io/posts/2020-05-University/_p4", "contents": "\n    Unfortunately, society isn\u2019t very well set up to support and validate young people for whom this is the better choice.\n    So, if you\u2019re considering doing such a thing, you not only need to reason about whether it\u2019s a good decision, but may also need to navigate tricky emotional and social challenges.\n  "}
{"id": "https://colah.github.io/posts/2020-05-University/_p5", "contents": "\n    I suspect that some people who write to me are really looking for validation and support of their preference to not go to university.\n    Unfortunately, I\u2019m not well positioned to give that support, because I don\u2019t know them well.\n  "}
{"id": "https://colah.github.io/posts/2020-05-University/_p6", "contents": "\n    What I can give, and what this essay aims to do,\n    is to share thoughts on ways to think about the decision,\n    navigating the social and emotional challenges,\n    and resources you may find helpful.\n  "}
{"id": "https://colah.github.io/posts/2020-05-University/_p7", "contents": "\n    My first suggestion is to compare university to concrete alternatives.\n    Instead of asking \u201cIs university good?\u201d, ask \u201cDo I have something more compelling to do?\u201d.\n    Instead of \u201cShould I do a PhD?\u201d, ask \u201cWhere can I find the best environment to grow as a researcher?\u201d.\n    And so forth.\n  "}
{"id": "https://colah.github.io/posts/2020-05-University/_p8", "contents": "\n    Spending a year doing something else is almost always reversible:\n    if you end up wanting to go back to university, it was just a gap year.\n    As a result, the most important question is whether it will be time well spent\n    and that requires you to think about what specifically you\u2019ll be doing.\n  "}
{"id": "https://colah.github.io/posts/2020-05-University/_p9", "contents": "\n    If your plan is to learn or work on projects independently, it\u2019s worth thinking especially carefully.\n    This can be great -- I did it for three years and grew a lot as a result -- but it can also very easily fail.\n    Some important questions to ask yourself are:\n  "}
{"id": "https://colah.github.io/posts/2020-05-University/_p10", "contents": "\n    Some of these questions are tempting to just say yes to without engaging:\n    \u201cOf course I can work self-directed without structure.\u201d\n    But these are actually difficult things, and switching to having no structure is jumping into the deep end.\n    Many of the people I\u2019ve seen successfully work independently in their late teens / early twenties had a history of independent study or projects before doing so.\n    For example, many Thiel Fellows had either self studied to the point where they were they largely knew the undergraduate curriculum in some topic, or had significant applied expertise in programming.\n  "}
{"id": "https://colah.github.io/posts/2020-05-University/_p11", "contents": "\n    Doing an internship, residency, or startup can be easier\n    -- at least in some regards -- than working independently.\n    There\u2019s more structure (especially in the first two) and a clearer path to supporting yourself.\n    For these, I would mostly focus on how valuable you think the experience will be.\n    Do you feel like you\u2019ll learn? Does it feel like a good community? Does it feel important?\n    If it feels urgent, is the urgency for good reasons?\n  "}
{"id": "https://colah.github.io/posts/2020-05-University/_p12", "contents": "\n    It\u2019s important to be aware that not having a degree can have several negative long-term consequences.\n  "}
{"id": "https://colah.github.io/posts/2020-05-University/_p13", "contents": "\n    University degrees have a lot of signaling and credentialing value.\n    They cheaply communicate to other individuals and organizations that you have some baseline skills, at least in theory.\n    In some fields, you can succeed without a degree by demonstrating skill in other ways (publications, open source projects, portfolios, talks, awards, work history, referrals, etc).\n    Other fields are less accepting.\n    How can you tell which type of field you're in?\n    One useful test can be to look for examples of people who are successful without degrees in your field.\n    Even if your field is very good about this, you may wish to move into another field at some later point and run into challenges where your previous successes aren\u2019t legible to the new field, or the new field is more credential focused.\n  "}
{"id": "https://colah.github.io/posts/2020-05-University/_p14", "contents": "\n    Unfortunately, traditional credentials may be more important for demographics than others.\n    As a white male, I suspect that it's easier for me to be taken seriously without a degree that it would be for others.\n    If you are part of a group that society is less likely to perceive as skilled by default, it may be harder to do without the validation of a degree.\n    This is obviously really unfair, but I don't feel qualified to give advice on it or comment on how big an issue it is.\n  "}
{"id": "https://colah.github.io/posts/2020-05-University/_p15", "contents": "\n    There's also a weird flip side to all these downsides.\n    Once you establish yourself as competent there is this kind of threshold effect where not having a university degree can suddenly start causing people to actually take you more seriously.\n    This kind of countersignalling effect seems to be common when you do non-traditional things.\n    \n"}
{"id": "https://colah.github.io/posts/2020-05-University/_p16", "contents": "\n    In addition to the direct career consequences, not having a degree can be a source of major immigration challenges.\n    The version of this I\u2019ve seen most often is people wanting to move to the US (usually the Bay Area) but being ineligible for most visas due to the lack of a degree.\n    If you think you may have this problem, try to do a short consultation with an immigration lawyer early: there may be things you can do to start building a case.\n    (For those immigrating to the US, the solution to this is usually to get an \u201calien of extraordinary ability\u201d O-1 visa, which doesn\u2019t require a degree but does require a lot of evidence of your accomplishments.)\n  "}
{"id": "https://colah.github.io/posts/2020-05-University/_p17", "contents": "\n    Unfortunately, even if you feel confident that you would be best served by taking a non-traditional path, many young people face significant social and emotional barriers to doing so, particularly from adults.\n    While some people are lucky enough to have a family that will support them doing something unusual, many are not.\n    This can manifest in ways ranging from gentle skepticism, to lack of support, all the way up to coercive measures.\n    It can also involve hurtful accusations or gossiping (eg. someone isn\u2019t going to university because they\u2019re lazy or failed).\n  "}
{"id": "https://colah.github.io/posts/2020-05-University/_p18", "contents": "\n    Skepticism or lack of support from family often comes from a good place.\n    People around you are, naturally, worried about what will happen to you if you do something unusual.\n    They may also face other pressures.\n    For example, they may worry that their peers will think they are being irresponsible by \u201callowing\u201d their child to do this.\n    They may even worry how it will influence legal proceedings like a divorce, if such a thing is relevant.\n    It can feel especially hurtful when these other pressures are clearly major factors.\n    It\u2019s a difficult and emotionally stressful situation on both sides.\n  "}
{"id": "https://colah.github.io/posts/2020-05-University/_p19", "contents": "\n    The situation is often complicated by deeply asymmetric power dynamics.\n    Depending on the situation, your parents may have direct legal power, you may depend on them for financial support, you may depend on them for housing, and even if you are self-sufficient they may have legal control of your assets.\n    They also have, independent of their hard power, a great deal of emotional power from the role and authority they\u2019ve exercised throughout your life.\n    This can make the situation scary, volatile, and in some cases potentially dangerous for you.\n  "}
{"id": "https://colah.github.io/posts/2020-05-University/_p20", "contents": "\n    The most common resolution I\u2019ve seen or heard of is external validation:\n    someone gets a Thiel Fellowship, or their startup gets funded, etc, and people around them become much more supportive.\n    This can be a somewhat disappointing resolution for the \u201cvalidated\u201d party (\u201cwhy couldn\u2019t they believe in me before I was endorsed?\u201d).\n    It may help to try and picture things from the perspective of someone who is scared for you and doesn\u2019t understand the situation very well.\n  "}
{"id": "https://colah.github.io/posts/2020-05-University/_p21", "contents": "\n    In many cases, communicating with your family may be helpful.\n    There\u2019s a lot of general advice on how to have difficult conversations (non-violent communication, difficult conversations, etc.) that might be useful in having an effective conversation.\n    There's also more specific advice on how to have particular kinds of difficult conversations with your parents (eg. telling them that you're gay, or an atheist, or other things they may not respond to well) that may have some transferable advice.\n    Unfortunately, there are also cases where communicating might make things worse, because of the asymmetric power adults may have over you.\n  "}
{"id": "https://colah.github.io/posts/2020-05-University/_p22", "contents": "\n    While it won\u2019t solve the fundamental issues, it can be extremely emotionally helpful to have an aligned support network.\n    Having people who were proud of me for good reasons and could recognize my successes and failures was deeply affirming and motivating for me.\n    This is something you can deliberately build by seeking out people who share your interests and values.\n  "}
{"id": "https://colah.github.io/posts/2020-05-University/_p23", "contents": "\n    (In truly extreme cases, such as abusive families,\n    there may be ways to get support from a family court, get yourself emancipated or have your custody transferred.\n    There may be softer courses of action, like living with family or friends for a bit without legal action.\n    These are extreme courses of action that may sometimes be the best of many terrible options in rare cases.\n    I've also heard stories where they made things worse.\n    I don't feel qualified to give advice on this; if you are considering it, please seek specialized advice and be careful.\n    You may be able to go to a legal clinic or get a pro-bono consultation with a family lawyer.\n    You may wish to confirm confidentiality rules before speaking, in case they are different for someone underage.\n    There may be specialized non-profits in your area that can give you advice, hotlines, or useful resources online.\n    I am not the right person to ask for advice on this.)\n  "}
{"id": "https://colah.github.io/posts/2020-05-University/_p24", "contents": "\n    For many people, university is a period of social development.\n    They learn social skills, make long-term friends, and form romantic relationships.\n    For some people, especially readers of this essay, it seems possible this is the biggest benefit of university.\n  "}
{"id": "https://colah.github.io/posts/2020-05-University/_p25", "contents": "\n    It seems like many precocious young adults who leave university have less friends their own age.\n    Does that mean that not going to university stunts social development?\n    It seems hard to tell to what extent going to university actually causally effects social outcomes rather than just being correlated with them.\n    After all, it seems like many of us also didn't connect that much with our peers in grade school.\n    For example, I was intensely bullied in elementary school, and while I had friends in high school, I wasn't that close to anyone.\n    In fact, until recently, the vast majority of my friends have been 5-10 years older than me.\n    Conversely, I have friends I'd put in the \"precocious teenager\" category who went to university but didn't seem to really make friends their own age, especially during their freshman year.\n  "}
{"id": "https://colah.github.io/posts/2020-05-University/_p26", "contents": "\n    My guess is that correlated issues -- like not fitting in with one's age group and maybe being a bit shy -- are the bigger effect,\n    but that there's also a direct causal effect from not going to university.\n    Either way, I suspect some readers may find it hard to make friends among peers, regardless of their decision on university.\n    So it seems good to talk about a little bit.\n  "}
{"id": "https://colah.github.io/posts/2020-05-University/_p27", "contents": "\n    Firstly, I think it's okay to have your friends be older than you.\n    Teenage-Chris had lots of deeply meaningful connections with older friends and I think that was fine.\n    They were often more emotionally mature, thoughtful, and principled than my teenage peers, and sometimes felt like older siblings.\n    Over time, the age gap became less significant and by my mid 20s it was gone.\n  "}
{"id": "https://colah.github.io/posts/2020-05-University/_p28", "contents": "\n    I do think there's a lot to be said for investing in being social if it doesn't come naturally for you.\n    University might make being social easier, but I think committing yourself to investing it it is much more important.\n    For me, I've always disliked bars and parties, but I deeply cherish having close friends, which requires me to meet people.\n    What should I do?\n    Partly I've found it important to push to go to parties even if I find them stressful.\n    But I've found it more helpful to just really deliberately seek out social contexts that I do like,\n    and to create new ones if I can't find them.\n    For example, last year I held lots of small tea parties, and it was lovely!\n  "}
{"id": "https://colah.github.io/posts/2020-05-University/_p29", "contents": "\n    Another concern is the effect of not going to university on romantic relationships.\n    Many people have their first serious relationship in university, and I've seen friends, both male and female, worry about having missed this.\n    For me, there was a point when it seemed like most of my peers had found a partner in undergrad,\n    and younger-Chris was dramatically worried that he'd lost his chance and would be alone forever.\n    In retrospect, I was more worried than I needed to be.\n    Most of those relationships didn't last and, at least in my social circles, most people find their long-term partner outside university.\n    I'm now inclined towards a middle view: going to university is helpful for finding your life partner, and missing out on that is perhaps the biggest cost of not going, but it's also only one factor.\n    It's cliche, but the thing that really does matter is putting yourself out there, meeting people you might connect with,\n    and waiting for the micromarriages to add up.\n  "}
{"id": "https://colah.github.io/posts/2020-05-University/_p30", "contents": "\n    There's one final subtle issue I want to mention, which intersects with all of the above:\n     not going to university can create a professional gap between you and your age group.\n    I became a full time researcher at Google Brain when I was 23.\n    In that role, I started teaching, mentoring, and even managing residents and PhD interns who were often quite a bit older than me.\n    And sometimes when I spent time with people in my age group outside of work -- often, junior PhD students at that point -- people would not infrequently ask me about opportunities to do an internship with me.\n    But the thing I wanted was to find friends and peers, not find an intern!\n    Even when people didn't do this, I worried a lot about the possibility that I was in a position of power relative to people I was interacting with.\n    Meanwhile, a lot of my professional colleagues were older than me and, having known me since I was a teenager, still treated me that way.\n    This dynamic of mismatched professional achievement faded for a variety of reasons as I moved into my mid and late 20s, but was still a strange aspect of that period of my life.\n  "}
{"id": "https://colah.github.io/posts/2020-05-University/_p31", "contents": "\n    Many of these opportunities don't have a formal application process.\n    You need to reach out to people.\n    This is a skill, but the basic recipe is to invest in figuring out who it makes sense to reach out to, understanding them, and writing a thoughtful\n    email.\n  "}
{"id": "https://colah.github.io/posts/2020-05-University/_p32", "contents": "\n    My life would be in a very different position, were it not for that fact that many people intervened positively and greatly helped me.\n    Michael Nielsen, Yoshua Bengio, Jeff Dean, and Greg Corrado all took significant actions to support my career:\n    Michael mentored me when I was just beginning to do deep learning research and walked me through writing my first paper.\n    Yoshua invited and paid for me to visit MILA in 2013, and managed to get me admitted as a graduate student without an undergrad degree\n    (then I turned him down, which he was very kind about).\n    Jeff took me on as an intern. Greg continued managing me as an intern, and helped me convert to a full-time researcher at Google.\n    This chain of events was possible because the Thiel Fellowship initially gave me two years to work on whatever I wanted.\n  "}
{"id": "https://colah.github.io/posts/2020-05-University/_p33", "contents": "\n  Equally critical were the many people who supported me outside the context of deep learning research.\n  The hacklab.to community was an incredibly positive part of my life as a teenager, especially\n  Leigh Honeywell, Rob Gilson, Nicholas Dodds, Kate Murphy, Jade Bilkey, Eric Boyd, Madison Kelly, Alex Leitch, Paul Wouters, Sen Nordstrom and Alaina Hardie.\n  I'm also grateful to many professors at the University of Toronto for allowing me to audit courses as a high school student, and then giving me permission to take advanced courses in my one year there.\n  Related to this, I'm lucky that Brad Bass gave me a summer job doing programming at his lab in grade 10 and 11 -- I think I wasn't very useful, but spending months programming full time was great.\n  Too many other people supported me for me to really have any hope of mentioning everyone, but to list some of them:\n  Jen Dodd, Peter Salus,\n  Yomna Nasser,\n  Shai Maharaj,\n  Danielle Strachman,\n  Dario Amodei, Tamsyn Waterhouse, Daniela Amodei, Alex Dingle, Tim Telleen-Lawton, Taren Stinebrickner-Kauffman,\n  and\n  Laura Ball.\n"}
{"id": "https://colah.github.io/posts/2020-05-University/_p34", "contents": "\n  Thanks as well to all those who commented on drafts of this essay including Zan Armstrong, Anja Boskovic, Erin McCloskey, Tom Reid, Deborah Raji, Ria Cheruvu, Emil Wallner, Ali Zaidi, and Danielle Strachman.\n"}
{"id": "https://colah.github.io/mailto:chris@colah.ca_p0", "contents": "File not found"}
{"id": "https://colah.github.io/mailto:chris@colah.ca_p1", "contents": "\n        The site configured at this address does not\n        contain the requested file.\n      "}
{"id": "https://colah.github.io/mailto:chris@colah.ca_p2", "contents": "\n        If this is your site, make sure that the filename case matches the URL.\n        For root URLs (like http://example.com/) you must provide an\n        index.html file.\n      "}
{"id": "https://colah.github.io/mailto:chris@colah.ca_p3", "contents": "\nRead the full documentation\n        for more information about using GitHub Pages.\n      "}
{"id": "https://colah.github.io/personal/micromarriages/_p0", "contents": "Posted on Aug 23, 2018"}
{"id": "https://colah.github.io/personal/micromarriages/_p1", "contents": "Abstract: Micromarriages are essentially micromorts, but for marriage instead of death. They provide a powerful framework for thinking about questions ranging from \u201cShould I go to this party?\u201d to \u201cShould I breakup with my partner?\u201d Keywords: humor, romance, decision analysis"}
{"id": "https://colah.github.io/personal/micromarriages/_p2", "contents": "A micromarriage is a one in a million chance that an action will lead to you getting married, relative to your default policy. Note that some actions, such as dying, have a negative number of micromarriages associated with them."}
{"id": "https://colah.github.io/personal/micromarriages/_p3", "contents": "This definition has important caveats. Most people do not include being coercively forced into a marriage when calculating micromarriages. It\u2019s also common to discount by how far in the future the marriage is, although the choice of discount rate can be controversial. (People who expect the world to end in the near future may advocate a steeper discount rate.)"}
{"id": "https://colah.github.io/personal/micromarriages/_p4", "contents": "Yes! For short-term relationships, the preferred metric unit is the millihookup, owing to the larger quantities in typical usage. The imperial system uses a generic romantic unit, the date, which does not distinguish between relationship types."}
{"id": "https://colah.github.io/personal/micromarriages/_p5", "contents": "Critics of the date unit note that a typical date (event) contains much less than 1 date (unit) of romantic success (due to romantic deflation since it was defined),1 and that romantic success in different relationship types isn\u2019t fungible."}
{"id": "https://colah.github.io/personal/micromarriages/_p6", "contents": "The average American is exposed to around 50,000 micromarriages a year between 18 and 35,2 but this varies greatly from person to person. With careful analysis and effort, you can increase your exposure!"}
{"id": "https://colah.github.io/personal/micromarriages/_p7", "contents": "The first step is to estimate the typical conversion rate for hours and dollars to micromarriages available to you. Then, when considering actions in the future, you can ask whether they are competitive with your other options."}
{"id": "https://colah.github.io/personal/micromarriages/_p8", "contents": "Even if you\u2019re doing everything right, romance is a highly stochastic process, where you very seldom see reward. There's lots of events that don't lead to dates, dates that don't lead to relationships, and relationships that don't work out. It can all be pretty disheartening."}
{"id": "https://colah.github.io/personal/micromarriages/_p9", "contents": "Micromarriages can help you gamify this process. Even if the date doesn\u2019t go anywhere, you can feel good about the estimated micromariages counting up. This helps you acquire more more micromarriages -- and hopefully find a partner!"}
{"id": "https://colah.github.io/personal/micromarriages/_p10", "contents": "Nope. I'm about 90% joking. I do think the general idea can sometimes be helpful, though."}
{"id": "https://colah.github.io/personal/micromarriages/_p11", "contents": "My OkCupid profile is VegetarianMath (straight male, monogamous, wants kids). You can also send me an email. :)"}
{"id": "https://colah.github.io/personal/micromarriages/_p12", "contents": "In case you were tempted to take this too seriously:"}
{"id": "https://colah.github.io/personal/micromarriages/_p13", "contents": "The original version of this essay arbitrarily had 1 date (unit) be \"roughly equivalent to 8,000 micromarriages or 50 millihookups.\" This was intended to be deliberately higher than my crude estimate (2,500 micromarriages), so I could joke about how people probably went on fewer, more serious dates on the past. However, I think readers were unclear about how serious the numbers were until later, and so it seemed better to remove them."}
{"id": "https://colah.github.io/personal/micromarriages/_p14", "contents": "\nEstimate:\n    The only number I found for how many first dates single people go on is references to an eHarmony study claiming that the average single person goes on 41 first dates a year. (I didn't search very hard.) That seems shockingly high to me, and I'm kind of skeptical of a study by eHarmony that I can't find any details on. Then again, maybe I'm just a weird person who doesn't have empathy for the mass dating ways of the population at large and no engagement in hookup culture. If you took these numbers seriously, assume 50,000 micromarriages a year (see below), and that people spend half their time single, you get 2,500 micrommariages per date (event)."}
{"id": "https://colah.github.io/personal/micromarriages/_p15", "contents": " I'm doubtful this is a useful estimate. Even if I believed this was true on average, I expect that for many people who are pickier about dates, the average number of micromarriages is higher. One useful framing might be to think about the number of micromarriages in a \"promising\" or \"serious\" date.\n  \u21a9"}
{"id": "https://colah.github.io/personal/micromarriages/_p16", "contents": "\n    This number is based on FiveThirtyEight\u2019s \u201cWhen Will Everyone I Know Be Married\u201d article, which is in turn based on the American Community Survey."}
{"id": "https://colah.github.io/personal/micromarriages/_p17", "contents": "\n      The fraction of the population married roughly reaches 60% for both genders by 35, starting from very close to zero at 18.\n      Ignoring divorces and assuming the unmarried population follows a geometric trend, falling by a constant ratio each year,\n      this suggests a roughly 5% chance of marriage per year, or ~50,000 micromarriages.\n  \u21a9"}
{"id": "https://colah.github.io/../../_p0", "contents": "Bad request"}
{"id": "https://colah.github.io/../../_p1", "contents": "\n        Your browser sent a request that this server could not understand.\n        Sorry about that. Please check your request for errors and try again.\n        Contact us if the problem persists.\n      "}
{"id": "https://colah.github.io/knn_p0", "contents": "File not found"}
{"id": "https://colah.github.io/knn_p1", "contents": "\n        The site configured at this address does not\n        contain the requested file.\n      "}
{"id": "https://colah.github.io/knn_p2", "contents": "\n        If this is your site, make sure that the filename case matches the URL.\n        For root URLs (like http://example.com/) you must provide an\n        index.html file.\n      "}
{"id": "https://colah.github.io/knn_p3", "contents": "\nRead the full documentation\n        for more information about using GitHub Pages.\n      "}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p0", "contents": "Posted on April  6, 2014"}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p1", "contents": "Recently, there\u2019s been a great deal of excitement and interest in deep neural networks because they\u2019ve achieved breakthrough results in areas such as computer vision.1"}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p2", "contents": "However, there remain a number of concerns about them. One is that it can be quite challenging to understand what a neural network is really doing. If one trains it well, it achieves high quality results, but it is challenging to understand how it is doing so. If the network fails, it is hard to understand what went wrong."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p3", "contents": "While it is challenging to understand the behavior of deep neural networks in general, it turns out to be much easier to explore low-dimensional deep neural networks \u2013 networks that only have a few neurons in each layer. In fact, we can create visualizations to completely understand the behavior and training of such networks. This perspective will allow us to gain deeper intuition about the behavior of neural networks and observe a connection linking neural networks to an area of mathematics called topology."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p4", "contents": "A number of interesting things follow from this, including fundamental lower-bounds on the complexity of a neural network capable of classifying certain datasets."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p5", "contents": "Let\u2019s begin with a very simple dataset, two curves on a plane. The network will learn to classify points as belonging to one or the other."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p6", "contents": "The obvious way to visualize the behavior of a neural network \u2013 or any classification algorithm, for that matter \u2013 is to simply look at how it classifies every possible data point."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p7", "contents": "We\u2019ll start with the simplest possible class of neural network, one with only an input layer and an output layer. Such a network simply tries to separate the two classes of data by dividing them with a line."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p8", "contents": "That sort of network isn\u2019t very interesting. Modern neural networks generally have multiple layers between their input and output, called \u201chidden\u201d layers. At the very least, they have one."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p9", "contents": "As before, we can visualize the behavior of this network by looking at what it does to different points in its domain. It separates the data with a more complicated curve than a line."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p10", "contents": "With each layer, the network transforms the data, creating a new representation.2 We can look at the data in each of these representations and how the network classifies them. When we get to the final representation, the network will just draw a line through the data (or, in higher dimensions, a hyperplane)."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p11", "contents": "In the previous visualization, we looked at the data in its \u201craw\u201d representation. You can think of that as us looking at the input layer. Now we will look at it after it is transformed by the first layer. You can think of this as us looking at the hidden layer."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p12", "contents": "Each dimension corresponds to the firing of a neuron in the layer."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p13", "contents": "In the approach outlined in the previous section, we learn to understand networks by looking at the representation corresponding to each layer. This gives us a discrete list of representations."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p14", "contents": "The tricky part is in understanding how we go from one to another. Thankfully, neural network layers have nice properties that make this very easy."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p15", "contents": "There are a variety of different kinds of layers used in neural networks. We will talk about tanh layers for a concrete example. A tanh layer \\(\\tanh(Wx+b)\\) consists of:"}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p16", "contents": "We can visualize this as a continuous transformation, as follows:"}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p17", "contents": "The story is much the same for other standard layers, consisting of an affine transformation followed by pointwise application of a monotone activation function."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p18", "contents": "We can apply this technique to understand more complicated networks. For example, the following network classifies two spirals that are slightly entangled, using four hidden layers. Over time, we can see it shift from the \u201craw\u201d representation to higher level ones it has learned in order to classify the data. While the spirals are originally entangled, by the end they are linearly separable."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p19", "contents": "On the other hand, the following network, also using multiple layers, fails to classify two spirals that are more entangled."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p20", "contents": "It is worth explicitly noting here that these tasks are only somewhat challenging because we are using low-dimensional neural networks. If we were using wider networks, all this would be quite easy."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p21", "contents": "(Andrej Karpathy has made a nice demo based on ConvnetJS that allows you to interactively explore networks with this sort of visualization of training!)"}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p22", "contents": "Each layer stretches and squishes space, but it never cuts, breaks, or folds it. Intuitively, we can see that it preserves topological properties. For example, a set will be connected afterwards if it was before (and vice versa)."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p23", "contents": "Transformations like this, which don\u2019t affect topology, are called homeomorphisms. Formally, they are bijections that are continuous functions both ways."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p24", "contents": "Theorem: Layers with \\(N\\) inputs and \\(N\\) outputs are homeomorphisms, if the weight matrix, \\(W\\), is non-singular. (Though one needs to be careful about domain and range.)"}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p25", "contents": "Proof: Let\u2019s consider this step by step:"}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p26", "contents": "Thus, if \\(W\\) has a non-zero determinant, our layer is a homeomorphism. \u220e"}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p27", "contents": "This result continues to hold if we compose arbitrarily many of these layers together."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p28", "contents": "Consider a two dimensional dataset with two classes \\(A, B \\subset \\mathbb{R}^2\\):"}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p29", "contents": "\\[A = \\{x | d(x,0) < 1/3\\}\\]"}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p30", "contents": "\\[B = \\{x | 2/3 < d(x,0) < 1\\}\\]"}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p31", "contents": "Claim: It is impossible for a neural network to classify this dataset without having a layer that has 3 or more hidden units, regardless of depth."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p32", "contents": "As mentioned previously, classification with a sigmoid unit or a softmax layer is equivalent to trying to find a hyperplane (or in this case a line) that separates \\(A\\) and \\(B\\) in the final represenation. With only two hidden units, a network is topologically incapable of separating the data in this way, and doomed to failure on this dataset."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p33", "contents": "In the following visualization, we observe a hidden representation while a network trains, along with the classification line. As we watch, it struggles and flounders trying to learn a way to do this."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p34", "contents": "In the end it gets pulled into a rather unproductive local minimum. Although, it\u2019s actually able to achieve \\(\\sim 80\\%\\) classification accuracy."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p35", "contents": "This example only had one hidden layer, but it would fail regardless."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p36", "contents": "Proof: Either each layer is a homeomorphism, or the layer\u2019s weight matrix has determinant 0. If it is a homemorphism, \\(A\\) is still surrounded by \\(B\\), and a line can\u2019t separate them. But suppose it has a determinant of 0: then the dataset gets collapsed on some axis. Since we\u2019re dealing with something homeomorphic to the original dataset, \\(A\\) is surrounded by \\(B\\), and collapsing on any axis means we will have some points of \\(A\\) and \\(B\\) mix and become impossible to distinguish between. \u220e"}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p37", "contents": "If we add a third hidden unit, the problem becomes trivial. The neural network learns the following representation:"}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p38", "contents": "With this representation, we can separate the datasets with a hyperplane."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p39", "contents": "To get a better sense of what\u2019s going on, let\u2019s consider an even simpler dataset that\u2019s 1-dimensional:"}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p40", "contents": "\\[A = [-\\frac{1}{3}, \\frac{1}{3}]\\]"}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p41", "contents": "\\[B = [-1, -\\frac{2}{3}] \\cup [\\frac{2}{3}, 1]\\]"}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p42", "contents": "Without using a layer of two or more hidden units, we can\u2019t classify this dataset. But if we use one with two units, we learn to represent the data as a nice curve that allows us to separate the classes with a line:"}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p43", "contents": "What\u2019s happening? One hidden unit learns to fire when \\(x > -\\frac{1}{2}\\) and one learns to fire when \\(x > \\frac{1}{2}\\). When the first one fires, but not the second, we know that we are in A."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p44", "contents": "Is this relevant to real world data sets, like image data? If you take the manifold hypothesis really seriously, I think it bears consideration."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p45", "contents": "The manifold hypothesis is that natural data forms lower-dimensional manifolds in its embedding space. There are both theoretical3 and experimental4 reasons to believe this to be true. If you believe this, then the task of a classification algorithm is fundamentally to separate a bunch of tangled manifolds."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p46", "contents": "In the previous examples, one class completely surrounded another. However, it doesn\u2019t seem very likely that the dog image manifold is completely surrounded by the cat image manifold. But there are other, more plausible topological situations that could still pose an issue, as we will see in the next section."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p47", "contents": "Another interesting dataset to consider is two linked tori, \\(A\\) and \\(B\\)."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p48", "contents": "Much like the previous datasets we considered, this dataset can\u2019t be separated without using \\(n+1\\) dimensions, namely a \\(4\\)th dimension."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p49", "contents": "Links are studied in knot theory, an area of topology. Sometimes when we see a link, it isn\u2019t immediately obvious whether it\u2019s an unlink (a bunch of things that are tangled together, but can be separated by continuous deformation) or not."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p50", "contents": "If a neural network using layers with only 3 units can classify it, then it is an unlink. (Question: Can all unlinks be classified by a network with only 3 units, theoretically?)"}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p51", "contents": "From this knot perspective, our continuous visualization of the representations produced by a neural network isn\u2019t just a nice animation, it\u2019s a procedure for untangling links. In topology, we would call it an ambient isotopy between the original link and the separated ones."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p52", "contents": "Formally, an ambient isotopy between manifolds \\(A\\) and \\(B\\) is a continuous function \\(F: [0,1] \\times X \\to Y\\) such that each \\(F_t\\) is a homeomorphism from \\(X\\) to its range, \\(F_0\\) is the identity function, and \\(F_1\\) maps \\(A\\) to \\(B\\). That is, \\(F_t\\) continuously transitions from mapping \\(A\\) to itself to mapping \\(A\\) to \\(B\\)."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p53", "contents": "Theorem: There is an ambient isotopy between the input and a network layer\u2019s representation if: a) \\(W\\) isn\u2019t singular, b) we are willing to permute the neurons in the hidden layer, and c) there is more than 1 hidden unit."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p54", "contents": "Proof: Again, we consider each stage of the network individually:"}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p55", "contents": "I imagine there is probably interest in programs automatically discovering such ambient isotopies and automatically proving the equivalence of certain links, or that certain links are separable. It would be interesting to know if neural networks can beat whatever the state of the art is there."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p56", "contents": "(Apparently determining if knots are trivial is NP. This doesn\u2019t bode well for neural networks.)"}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p57", "contents": "The sort of links we\u2019ve talked about so far don\u2019t seem likely to turn up in real world data, but there are higher dimensional generalizations. It seems plausible such things could exist in real world data."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p58", "contents": "Links and knots are \\(1\\)-dimensional manifolds, but we need 4 dimensions to be able to untangle all of them. Similarly, one can need yet higher dimensional space to be able to unknot \\(n\\)-dimensional manifolds. All \\(n\\)-dimensional manifolds can be untangled in \\(2n+2\\) dimensions.6"}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p59", "contents": "(I know very little about knot theory and really need to learn more about what\u2019s known regarding dimensionality and links. If we know a manifold can be embedded in n-dimensional space, instead of the dimensionality of the manifold, what limit do we have?)"}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p60", "contents": "The natural thing for a neural net to do, the very easy route, is to try and pull the manifolds apart naively and stretch the parts that are tangled as thin as possible. While this won\u2019t be anywhere close to a genuine solution, it can achieve relatively high classification accuracy and be a tempting local minimum."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p61", "contents": "It would present itself as very high derivatives on the regions it is trying to stretch, and sharp near-discontinuities. We know these things happen.7 Contractive penalties, penalizing the derivatives of the layers at data points, are the natural way to fight this.8"}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p62", "contents": "Since these sort of local minima are absolutely useless from the perspective of trying to solve topological problems, topological problems may provide a nice motivation to explore fighting these issues."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p63", "contents": "On the other hand, if we only care about achieving good classification results, it seems like we might not care. If a tiny bit of the data manifold is snagged on another manifold, is that a problem for us? It seems like we should be able to get arbitrarily good classification results despite this issue."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p64", "contents": "(My intuition is that trying to cheat the problem like this is a bad idea: it\u2019s hard to imagine that it won\u2019t be a dead end. In particular, in an optimization problem where local minima are a big problem, picking an architecture that can\u2019t genuinely solve the problem seems like a recipe for bad performance.)"}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p65", "contents": "The more I think about standard neural network layers \u2013 that is, with an affine transformation followed by a point-wise activation function \u2013 the more disenchanted I feel. It\u2019s hard to imagine that these are really very good for manipulating manifolds."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p66", "contents": "Perhaps it might make sense to have a very different kind of layer that we can use in composition with more traditional ones?"}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p67", "contents": "The thing that feels natural to me is to learn a vector field with the direction we want to shift the manifold:"}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p68", "contents": "And then deform space based on it:"}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p69", "contents": "One could learn the vector field at fixed points (just take some fixed points from the training set to use as anchors) and interpolate in some manner. The vector field above is of the form:"}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p70", "contents": "\\[F(x) = \\frac{v_0f_0(x) + v_1f_1(x)}{1+f_0(x)+f_1(x)}\\]"}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p71", "contents": "Where \\(v_0\\) and \\(v_1\\) are vectors and \\(f_0(x)\\) and \\(f_1(x)\\) are n-dimensional gaussians. This is inspired a bit by radial basis functions."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p72", "contents": "I\u2019ve also begun to think that linear separability may be a huge, and possibly unreasonable, amount to demand of a neural network. In some ways, it feels like the natural thing to do would be to use k-nearest neighbors (k-NN). However, k-NN\u2019s success is greatly dependent on the representation it classifies data from, so one needs a good representation before k-NN can work well."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p73", "contents": "As a first experiment, I trained some MNIST networks (two-layer convolutional nets, no dropout) that achieved \\(\\sim 1\\%\\) test error. I then dropped the final softmax layer and used the k-NN algorithm. I was able to consistently achieve a reduction in test error of 0.1-0.2%."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p74", "contents": "Still, this doesn\u2019t quite feel like the right thing. The network is still trying to do linear classification, but since we use k-NN at test time, it\u2019s able to recover a bit from mistakes it made."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p75", "contents": "k-NN is differentiable with respect to the representation it\u2019s acting on, because of the 1/distance weighting. As such, we can train a network directly for k-NN classification. This can be thought of as a kind of \u201cnearest neighbor\u201d layer that acts as an alternative to softmax."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p76", "contents": "We don\u2019t want to feedforward our entire training set for each mini-batch because that would be very computationally expensive. I think a nice approach is to classify each element of the mini-batch based on the classes of other elements of the mini-batch, giving each one a weight of 1/(distance from classification target).9"}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p77", "contents": "Sadly, even with sophisticated architecture, using k-NN only gets down to 5-4% test error \u2013 and using simpler architectures gets worse results. However, I\u2019ve put very little effort into playing with hyper-parameters."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p78", "contents": "Still, I really aesthetically like this approach, because it seems like what we\u2019re \u201casking\u201d the network to do is much more reasonable. We want points of the same manifold to be closer than points of others, as opposed to the manifolds being separable by a hyperplane. This should correspond to inflating the space between manifolds for different categories and contracting the individual manifolds. It feels like simplification."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p79", "contents": "Topological properties of data, such as links, may make it impossible to linearly separate classes using low-dimensional networks, regardless of depth. Even in cases where it is technically possible, such as spirals, it can be very challenging to do so."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p80", "contents": "To accurately classify data with neural networks, wide layers are sometimes necessary. Further, traditional neural network layers do not seem to be very good at representing important manipulations of manifolds; even if we were to cleverly set weights by hand, it would be challenging to compactly represent the transformations we want. New layers, specifically motivated by the manifold perspective of machine learning, may be useful supplements."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p81", "contents": "(This is a developing research project. It\u2019s posted as an experiment in doing research openly. I would be delighted to have your feedback on these ideas: you can comment inline or at the end. For typos, technical errors, or clarifications you would like to see added, you are encouraged to make a pull request on github.)"}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p82", "contents": "Thank you to Yoshua Bengio, Michael Nielsen, Dario Amodei, Eliana Lorch, Jacob Steinhardt, and Tamsyn Waterhouse for their comments and encouragement."}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p83", "contents": "This seems to have really kicked off with Krizhevsky et al., (2012), who put together a lot of different pieces to achieve outstanding results. Since then there\u2019s been a lot of other exciting work.\u21a9"}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p84", "contents": "These representations, hopefully, make the data \u201cnicer\u201d for the network to classify. There has been a lot of work exploring representations recently. Perhaps the most fascinating has been in Natural Language Processing: the representations we learn of words, called word embeddings, have interesting properties. See Mikolov et al. (2013), Turian et al. (2010), and, Richard Socher\u2019s work. To give you a quick flavor, there is a very nice visualization associated with the Turian paper.\u21a9"}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p85", "contents": "A lot of the natural transformations you might want to perform on an image, like translating or scaling an object in it, or changing the lighting, would form continuous curves in image space if you performed them continuously.\u21a9"}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p86", "contents": "Carlsson et al. found that local patches of images form a klein bottle.\u21a9"}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p87", "contents": "\\(GL_n(\\mathbb{R})\\) is the set of invertible \\(n \\times n\\) matrices on the reals, formally called the general linear group of degree \\(n\\).\u21a9"}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p88", "contents": "This result is mentioned in Wikipedia\u2019s subsection on Isotopy versions.\u21a9"}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p89", "contents": "See Szegedy et al., where they are able to modify data samples and find slight modifications that cause some of the best image classification neural networks to misclasify the data. It\u2019s quite troubling.\u21a9"}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p90", "contents": "Contractive penalties were introduced in contractive autoencoders. See Rifai et al. (2011).\u21a9"}
{"id": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/_p91", "contents": "I used a slightly less elegant, but roughly equivalent algorithm because it was more practical to implement in Theano: feedforward two different batches at the same time, and classify them based on each other.\u21a9"}
